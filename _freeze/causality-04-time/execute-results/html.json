{
  "hash": "3dd5a9cb3a1fbad2a306fc5f19254817",
  "result": {
    "markdown": "# Identification and Time\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setup\"}\nlibrary(tidyverse)\n\ntheme_set(\n  theme_bw(base_family = \"Crimson Text\")\n)\n```\n:::\n\n\nThis notebook deals with methods that use time to solve causal inference problems.\n\nFixed effects\n\nDifferences in differences\n\nRegression discontinuity\n\n*Bring the stuff from the longitudinal notebooks.*\n\n|               |                                       |                          |\n|---------------|---------------------------------------|--------------------------|\n|               | Observed                              | Unobserved               |\n| Time-constant | Experiments, Matching, Regression, FE | Experiments, FE          |\n| Time-varying  | Experiments, Matching, Regression, FE | Experiments, Experiments |\n\n## Fixed Effects\n\n**Identification using *variation within* groups or individuals**\n\nRepeated observations within groups or individuals can provide a means for adjusting for unobserved confounders. This is conceptually similar to twin studies in which we look for variation *within* pairs of twins. We can extend this to larger groups or repeated observations.\n\nThe simplest regression model that handles this simply adds *varying-intercepts* $\\alpha_i$ or \"fixed effects.\"\n\n$$\ny_{ij} = \\beta_0 + \\theta x_{ij} + \\alpha_i + \\varepsilon_{ij}\n$$\n\nWith more than two observations per group, we can fit this model using *differences.*\n\n$$\ny_{ij} - \\overline y_i = \\theta (x_{ij} - \\overline x_i) + \\varepsilon_{ij} - \\overline \\varepsilon_{i}\n$$\n\n**The causal predictor needs to vary *within* groups.**\n\n> This study design can create comparisons across different levels of the treatment variable only if it varies within group. In examples where the treatment is dichotomous, it is possible for a substantial portion of the sample to exhibit no variation at all in the causal variable *within groups.* For instance, suppose a varying-intercepts model is used to estimate the effect of maternal employment on child cognitive outcomes by including indicators for each family (set of siblings) in a regression of outcomes on an indicator for whether the mother worked in the first year of the child's life. In some families, the mother may not have varied her employment status across children. Therefore, no inferences about the effect of maternal employment status can be made for these families. We can only estimate effects for the type of family where the mother varied her employment choice across the children (for example, working after her first child was born but staying home from work after the second).\n>\n> @gelman2020 [pp. 441]\n\n**Within-person variation (or \"fixed effects\")**\n\nPanel data provides the most common use of varying intercepts to estimate causal effects. Here we use repeated observations within the same individuals to adjust for time-constant unobserved confounders. Here, issues of balance and overlap still exist, but they only apply for *within-*person comparisons.\n\nIf the purpose of **matching/weighting** is to get rid of all observable confounders, then the purpose of **fixed-effects** is to *additionally* get rid of all time-constant unobserved confounders.\n\n**Weight Loss Example**\n\nLet's say I weigh an individual in January (time 1) and again in July (time 2). This individual was not dieting before January, but does diet between January and July. In July, we find that the individual weighs 10kg less.\n\nWe want to know how much the diet *caused* the individual's weight to change. That is, I'd like to compare the actual weight loss ( $\\text{weight}_\\text{July}^1 - \\text{weight}_\\text{Jan}^1$ ) to a counterfactual weight loss in a world where where there is no diet ( $\\text{weight}_\\text{July}^0 - \\text{weight}_\\text{Jan}^0$ ).\n\nWe can represent weight at two different time points using the following equations:\n\n$$\n\\begin{align}\n\\text{weight}_1 &= \\theta \\ \\text{diet}_1 + \\mu + \\epsilon_1, \\\\\n\\text{weight}_2 &= \\theta \\ \\text{diet}_2 + \\mu + \\epsilon_2\n\\end{align}\n$$\n\nHere, $\\theta$ is the *treatment effect* of the diet; $\\mu$ represents everything about the individual that *does not* change (i.e., time-constant) and that affects their weight (i.e., a personal *fixed effect);* $\\epsilon_1$ and $\\epsilon_2$ represent other time-varying effects on weight.\n\nFinally, because we already assumed that the individual was not dieting at time 1, we know that $\\text{diet}_1 = 0$. Subtracting both equations, we get the following:\n\n$$\n\\text{weight}_2 - \\text{weight}_1 = \\theta + \\underbrace{(\\mu - \\mu)}_{0}  + (\\epsilon_2 - \\epsilon_1)\n$$\n\nThe power of fixed-effects estimation allows us to get rid of $(\\mu - \\mu)$ by simple subtraction; by comparing individuals to themselves (*within-person* variance), we have gotten rid of *everything* about them that does not change (i.e., time-constant), *even if we do not know what that is.*\n\nIn other words, this simple pre-post comparison allows for a good estimate of $\\theta$ *if* we are also willing to believe that $E[\\epsilon_2 - \\epsilon_1] = 0$. Maybe the individual would have lost weight anyway even without dieting because January is during the holidays when it is easier to gain weight (i.e., $\\epsilon_1 > \\epsilon_2)$.\n\nTo deal with this, we might want to compare people who diet between January and July with those who do *not* diet over the same period to capture the effect of the passage of time. This is called **difference-in-differences** estimation.\n\n## Difference-in-Differences\n\n**Identification using within *and* between group variation.**\n\nAlmost all identification strategies rely on comparisons across groups that have and have not been exposed to a treatment. *Difference-in-differences* strategies additionally make use of another source of variation---i.e., time---to help adjust for potential unobserved differences across groups.\n\nThe goal is to be able to compare the *within* variation in the treated group to the *within* variation in the untreated group.\n\n**Assumptions**\n\nThe change in $y$ for the control group needs to represent what *would* have happened to the treatment group in the absence of the treatment.\n\nPotential change, given that $P=1$ means post-exposure and $P=0$ means pre-exposure.\n\n$$\n\\begin{align}\nd^0 &= y^0_{P=1} - y^0_{P=0}, \\\\\\\\\nd^1 &= y^1_{P=1} - y^1_{P=0}\n\\end{align}\n$$\n\n*Average Treatment Effects:*\n\n$$\n\\begin{align}\n\\text{ATE} &= \\text{E}[d^1 - d^0], \\\\\\\\\n\\text{ATT} &= \\text{E}[d^1 - d^0 \\mid T = 1], \\\\\\\\\n\\text{ATU} &= \\text{E}[d^1 - d^0 \\mid T = 0]\n\\end{align}\n$$\n\n*Ignorability assumption:*\n\n$$\nd^0, d^1 \\perp T\n$$\n\n> This design is often presented as if it provides a weaker set of assumptions than traditional observational studies, as it a quasi-experiment, not just a study where we have to hope we've measured all confounders. However, looking carefully at the assumptions, it is unclear if the assumption of randomly assigned changes in potential outcome is truly more plausible than the assumption of randomly assigned potential outcomes for those with the same value of the pre-treatment variable (and of course both approaches could allow for conditioning on additional covariates). In either case, we need not assume actual random manipulation of treatment assignment for either assumption to hold, only results that would be consistent with such manipulation.\n>\n> @gelman2020 [pp. 444-5]\n\nThe **parallel trends** assumption is inherently *unobservable.* It says that the difference between the treated and untreated groups *would* have remained the same in the post-treatment period as it was in the pre-treatment period, as depicted in @fig-parallel-trends. This assumption fails, among other reasons, when there are *other* treatments affecting the untreated groups that are absent in the treated groups.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(\n  t = c(\"Time 0\", \"Time 1\"),\n  y1 = 1:2,\n  y0 = 0:1,\n  y = c(1, 3)\n) |> \n  pivot_longer(!c(t, y), names_to = \"g\") |> \n  ggplot(aes(t, value, group = g)) + \n  geom_line(linetype = \"dashed\") + \n  geom_vline(xintercept = c(\"Time 0\", \"Time 1\")) +\n  geom_line(aes(t, y)) + \n  geom_segment(x = \"Time 1\", y = 2, yend = 3, xend = \"Time 1\", linewidth = 1.5, \n               color = \"steelblue1\") +\n  labs(y = NULL, x = NULL) + \n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), \n        panel.grid = element_blank()) + \n  annotate(\"text\", x = 2.2, y = 2.5, label = latex2exp::TeX(r\"($E[d^1 - d^0]$)\"), family = \"Crimson Text\")\n```\n\n::: {.cell-output-display}\n![Parallel Trends](causality-04-time_files/figure-html/fig-parallel-trends-1.png){#fig-parallel-trends width=384}\n:::\n:::\n\n\n*Note. If parallel trends hold for* $Y$*, then it does not hold for* $\\log(Y)$ *or any other non-linear transformation.*\n\n**Regression Framework**\n\nSuppose that time is represented with a simple before-and-after exposure variable $P$. We can see the difference-in-differences strategies within the traditional regression framework in terms of a simple interaction.\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 P_i + \\underbrace{\\theta \\ x_i P_i}_\\text{diff-n-diff} + \\varepsilon_i\n$$ {#eq-diff-interaction}\n\nThe difference-in-differences estimand is simply the coefficient on the interaction term.\n\n> Intuitively this makes sense because the interaction term represents the difference in the slope coefficients on time across the treatment groups where the slope coefficients on time each represent a difference in means (across time points). Equivalently, we could say that the interaction term represents the difference in slope coefficients on treatment across the time periods.\n>\n> @gelman2020 [pp. 443]\n\n|         | $X = 0$ | $X = 1$ |\n|---------|---------|---------|\n| $P = 0$ | 80      | 80      |\n| $P=1$   | 75      | 70      |\n\n: Toy Example {#tbl-diff-n-diff-toy}\n\n<aside>Note that *everyone* has the same time values \"pre\" and \"post.\" There is no *between* person variation in time. If there was, we would have to do something like **two-way fixed effects**.</aside>\n\nWe can combine the values in @tbl-diff-n-diff-toy with @eq-diff-interaction to get the following values:\n\n$$\n\\beta_0 = 80, \\ \\beta_1 = 0, \\ \\beta_2 = -5, \\theta = -5\n$$\n\n**Two-way fixed effects**\n\nAlternatively, we can think of this in terms of two-way fixed effects.\n\n$$\ny_i = \\alpha_{g[i]} + \\alpha_{t[i]} + \\underbrace{\\beta \\ T_i}_\\text{diff-n-diff} + \\varepsilon_i\n$$ {#eq-diff-twfe}\n\nThe $\\theta$ coefficient in @eq-diff-interaction and @eq-diff-twfe is equivalent, provided we only have two groups and two time periods.\n\nUnfortunately, the TWFE approach for calculating difference-in-differences effects does *not* work particularly well for \"rollout designs\" (or \"staggered treatment timing\") in which the treatment is applied at different times to different groups.\n\nExample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"organ_donations\", package = \"causaldata\")\nlibrary(fixest)\nlibrary(lme4)\nlibrary(plm)\n\nd <- organ_donations |> \n  janitor::clean_names() |> \n  mutate(treated = state == \"California\" & quarter %in% c('Q32011','Q42011','Q12012')) |> \n  mutate(state = factor(state), quarter = factor(quarter))\n\nclfe <- feols(rate ~ treated | state + quarter, data = d)\nols <- lm(rate ~ treated + state + quarter, data = d)\n\ntwfe <- plm(\n  rate ~ treated, \n  index = c(\"quarter\", \"state\"), \n  model = \"within\", \n  effect = \"twoways\",\n  data = d\n)\n\nmodelsummary::modelsummary(\n  list(feols = clfe, lm = ols, plm = twfe), \n  coef_omit = c(-1), \n  gof_map = \"nobs\"\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> feols </th>\n   <th style=\"text-align:center;\"> lm </th>\n   <th style=\"text-align:center;\"> plm </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> treatedTRUE </td>\n   <td style=\"text-align:center;\"> −0.022 </td>\n   <td style=\"text-align:center;\"> −0.022 </td>\n   <td style=\"text-align:center;\"> −0.022 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> (0.006) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> (0.020) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> (0.020) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 162 </td>\n   <td style=\"text-align:center;\"> 162 </td>\n   <td style=\"text-align:center;\"> 162 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n*Note the difference in standard errors.*\n\nTest of prior trends:\n\n1.  Use the data from before the treatment period.\n\n2.  Do a NHST on the $\\theta$ coefficient in the following regression:\n\n    $$\n    y_i = \\alpha_{g[i]} + \\beta \\ t_i + \\theta \\ t_i \\times g_i + \\varepsilon_i\n    $$\n\n***Dynamic Treatment Effects***\n\nInstead of just focusing on \"before\" and \"after\" effects, we can add a separate effect for each time period. A common way of doing in this is to generated a *centered* time variable---i.e., you subtract the treatment period from the original time variable. Then we just interact the treatment variable with a set of indicator variables for each of the time periods.\n\nFor example, suppose we have 3 time periods and that the treatment occurs in $t = 0$.\n\n$$\ny_i = \\alpha_{g[i]} + \\alpha_{t[i]} + \\theta_{-1[i]} \\times T_i + \\theta_{1[i]} \\times T_i + \\varepsilon_i\n$$\n\n<aside>$\\theta_0$ *is dropped to avoid perfect multicollinearity*</aside>\n\nHere, we *should not* find discernible effects among the before-treatment coefficients. Our confidence intervals should also blow up in size because we have less data. @fig-diff-dynamic presents the results for this kind of setup.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse); library(fixest)\nod <- causaldata::organ_donations\n\n# Treatment variable\nod <- od %>% mutate(California = State == 'California')\n\n# Interact quarter with being in the treated group using\n# the fixest i() function, which also lets us specify\n# a reference period (using the numeric version of Quarter)\nclfe <- feols(Rate ~ i(Quarter_Num, California, ref = 3) | \n            State + Quarter_Num, data = od)\n\n# And use coefplot() for a graph of effects\ncoefplot(clfe)\n```\n\n::: {.cell-output-display}\n![Dynamic Effects in Organ Donation Rates Example](causality-04-time_files/figure-html/fig-diff-dynamic-1.png){#fig-diff-dynamic width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## something fucked up is going on here\nunique(od$Quarter_Num - 3L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2 -1  0  1  2  3\n```\n:::\n\n```{.r .cell-code}\nod <- od |> \n  mutate(num = factor(Quarter_Num - 3L)) |> \n  mutate(num = fct_relevel(num, \"0\"))\n  \nlm(Rate ~ factor(State) + num + num*California, data = od) |> \n  broom::tidy(conf.int = TRUE)  |> \n  slice(34:38) |> \n  ggplot(aes(term, estimate)) + \n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](causality-04-time_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nAdd example from Theory and Credibility\n\n*Note. If you combine difference-in-differences with matching, the answer will be exactly the same as long as you match on time-constant observable.*\n\n## Exercises\n\ndiff-n-diff\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsw <- haven::read_dta(\"data/nsw.dta\")\nnsw_long <- nsw |>\n  panelr::long_panel(\n    id = \"id\",\n    wave = \"t\",\n    periods = c(75, 78),\n    label_location = \"end\"\n  ) |> \n  mutate(post = if_else(t == 78, 1L, 0L))\n\nglimpse(nsw_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,444\nColumns: 12\nGroups: id [722]\n$ id        <fct> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10…\n$ t         <dbl> 75, 78, 75, 78, 75, 78, 75, 78, 75, 78, 75, 78, 75, 78, 75, …\n$ data_id   <chr> \"Lalonde Sample\", \"Lalonde Sample\", \"Lalonde Sample\", \"Lalon…\n$ treat     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ age       <dbl> 37, 37, 22, 22, 30, 30, 27, 27, 33, 33, 22, 22, 23, 23, 32, …\n$ education <dbl> 11, 11, 9, 9, 12, 12, 11, 11, 8, 8, 9, 9, 12, 12, 11, 11, 16…\n$ black     <dbl> 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, …\n$ hispanic  <dbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ married   <dbl> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ nodegree  <dbl> 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, …\n$ re        <dbl> 0.0000, 9930.0459, 0.0000, 3595.8940, 0.0000, 24909.4492, 0.…\n$ post      <int> 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, …\n```\n:::\n\n```{.r .cell-code}\nggplot(nsw_long, aes(x = post, y = re, group = factor(treat))) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(limits = c(0, 1), breaks = c(0,1), labels = c(\"1975\", \"1978\")) +\n  theme(legend.position = \"top\") +\n  labs(x = \"year\",\n       y = \"income ($)\",\n       title = \"Difference in differences for NSW\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](causality-04-time_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndidreg <- lmer(\n  re ~ treat * post + (1 | id), \n  data = nsw_long, \n  REML = FALSE\n)\n\ndidreg\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: re ~ treat * post + (1 | id)\n   Data: nsw_long\n      AIC       BIC    logLik  deviance  df.resid \n 29058.10  29089.75 -14523.05  29046.10      1438 \nRandom effects:\n Groups   Name        Std.Dev.\n id       (Intercept) 2212    \n Residual             5230    \nNumber of obs: 1444, groups:  id, 722\nFixed Effects:\n(Intercept)        treat         post   treat:post  \n    3026.68        39.42      2063.37       846.89  \n```\n:::\n\n```{.r .cell-code}\nols <- lm(re ~ treat*post, data = nsw_long)\n\nm2 <- feols(re ~ treat*post | id, data = nsw_long)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThe variable 'treat' has been removed because of collinearity (see $collin.var).\n```\n:::\n\n```{.r .cell-code}\nmodelsummary::modelsummary(\n  list(lmer = didreg, lm = ols, feols = m2),\n  gof_map = c(\"nobs\", \"icc\")\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> lmer </th>\n   <th style=\"text-align:center;\"> lm </th>\n   <th style=\"text-align:center;\"> feols </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 3026.683 </td>\n   <td style=\"text-align:center;\"> 3026.683 </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (275.435) </td>\n   <td style=\"text-align:center;\"> (275.817) </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> treat </td>\n   <td style=\"text-align:center;\"> 39.415 </td>\n   <td style=\"text-align:center;\"> 39.415 </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (429.447) </td>\n   <td style=\"text-align:center;\"> (430.043) </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> post </td>\n   <td style=\"text-align:center;\"> 2063.366 </td>\n   <td style=\"text-align:center;\"> 2063.366 </td>\n   <td style=\"text-align:center;\"> 2063.366 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (358.761) </td>\n   <td style=\"text-align:center;\"> (390.065) </td>\n   <td style=\"text-align:center;\"> (324.753) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> treat × post </td>\n   <td style=\"text-align:center;\"> 846.888 </td>\n   <td style=\"text-align:center;\"> 846.888 </td>\n   <td style=\"text-align:center;\"> 846.888 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (559.365) </td>\n   <td style=\"text-align:center;\"> (608.173) </td>\n   <td style=\"text-align:center;\"> (581.796) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> SD (Intercept id) </td>\n   <td style=\"text-align:center;\"> 2211.709 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1px\"> SD (Observations) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 5229.795 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 1444 </td>\n   <td style=\"text-align:center;\"> 1444 </td>\n   <td style=\"text-align:center;\"> 1444 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ICC </td>\n   <td style=\"text-align:center;\"> 0.2 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSlide 82 add more adjustment variables and see that the estimates remain the same\n\nNote. Using within-person variation is a powerful strategy, but it does not allow for making generalizations about \"the overall effect\" of something---i.e., it's the effect *conditional* on the fact that there *is* within person variation. We can't generalize automatically to groups of people that don't have within person variance.\n\n## Regression Discontinuity?\n\n## Additional Resources\n\nDifference-in-Differences\n\n:   @goodman-bacon2021, @callaway2021, @santanna2020, @sun2021, @dechaisemartin2020\n\nRegression Discontinuity\n\n:   @gelman2019, @gelman2015\n\n## Extra\n\nBetween and within variation.\n\nTwo typical situations:\n\n-   clusters (e.g., students nested in classrooms).\n\n-   repeated observations (i.e., observations nested in subjects)\n\nfixed effects is like your mom saying \"you should not compare yourself to other people\"\n\nfixed effects driven estimates might not generalize to people who don't change\n\nintra-class correlation $\\to$ variance partitioning coefficient\n\nwhen there is no within-person variation (what year were you born?), vpc = 100%\n\naverages predicting averages is not the same as fluctuations predicting fluctuations\n\nExtending the mixed model (slide 131)\n\nWithin-between hybrid model\n\n$$\ny_{it} = \\gamma_0 + \\gamma_1 z_i + \\beta_W (x_{it} - \\bar x_i) + \\beta_B \\bar x_i + \\theta_t + \\alpha_i + \\epsilon_{it}\n$$\n\nby construction, $\\alpha_i$ and $(x_{it} - \\bar x_i)$ are independent\n\nCorrelated random effects (contextual) model\n\n$$\ny_{it} = \\gamma_0 + \\gamma_1 z_i + \\beta_W x_{it} + \\beta_C \\bar x_i + \\theta_t + \\alpha_i + \\epsilon_{it}\n$$\n\nNote $\\beta_C = \\beta_B - \\beta_W$\n\nBoth fit the data equally well, but the interpretation of the coefficients is slightly different.\n\n$\\beta_W$ is literally the same thing you get when using \"fixed effects\"\n\nSchunck and Perales Within and between cluster effects in generalized linear mixed models\n",
    "supporting": [
      "causality-04-time_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}