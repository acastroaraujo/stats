[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nIt contains my notes on various things related to applied statistics."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Some general remarks about statistics and sociology"
  },
  {
    "objectID": "00-regression.html",
    "href": "00-regression.html",
    "title": "Regression",
    "section": "",
    "text": "add stuff here."
  },
  {
    "objectID": "std-errors.html",
    "href": "std-errors.html",
    "title": "2  Standard Errors",
    "section": "",
    "text": "The sampling distribution of regression coefficients follows a normal distribution.\nIn the case of one predictor \\(X\\):\n\\[\n\\begin{align}\nY &\\sim \\text{Normal}(\\beta_0 + \\beta_1 X, \\ \\sigma^2) \\\\\\\\\n\\hat{\\beta_i} &\\sim \\text{Normal}(\\beta_i, \\text{SE} (\\hat \\beta_i)^2)\\\\\\\\\n\\text{SE}(\\hat \\beta_1) &= \\sqrt{\\frac{\\hat \\sigma^2}{\\sum_{i=1}^n (x - \\bar x)^2}} = \\sqrt{\\frac{\\hat \\sigma^2}{\\text{var}(X) \\times n}}\n\\end{align}\n\\]\n\nThe standard deviation of a sampling distribution is often referred to as a standard error.\n\nIn the case of multiple predictors (\\(\\mathbf X\\)) and coefficients (\\(\\boldsymbol \\beta\\)):\n\\[\n\\begin{align}\nY &\\sim \\text{Normal}(\\boldsymbol{X \\beta}, \\sigma^2 \\boldsymbol I) \\\\\n\\boldsymbol{\\hat \\beta} &\\sim \\text{Normal}(\\boldsymbol \\beta, \\boldsymbol \\Sigma) \\\\\n\\boldsymbol \\Sigma &=\\widehat{\\text{var}} \\hat{(\\boldsymbol \\beta)} = \\hat \\sigma^2 (\\boldsymbol{X^\\top X})^{-1}\n\\end{align}\n\\]\nThis is what allows us to do hypothesis testingTM on regression coefficients.\nWe calculate the standard error because we want to know the standard deviation of that sampling distribution. But our standard errors are probably always wrong.\nTwo assumptions:\n\n\\(\\varepsilon\\) is normally distributed.\n\\(\\varepsilon\\) is independent and identically distributed.\n\nThese assumptions are most obviously false when dealing with time series or data that’s geographically clustered (i.e., temporal or spatial autocorrelation).\nThere could also be heteroskedasticity—i.e., the variance of \\(\\varepsilon\\) is related to other variables in the model.\nFixing Standard Errors\n\nHeteroskedasticity-robust sandwich estimator (e.g., Huber-White). We weight observations with big residuals more when calculating the variance.\n\\[\n(\\boldsymbol{X^\\top X})^{-1}(\\boldsymbol{X^\\top \\Sigma X}) (\\boldsymbol{X}^\\top X)^{-1}\n\\]\nHeteroskedasticity and Autocorrelation Consistent (HEC) standard errors (e.g., Newey-West).\nClustered standard errors. Why not go with multilevel models instead?\nBootstrap methods.\n\n\nSandwich estimators leave the coefficient estimates (\\(\\boldsymbol{\\hat \\beta}\\)) intact!\n\nWhy do economists care so much about standard errors? They have a toxic culture that values “gotcha!” moments during conference presentations.\nWe can use the estimatr package to easily calculate these robust standard errors.\n\nlibrary(estimatr)\ndata(restaurant_inspections, package = \"causaldata\")\n\nm1 <- lm(inspection_score ~ Year + Weekend, \n         data = restaurant_inspections)\n\nm2 <- lm_robust(inspection_score ~ Year + Weekend, \n          data = restaurant_inspections, \n          se_type = \"HC1\"\n)\n\nmodelsummary::msummary(list(m1, m2), gof_omit = \".\")\n\n\n\n \n  \n      \n     (1) \n      (2) \n  \n \n\n  \n    (Intercept) \n    185.380 \n    185.380 \n  \n  \n     \n    (12.809) \n    (12.149) \n  \n  \n    Year \n    −0.046 \n    −0.046 \n  \n  \n     \n    (0.006) \n    (0.006) \n  \n  \n    WeekendTRUE \n    2.057 \n    2.057 \n  \n  \n     \n    (0.433) \n    (0.351) \n  \n\n\n\n\n\nThis is how the matrix algebra works:\n\nX <- model.matrix(m1)\nA <- solve(t(X) %*% X) \nr <- m1$residuals\nB <- (t(X) %*% diag(r^2) %*% X)\nvarB <- A %*% B %*% A\nsqrt(diag(varB)) ## robust standard errors\n\n(Intercept)        Year WeekendTRUE \n12.14823297  0.00604299  0.35122580 \n\n\nSee King and Roberts (2015) for more.\n\n\n\n\nKing, Gary, and Margaret E. Roberts. 2015. “How Robust Standard Errors Expose Methodological Problems They Do Not Fix, and What to Do About It.” Political Analysis 23 (2): 159–79. https://doi.org/10.1093/pan/mpu015."
  },
  {
    "objectID": "marginal-effects.html",
    "href": "marginal-effects.html",
    "title": "3  Marginal Effects",
    "section": "",
    "text": "https://theeffectbook.net/ch-StatisticalAdjustment.html#eq:regression-15\nMarginal effect at the mean. Icky.\nAverage Marginal effects."
  },
  {
    "objectID": "causality-00-intro.html#notation",
    "href": "causality-00-intro.html#notation",
    "title": "Causality",
    "section": "Notation",
    "text": "Notation\nThe difference between seeing and doing is enormous. It is the reason why we do not regard movements in a thermostat to be a cause of the temperature outside.\n\\[\n\\Pr(Y \\mid T) \\neq \\Pr(Y \\mid do(T))\n\\]\n\nPearl\n\nPotential Outcomes\n\n\\(T\\) is a binary treatment variable. The terms “treatment” and “cause” are used interchangeably.\n\\(Y\\) is the outcome we observe.\n\\(Y^0\\) is the the value the outcome would take if \\(T=0\\).\n\\(Y^1\\) is the value the outcome would take if \\(T=1\\).\n\\(Y^0\\) and \\(Y^1\\) are the potential outcomes.\nWe see \\(Y^0\\) or \\(Y^1\\) for the same unit, but never both. This is the fundamental problem of causal inference.\nWhen \\(T=1\\), \\(Y^0\\) is the counterfactual.\nWhen \\(T=0\\), \\(Y^1\\) is the counterfactual.\n\n\nRubin\n\nOne might object to the fundamental problem of causal inference by noting situations where we can actually measure both \\(Y^0\\) and \\(Y^1\\) on the same unit \\(i\\). For example, we could drink warm milk one evening and coffee another evening, and then measure our amount of sleep time. Holland (1986) refers to this as temporal stability and causal transience. This is the same situation in which we flick the same light switch on and off to figure out if it turns on the light to a room. In other settings, it is perfectly reasonable to assume that \\(Y^1_i = Y^1_j\\) and that \\(Y^0_i = Y^0_j\\) (unit homogeneity assumption). For example, this is very common in laboratories that take special care in standardizing the units of an experiment (e.g., mice).\n\nThese objections cannot usually be made in most social science applications.\nHowever, matched pair experiments of identical twins come close to this homogeneity assumption."
  },
  {
    "objectID": "causality-00-intro.html#identification-and-causal-diagrams",
    "href": "causality-00-intro.html#identification-and-causal-diagrams",
    "title": "Causality",
    "section": "Identification and Causal Diagrams",
    "text": "Identification and Causal Diagrams\nIdentification refers to the idea of identifying the effect of a treatment (or cause) on an outcome.\nCausal diagrams or DAGs are graphical representations of a data generating process. Everything we draw is hopefully an informed assumption; everything that’s not in the diagram is also an assumption. In other words, DAGs encode identifying assumptions.\n\nPaths.\nDirect effects.\nIndirect effects.\nTotal effects.\nFront door paths. They point away from the treatment. If we want to estimate “total effects,” then we want to keep all of them open.\nBack door paths. They point towards the treatment; we want to close them off.\nConfounding. A property of paths, not of variables.\nColliders. A variable is a collider in a path iff both arrows point at it.\n\\[\na \\to b \\to c_\\text{ollider} \\leftarrow d \\leftarrow e \\to f\n\\]\nHere, \\(b\\) and \\(c\\) are unrelated unless we remove variation in \\(c\\).\nIn other words, we close paths by removing variation from one variable along the path (i.e., adjusting); but if the variable is a collider, then removing variation will actually open a path that was already closed.\nAn often unacknowledged way of adjusting for colliders is during the sample selection phase. If we have a sample of college students, it means we are adjusting for college attendance.\nOpen Path. A path in which there is variation in all variables along the path (and no variation in colliders).\nClosed Path. A path in which there is at least one variable with no variation (or a collider with variation).\n\nThe idea of a directed acyclical graph (DAG) implies that there are no cycles. If a variable causes itself, it’s near impossible to isolate or identifying the cause of anything. The world is full with feedback loops of all sorts, but we deal with them through the incorporation of time or by isolating one effect through some kind of experimental scenario.\nNote that DAGs are agnostic about functional form. This includes interactions among variables! Some people deal with interactions by drawing arrows toward arrows or by representing interactions explicitly as separate nodes.\nThe nicest thing about DAGs is that they help us spell out the testable implications of our assumptions. For example, if our causal diagram implies that a relationship between variables is zero, we can check for that (this is called a placebo test).\n\nPlacebo test\nDon’t confuse with placebo effect!\n\nCausal diagrams also help us understand the idea the idea identification doesn’t necessarily require us to take care of all back door paths; sometimes we can use randomization. Sometimes this is achieved in experimental settings, but sometimes we can form some form of randomization in real-world settings (i.e., natural experiments). As depicted in Figure 1, experiments and instrumental variables have equivalent representations in DAGs.\n\nFinding front doors using randomization\n\n\n\n\nFigure 1: Randomization\n\n\n\nRandomness ensures that the path between unobserved \\(U\\) and the treatment is severed"
  },
  {
    "objectID": "causality-00-intro.html#additional-resources",
    "href": "causality-00-intro.html#additional-resources",
    "title": "Causality",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGeneral Resources\n\nHernan and Robins (2023), Imbens and Rubin (2015), Morgan and Winship (2014), Angrist and Pischke (2009), Ashworth, Berry, and Mesquita (2021)\n\nPotential Outcomes\n\nRubin (2005), Holland (1986)\n\nCausal Diagrams\n\nPearl (2009), Pearl, Glymour, and Jewell (2016)\n\nMachine Learning and Causal Inference\n\nDorie et al. (2019)\n\n\n\n\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless Econometrics. Princeton university press.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita. 2021. Theory and Credibility: Integrating Theoretical and Empirical Social Science. Princeton University Press.\n\n\nBollen, Kenneth A. 1989. Structural Equations with Latent Variables. John Wiley & Sons.\n\n\nDorie, Vincent, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone. 2019. “Automated Versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition.” Statistical Science 34 (1): 43–68. https://doi.org/10.1214/18-STS667.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What If. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60. https://doi.org/10.2307/2289064.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. 1st edition. New York: Cambridge University Press.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals and Causal Inference: Methods and Principles for Social Research. 2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference. 2nd edition. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. 1st edition. Wiley.\n\n\nRubin, Donald B. 2005. “Causal Inference Using Potential Outcomes: Design, Modeling, Decisions.” Journal of the American Statistical Association 100 (469): 322331."
  },
  {
    "objectID": "causality-01-experiments.html#notation",
    "href": "causality-01-experiments.html#notation",
    "title": "4  Experiments",
    "section": "4.1 Notation",
    "text": "4.1 Notation\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\n\\[\nY^0, Y^1 \\perp T\n\\tag{4.1}\\]\n\nIgnorability\n\nThus, in expectation:\n\\[\n\\begin{align}\nE[Y^0 \\mid T = 0] &= E[Y^0 \\mid T = 1], \\\\\\\\\nE[Y^1 \\mid T = 0] &= E[Y^1 \\mid T = 1]\n\\end{align}\n\\tag{4.2}\\]\n\n…ignorability implies that the value of someone’s potential outcomes does not provide any information about his or her treatment group assignment”.\nGelman, Hill, and Vehtari (2020, pp. 350–1)\n\nWhich means that we can easily estimate the average causal effect of \\(T\\) over all units in a population:\n\\[\n\\text{ATE} = \\underbrace{E[Y^1 - Y^0] = E[Y^1] - E[Y^0]}_\\text{by linearity of expectations}\n\\tag{4.3}\\]\nIn the absence of randomization, so that treatment and control groups differ on pre-treatment characteristics, we might observe the following biases:\n\nBaseline bias. The two groups might be different from each other whether they get treated or not.\nTreatment effect heterogeneity. The two groups might respond differently to the treatment.\nThis is important because researchers sometimes assume a constant effect for every unit in the population. This is implied in the unit homogeneity assumption. But if the variability of causal effects is large across the population, then the ATE might not represent the causal effect of a specific unit very well; the ATE might be irrelevant, no matter how carefully we estimate it.\n\nThree types of treatment effects:\n\n\\(\\text{ATE}\\), for all units (effect of switching)\n\\(\\text{ATT}\\), for treated units (effect of taking away treatment)\n\\(\\text{ATC}\\) or \\(\\text{ATU}\\), for untreated units (effect of adding treatment)\n\n\nSome times we see these terms pre-fixed with an \\(S\\) (for sample) or a \\(P\\) (for population).\n\nTable 4.1 allows us to gain further intuition on these calculations with a hypothetical example in which people are assigned college degrees randomly.\n\n\nTable 4.1: Potential Outcomes Example\n\n\nGroup (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\nCollege Degree (\\(T = 1\\))\n1000\n600*\n\n\nNo Degree (\\(T = 0\\))\n800*\n500\n\n\n\n\n\n\nSimple experiments of these kind are obviously impossible in the social sciences.\n* means unobservable\n\nIf 30% of the population has a degree…\n\n\n\nWhat is the naive estimate?\n500\nWhat is the ATT?\n400\n\n\n\n\nWhat is the ATC?\n300\nWhat is the ATE?\n\\(0.3\\times 400 + 0.7 \\times 300 = 330\\)\n\n\n\nMore than two treatment levels, continuous treatments, and multiple treatment factors\nMultiple treatment effects can be defined relative to a baseline level, following the general principles of regression modeling with indicator (or dummy) variables.\nTreatment levels can be continuous.\n\nTo conceptualize randomization with a continuous treatment, think of spinning a spinner that can land on any of the potential levels of the treatment assignment. As with regression inputs in general, it can make sense to fit more complicated models as suggested by theory or supported by data. A linear model—which estimates the average effect on \\(y\\) for each additional unit of \\(z\\)—is a natural starting point for effects that are believed to be monotonically increasing or decreasing functions of the treatment level.\nGelman, Hill, and Vehtari (2020, 342)\n\nFinally, we can also consider multiple simultaneous treatments.\n\n…multiple treatments can be administered in combination. For instance, depressed individuals could be randomly assigned to receive nothing, drugs, counseling sessions, or both drugs and counseling sessions. These combinations could be modeled as two treatments and their interaction or as four distinct treatments.\nGelman, Hill, and Vehtari (2020, 342)\n\nUsing design and analysis to address imbalance and lack of overlap between treatment and control groups\nRegression can be useful even in the context of randomized experiments.\n\nIn practice, we can never ensure that treatment and control groups are balanced on all relevant pre-treatment characteristics. However, there are statistical approaches that may bring us closer. At the design stage, we can use randomization to ensure that treatment and control groups are balanced in expectation, and we can use blocking to reduce the variation in any imbalance. At the analysis stage, we can adjust for pre-treatment variables to correct for differences between the two groups to reduce bias in our estimate of the sample average treatment effect. We can further adjust for differences between sample and population if our goal is to estimate the population average treatment effect.\nGelman, Hill, and Vehtari (2020, 344)\n\nWe can increase the precision of treatment effect estimates by adjusting for pre-treatment variables that are predictive of the outcome—i.e., we get lower standard errors on the treatment parameter.\nGroup or cluster-randomized experiments\nSometimes it’s difficult to randomize treatment assignments to individual units, and so we might randomize groups or clusters to receive the treatment instead.\n\nA decision to assign treatments at the group level can be driven by cost or logistical concerns. It might be more cost effective to provide free flu shots to a random subset of health clinics, for example, than to have professionals go to every clinic and then randomly assign individuals to receive shots. Assignment at the clinic level would also avoid creating ill will among potential study participants being deprived of a service that others in the same location are able to receive. Cluster-randomized experiments are also used to avoid spillover or contagion effects (which can also be considered as violations of the stable unit treatment value assumption or SUTVA).\nGelman, Hill, and Vehtari (2020, 349–50)"
  },
  {
    "objectID": "causality-01-experiments.html#sutva",
    "href": "causality-01-experiments.html#sutva",
    "title": "4  Experiments",
    "section": "4.2 SUTVA",
    "text": "4.2 SUTVA\nBesides ignorability (see ?eq-ignore), randomization implies certain other properties which we might consider to be assumptions.\nThe most important one is the stable unit treatment value assumption (SUTVA). It means that we assume that there is no interference among units (i.e., no spillovers) and no hidden versions of the treatment.\nThus, one can imagine person \\(i\\)’s outcome to be “a function not only of her own treatment assignment, but also the treatment assignments of others in the sample” (Gelman, Hill, and Vehtari 2020, 353). These becomes intractable very quickly. For example, in a sample of just 10 people and a binary treatment, we would have 210 = 1024 potential outcomes for each person. Thus, researchers often hope that there is no interference among units, or else they think about modeling spillover in some way.\nSUTVA also implies that there are no hidden versions of treatments—i.e., we want \\(t_i\\) to equal \\(t_j\\).\n\nExamples of potential SUTVA violations abound. An experiment testing the effect of a new fertilizer by randomly assigning adjacent plots to treatment or control is a classic example. Fertilizer from one plot might leach into an adjacent plot assigned to receive no fertilizer and thus affect the yield in that control plot. Vaccines that reduce the probability of a contagious disease within a school, business, or community could easily lead to violation of SUTVA if the vaccine is actually effective. Consider an experiment that recruited families from the same public housing complex and randomized them to receive a voucher to move to a better neighborhood or not. This could suffer from interference if a given family moving might influence (positively or negatively) the well-being of another family that happened to be randomized to not receive the voucher.\nGelman, Hill, and Vehtari (2020, 353)\n\nIn educational settings, potential SUTVA violations are often a reason to assign treatments at the classroom or school level."
  },
  {
    "objectID": "causality-01-experiments.html#extra",
    "href": "causality-01-experiments.html#extra",
    "title": "4  Experiments",
    "section": "4.3 Extra",
    "text": "4.3 Extra\n\nlibrary(tidyverse)\ntheme_set(\n  theme_light(base_family = \"Avenir Next Condensed\") +\n  theme(strip.background = element_rect(fill = \"#666666\"))\n)\n\n\n4.3.1 Average Treatment Effects\n\n18.4 The table below describes a hypothetical experiment on 8 people. Each row of the table gives a participant and her pre-treatment predictor \\(x\\), treatment indicator \\(z\\), and potential outcomes \\(y^0\\) and \\(y^1\\).\n\n\n\nCode\nd <- data.frame(\n  X = c(3, 5, 2, 8, 5, 10, 2, 11),\n  Z = c(0, 0, 1, 0, 0, 1, 1, 1),\n  Y0 = c(5, 8, 5, 12, 4, 8, 4, 9),\n  Y1 = c(5, 10, 3, 13, 2, 9, 1, 13), \n  row.names = LETTERS[1:8]\n)\n\nd <- d |> \n  mutate(Y = ifelse(Z == 0, Y0, Y1))\n\nknitr::kable(d)\n\n\n\n\n\n\nX\nZ\nY0\nY1\nY\n\n\n\n\nA\n3\n0\n5\n5\n5\n\n\nB\n5\n0\n8\n10\n8\n\n\nC\n2\n1\n5\n3\n3\n\n\nD\n8\n0\n12\n13\n12\n\n\nE\n5\n0\n4\n2\n4\n\n\nF\n10\n1\n8\n9\n9\n\n\nG\n2\n1\n4\n1\n1\n\n\nH\n11\n1\n9\n13\n13\n\n\n\n\n\nNaive estimate:\n\ni <- as.logical(d$Z)\n\n## Naive Estimate: \nmean(d$Y1[i] - d$Y0[!i])\n\n[1] -0.75\n\n## ATT:\nmean(d$Y1[i] - d$Y0[i])\n\n[1] 0\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\n\nSimulate a new completely randomized experiment on these 8 people; that is, resample \\(z\\) at random with the constraint that equal numbers get the treatment and the control.\n\n\nj <- sample(i)\n\n## Naive Estimate: \nmean(d$Y1[j] - d$Y0[!j])\n\n[1] -1\n\n## ATT:\nmean(d$Y1[j] - d$Y0[j])\n\n[1] 0.25\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\nAgain:\n\nk <- sample(i)\n\n## Naive Estimate: \nmean(d$Y1[k] - d$Y0[!k])\n\n[1] -1\n\n## ATT:\nmean(d$Y1[k] - d$Y0[k])\n\n[1] 0.25\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\n\n\n4.3.2 The Electric Company\nAdd description of dataset. This is a randomized experiment.\n\n\n\n\n\n\n\nCode\nurl <- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\n\nd <- read_csv(url)[, -1] |> \n  mutate(across(grade:pair_id, as.integer))\n\nglimpse(d)\n\n\nRows: 192\nColumns: 6\n$ post_test <dbl> 48.9, 70.5, 89.7, 44.2, 77.5, 84.7, 78.9, 86.8, 60.8, 75.7, …\n$ pre_test  <dbl> 13.8, 16.5, 18.5, 8.8, 15.3, 15.0, 19.4, 15.0, 11.8, 16.4, 1…\n$ grade     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ treatment <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ supp      <int> 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, …\n$ pair_id   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n\n\nThe supp variable indicates a subtlety in the experiment—i.e., every teacher had the choice of replacing or supplementing the regular reading program with the television show. Thus, this experiment really estimated the effect of making the program available.\n\nThe notebook on regression adjustments and causal inference revisits the supp variable as a different treatment on its own.\n\n\n\nCode\nd |> \n  mutate(\n    grade = paste(\"Grade\", grade),\n    t = ifelse(treatment == 1, \"Treatment\", \"Control\")) |> \n  ggplot(aes(post_test, y = grade, color = t)) + \n  stat_summary(size = 1/8, fun.args = list(mult = 2), position = position_dodge(1/3)) +\n  labs(y = NULL, color = NULL) + \n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThe point of this exercise is to demonstrate that it’s almost always a good idea to include pre-treatment information when analyzing experimental data.\n\nUnder a clean randomization, adjusting for pre-treatment predictors in this way does not change what we are estimating. However, if the predictor has a strong association with the outcome it can help to bring each estimate closer (on average) to the truth, and if the randomization was less than pristine, the addition of predictors to the equation may help us adjust for systematically unbalanced characteristics across groups. Thus, this strategy has the potential to adjust for both random and systematic differences between the treatment and control groups (that is, to reduce both variance and bias), as long as these differences are characterized by differences in the pre-test.\nGelman, Hill, and Vehtari (2020, 368)\n\nJust look at the change in coefficient and standard error estimates:\n\nmod1 <- lm(post_test ~ treatment, data = d)\nmod2 <- lm(post_test ~ treatment + pre_test, data = d)\nmod3 <- lm(post_test ~ treatment + pre_test + grade, \n           data = mutate(d, grade = factor(grade)))\n\nmodelsummary::msummary(\n  models = list(mod1, mod2, mod3), \n  gof_map = NA\n)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    94.321 \n    61.558 \n    58.456 \n  \n  \n     \n    (1.794) \n    (1.471) \n    (1.485) \n  \n  \n    treatment \n    5.657 \n    4.734 \n    4.052 \n  \n  \n     \n    (2.537) \n    (1.160) \n    (1.063) \n  \n  \n    pre_test \n     \n    0.460 \n    0.800 \n  \n  \n     \n     \n    (0.017) \n    (0.055) \n  \n  \n    grade2 \n     \n     \n    −21.722 \n  \n  \n     \n     \n     \n    (3.504) \n  \n  \n    grade3 \n     \n     \n    −29.846 \n  \n  \n     \n     \n     \n    (4.666) \n  \n  \n    grade4 \n     \n     \n    −32.870 \n  \n  \n     \n     \n     \n    (5.242) \n  \n\n\n\n\n\nNote. The grade coefficients are increasingly negative because they are positively correlated with pre_test. Remember, we can’t expect to interpret these coefficients causally (Westreich and Greenland 2013).\nAnd here’s a separate model for each grade:\n\n\nCode\nfit1 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ treatment, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy(conf.int = TRUE) |> \n    filter(term == \"treatment\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit2 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ treatment + pre_test, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy(conf.int = TRUE) |> \n    filter(term == \"treatment\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit1 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ treatment)\")\n\nfit2 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ treatment + pre_test)\")\n\n\n\n\n\n\n\n\n(a) Regression on treatment indicator\n\n\n\n\n\n\n\n(b) Regression on treatment indicator and pre-test\n\n\n\n\nFigure 4.1: Estimates, 50%, and 95% intervals for the effect of watching The Electric Company. The same model has been fitted separately for each grade.\n\n\n\nSo, adding pre-treatment information will always make for more precise and less biased estimates. But don’t adjust for post-treatment variables. In fact, always draw a DAG.\nAdjusting for a post-treatment variable \\(q\\) breaks down the assumption of ignorability.\n\\[\nY^0, Y^1 \\not \\perp T \\mid q\n\\]"
  },
  {
    "objectID": "causality-01-experiments.html#additional-resources",
    "href": "causality-01-experiments.html#additional-resources",
    "title": "4  Experiments",
    "section": "4.4 Additional Resources",
    "text": "4.4 Additional Resources\n\nExperiments\n\nDruckman (2022), Gerber and Green (2012)\n\nInterference Among Units\n\nRosenbaum (2007), Hudgens and Halloran (2008), Aronow and Samii (2017).\n\nPlacebo effects\n\nMeissner et al. (2011), Beecher (1955)\n\n\n\n\n\n\nAronow, Peter M., and Cyrus Samii. 2017. “Estimating Average Causal Effects Under General Interference, with Application to a Social Network Experiment.” The Annals of Applied Statistics 11 (4): 1912–47. https://doi.org/10.1214/16-AOAS1005.\n\n\nBeecher, Henry K. 1955. “The Powerful Placebo.” Journal of the American Medical Association 159 (17): 16021606.\n\n\nDruckman, Jamie. 2022. Experimental Thinking. Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: W. W. Norton & Company.\n\n\nHudgens, Michael G., and M. Elizabeth Halloran. 2008. “Toward Causal Inference with Interference.” Journal of the American Statistical Association 103 (482): 832842.\n\n\nMeissner, Karin, Ulrike Bingel, Luana Colloca, Tor D. Wager, Alison Watson, and Magne Arve Flaten. 2011. “The Placebo Effect: Advances from Different Methodological Approaches.” Journal of Neuroscience 31 (45): 1611716124.\n\n\nRosenbaum, Paul R. 2007. “Interference Between Units in Randomized Experiments.” Journal of the American Statistical Association 102 (477): 191200.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177 (4): 292–98. https://doi.org/10.1093/aje/kws412."
  },
  {
    "objectID": "causality-02-regression.html#translating-dags",
    "href": "causality-02-regression.html#translating-dags",
    "title": "5  Regression Adjustments",
    "section": "5.1 Translating DAGs",
    "text": "5.1 Translating DAGs\n\n\n\nFigure 5.1: Flow Chart For Constructing Regression Equations (Huntington-Klein 2021, 199)\n\n\nSuppose our causal diagram is as follows:\n\n\n\n\n\nFurther suppose that the true data generating process is as follows:\n\\[\ny_i = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i + \\varepsilon_i\n\\]\nIf we omit \\(z\\), then our regression is as follows:\n\\[\ny_i = \\beta_0^* + \\beta_1^* t_i+ \\varepsilon^*_i\n\\]\nFinally, suppose that the association between \\(z\\) and \\(t\\) is defined in terms of a third regression:\n\\[\nz_i = \\gamma_0 + \\gamma_1 t_i + u_i\n\\]\nIf we substitute this representation of \\(t\\) into the original “true” regression and rearrange the terms, we then have the following:\n\\[\ny_i = \\underbrace{(\\beta_0 + \\beta_2 \\gamma_0)}_{\\beta_0^*} + \\underbrace{(\\beta_1 + \\beta_2 \\gamma_1)}_{\\beta_1^*} \\ t_i + \\underbrace{(\\beta_2 u + \\varepsilon)}_{\\varepsilon^*}\n\\]\nThus, if there’s no association between the treatment and the purported confounder (\\(\\gamma_1 = 0\\)), then there is no bias.\nIn R:\n\nN <- 1e3\nb0 <- 1 \nb1 <- 2  ## effect of t\nb2 <- 3\ng0 <- 2\ng1 <- 1  ## source of correlation between t and z\n\nd <- tibble::tibble(\n  z = runif(N, min = -50, max = 50),\n  t = rnorm(N, mean = (z - g0)/g1, sd = 1),\n  y = rnorm(N, mean = b0 + b1*t + b2*z, sd = 1)\n)\n\nmtrue <- lm(y ~ t + z, data = d)\nmconf <- lm(y ~ t, data = d)\n\nmodelsummary::msummary(\n  models = list(\"True DGP\" = mtrue, \"Omitted Variable\" = mconf), \n  gof_map = NA\n)\n\n\n\n \n  \n      \n    True DGP \n     Omitted Variable \n  \n \n\n  \n    (Intercept) \n    1.021 \n    6.963 \n  \n  \n     \n    (0.074) \n    (0.098) \n  \n  \n    t \n    2.010 \n    4.994 \n  \n  \n     \n    (0.033) \n    (0.003) \n  \n  \n    z \n    2.989 \n     \n  \n  \n     \n    (0.033) \n     \n  \n\n\n\n\n\nHere, we see that \\(\\beta_1 \\approx 2\\) and \\(\\beta_1^* \\approx 5\\).\n\n5.1.1 The Electric Company\nSetting up a regression for causal inference\nGoing back to the Electric Company example, we can think of supp, whether teachers chose to replace or supplementing the regular reading program with the television show, as a different treatment.\nGiven that this decision is not randomized, we cannot simply compare outcomes across the two new treatment groups. However, we can assume ignorability if we are willing to believe that pre_test is the only confounding variable—i.e., that the probability of assignment was determined by the average pre-test scores in that classroom.\nThis is a strong assumption, it requires a leap of faith.\n\n\nSetup\nlibrary(tidyverse)\n\ntheme_set(\n  theme_light(base_family = \"Avenir Next Condensed\") +\n  theme(strip.background = element_rect(fill = \"#666666\"))\n)\n\nurl <- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\n\nd <- read_csv(url)[-1] |> \n  mutate(across(grade:pair_id, as.integer)) |> \n  filter(treatment == 1)\n\n\nJust look at the change in coefficient and standard error estimates:\n\nmod1 <- lm(post_test ~ supp, data = d)\nmod2 <- lm(post_test ~ supp + pre_test, data = d)\nmod3 <- lm(post_test ~ supp + pre_test + grade, \n           data = mutate(d, grade = factor(grade)))\n\nmodelsummary::msummary(\n  models = list(mod1, mod2, mod3), \n  gof_map = NA\n)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    98.189 \n    66.758 \n    61.964 \n  \n  \n     \n    (2.830) \n    (2.377) \n    (2.467) \n  \n  \n    supp \n    2.816 \n    3.468 \n    3.589 \n  \n  \n     \n    (3.550) \n    (1.796) \n    (1.658) \n  \n  \n    pre_test \n     \n    0.424 \n    0.820 \n  \n  \n     \n     \n    (0.026) \n    (0.093) \n  \n  \n    grade2 \n     \n     \n    −24.741 \n  \n  \n     \n     \n     \n    (5.992) \n  \n  \n    grade3 \n     \n     \n    −34.816 \n  \n  \n     \n     \n     \n    (7.613) \n  \n  \n    grade4 \n     \n     \n    −37.935 \n  \n  \n     \n     \n     \n    (8.832) \n  \n\n\n\n\n\nNote. The grade coefficients are increasingly negative because they are positively correlated with pre-test scores. Remember, we cannot interpret confounders causally(see Westreich and Greenland 2013).\nAnd here’s a separate model for each grade:\n\n\nCode\nfit1 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ supp, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy() |> \n    filter(term == \"supp\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit2 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ supp + pre_test, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy() |> \n    filter(term == \"supp\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit1 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ supp)\")\n\nfit2 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ supp + pre_test)\")\n\n\n\n\n\n\n\n\n(a) Regression on non-random treatment indicator\n\n\n\n\n\n\n\n(b) Regression on non-random treatment indicator and pre-test\n\n\n\n\nFigure 5.2: Estimates, 50%, and 95% intervals for the effect of watching The Electric Company as a supplement rather than a replacement. The same model has been fitted separately for each grade."
  },
  {
    "objectID": "causality-02-regression.html#notation",
    "href": "causality-02-regression.html#notation",
    "title": "5  Regression Adjustments",
    "section": "5.2 Notation",
    "text": "5.2 Notation\nThe logic of regression adjustments is relatively straightforward, we want apples to apples comparisons. This is the same logic behind matching and weighting.\nInstead of a simple independence assumption that we have for randomized experiments, we now have to rely on a conditional ignorability. Just like in the case of experiments, we want distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator should be independent, conditional on the covariates \\(\\boldsymbol{X}\\) used in the analysis.\n\nIgnorability, Conditional Independence Assumption (CIA), or Selection on Observables\n\n\\[\nY^0, Y^1 \\perp T \\mid \\boldsymbol X\n\\tag{5.1}\\]\nThus, in expectation:\n\\[\n\\begin{align}\nE\\big[Y^1 \\mid T=1, \\boldsymbol{X} \\big] &= E\\big[Y^1 \\mid T=0, \\boldsymbol{X} \\big] \\\\\nE\\big[Y^0 \\mid T=1, \\boldsymbol{X} \\big] &= E\\big[Y^0 \\mid T=0, \\boldsymbol{X} \\big]\n\\end{align}\n\\tag{5.2}\\]\nThese conditional quantities can be used to construct the marginal effects needed to construct the average treatment effect.\n\\[\n\\text{ATE} = E[Y^1 - Y^0] = \\underbrace{E\\Big[E[Y^1 \\mid \\boldsymbol{X}] \\Big] - E \\Big[E[Y^0 \\mid \\boldsymbol{X}] \\Big] = E[Y^1] - E[Y^0]}_\\text{Law of Iterated Expectations}\n\\tag{5.3}\\]\nThis strategy will get more complicated as the vector \\(\\boldsymbol X\\) grows in size.\n\nOnce the number and type of confounders gets more complicated, perhaps the simplest parametric model that we can fit to estimate these expectations is linear regression. For instance, if we assume that the treatment effect, \\(\\tau\\), is constant (or at least additive) we might posit that \\(E( Y^Z \\mid \\boldsymbol X) = \\beta_0+ \\boldsymbol{X \\beta}+ \\tau Z\\) . If ignorability is satisfied and this model holds, we simply need to regress the outcome on the treatment indicator and confounders. The estimated coefficient on \\(Z\\) from this fit, \\(\\hat \\tau\\), can be conceptualized as a weighted version of all of the conditional effect estimates. However, fitting a model to estimate these quantities is not without potential weaknesses. The two most obvious concerns are imbalance and lack of complete overlap…\nGelman, Hill, and Vehtari (2020, 390)\n\nNote. This sometimes shows up in econometrics as the exogeneity assumption."
  },
  {
    "objectID": "causality-02-regression.html#balance-and-overlap",
    "href": "causality-02-regression.html#balance-and-overlap",
    "title": "5  Regression Adjustments",
    "section": "5.3 Balance and Overlap",
    "text": "5.3 Balance and Overlap\nThe sort of bias that we get from confounding can be interpreted more precisely as imbalance in the potential outcomes across treatment groups. This is the sort of imbalance is unlikely with randomization, but it’s almost guaranteed in observational studies.\nThere are different methods that attempt to address imbalance and lack of overlap: stratification, regression adjustments, matching, weighting, or a combination of these.\n\nImbalance and lack of complete overlap are issues for causal inference even if ignorability holds because they force us to rely more heavily on model specification and less on direct support from the data.\nGelman, Hill, and Vehtari (2020, 391)\n\n\n5.3.1 Imbalance\nImbalance occurs if the distributions of confounders differ for the treatment and control groups. Figure 5.3 shows two examples of imbalance with respect to a single covariate \\(x\\). The problem with imbalance is that it forces us to rely more heavily on model specification and less on direct support from the data. This is true even if ignorability holds.\n\n\nCode\nggplot() + \n  xlim(0, 1) +\n  geom_function(fun = \\(x) dbeta(x, 5, 2)) +\n  geom_function(fun = \\(x) dbeta(x, 2, 5), linetype = \"dashed\") +\n  geom_vline(xintercept = c(0.4, 0.6), linetype = \"dotted\") +\n  labs(x = \"X\", y = NULL) +\n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) \n\nggplot() + \n  xlim(0, 1) +\n  geom_function(fun = \\(x) dbeta(x, 3, 2.2), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dbeta(x, 2.2, 3)) +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\") +\n  labs(x = \"X\", y = NULL) +\n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) \n\n\n\n\n\n\n\n\n(a) The groups differ in their averages but cover the same range of \\(x\\)\n\n\n\n\n\n\n\n(b) A more subtle form of imbalance, the groups have the same average but different distributions\n\n\n\n\nFigure 5.3: Imbalance in distributions across treatment and control groups\n\n\n\nConsider what happens if we try to make inferences about the effect of a treatment \\(\\theta\\) on \\(y\\), while adjusting for \\(x\\).\n\\[\n\\begin{align}\n\\text{treated:}  &&y_i &= \\beta_0 + \\theta + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i \\\\ \\\\\n\\text{control:} &&y_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i\n\\end{align}\n\\]\nAveraging over each group separately, solving the second equation for \\(\\beta_0\\), and then plugging that into the first equation to solve for \\(\\theta\\) yields the following equation:\n\\[\n\\theta = (\\overline y_t - \\overline y_c) - \\underbrace{\\beta_1 (\\overline x_t - \\overline x_c) - \\beta_2 (\\overline {x^2_t} - \\overline {x^2_c})}_\\text{adjustment}\n\\tag{5.4}\\]\nThis is why it’s so important to get the model specification right. For example, notice that if we don’t include the quadratic term in the linear model, the estimate of \\(\\theta\\) will be off by \\(\\beta_2 (\\overline {x^2_t} - \\overline {x^2_c})\\).\nMore importantly, if the distribution of covariates is similar across treatment groups, then the model specification matters less—i.e., if there is balance, then \\(\\overline x_t - \\overline x_c\\) is closer to zero. This is why methods that match or weight to create balance may help to create some immunity from failure to correctly specify the model.\n\n\n5.3.2 Lack of Complete Overlap\nFigure 5.4 shows what lack of complete overlap (with respect to \\(x\\)) might look like:\n\n\nCode\nggplot() + \n  xlim(-4, 10) + \n  geom_function(fun = \\(x) dnorm(x, -1, 1), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dnorm(x, 7, 1)) + \n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) + \n  labs(x = \"X\", y = NULL, title = \"No overlap\")\n\nggplot() + \n  xlim(-4, 10) + \n  geom_function(fun = \\(x) dnorm(x, 1, 1), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dnorm(x, 4, 1)) + \n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) + \n  labs(x = \"X\", y = NULL, title = \"Partial overlap\")\n\nggplot() + \n  xlim(-4, 10) + \n  geom_function(fun = \\(x) dnorm(x, 2, 3), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dnorm(x, 7, 1)) + \n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) + \n  labs(x = \"X\", y = NULL, title = \"Partial overlap\")\n\n\n\n\n\n\n\n\n(a) Two distributions with no overlap\n\n\n\n\n\n\n\n(b) Two distributions with partial overlap\n\n\n\n\n\n\n\n(c) The range of one distribution is a subset of the range of the other.\n\n\n\n\nFigure 5.4: Lack of complete overlap in distributions across treatment and control groups.\n\n\n\nLack of complete overlap or “common support” creates problems because in this setting we have treatment or control observations for which we have no empirical counterfactuals. Thus, knowledge about treatment effects is inherently limited in regions of non-overlap. Any causal inference in Figure 5.4 (a) would rely on modeling assumptions instead of having direct support from the data. In Figure 5.4 (c) causal inference is possible for the full treatment group but only for a subset of the control group.\n\nThere is an important correspondence between areas of overlap and the estimand of interest, and it can make sense to choose your inferential goal based on the support in the data.\nGelman, Hill, and Vehtari (2020, 392)\n\n\nATE, ATT, or ATU?\n\nFigure 5.5 shows how inferences regarding areas with no overlap inevitably rely on modeling assumptions. A traditional regression model fitted to data without complete overlap is forced to extrapolate beyond the support of the data.\nNote, however, that even the incorrectly specified linear regression lines provide a decent fit in the overlapping region, as shown in Figure 5.5 (c) and Figure 5.5 (d).\n\n\nCode\nfY1 <- function(x) exp(x/5) + 2*cos(x + 1) + 15\nfY0 <- function(x) 6*log(x + 1) - 0.08*x^2 + 5\n\nd_sim <- tibble(\n  x = runif(100, 0, 10),\n  control = rnorm(length(x), mean = fY0(x), sd = 1),\n  treatment = rnorm(length(x), mean = fY1(x), sd = 1),\n) |> \n  tidyr::pivot_longer(\n    cols = control:treatment, \n    names_to = \"z\", \n    values_to = \"y\", \n    names_transform = factor\n  ) |> \n  filter(z == \"control\" & x <= 7 | z == \"treatment\" & x >= 3)\n\npar_s <- lm(y ~ x + z, data = d_sim)\nvar_s <- lm(y ~ x*z, data = d_sim)\nsub_par_s <- lm(y ~ x + z, data = d_sim, subset = x >= 3 & x <= 7)\n\ng <- d_sim |> \n  ggplot(aes(x, y)) + \n  geom_function(fun = fY0) + \n  geom_function(fun = fY1) + \n  geom_point(aes(shape = z), show.legend = FALSE, \n             fill = \"skyblue\", color = \"#4C4C4C\", size = 1) + \n  scale_shape_manual(values = c(19, 21)) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) +\n  theme(\n    axis.title.y = element_text(angle = 0, vjust = 1/2),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) \n\nd_grid <- tidyr::crossing(\n  x = seq(0, 10, length.out = 100),\n  z = factor(c(\"control\", \"treatment\"))\n)\n\ng +\n  geom_line(\n    data = broom::augment(par_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\ng +\n  geom_line(\n    data = broom::augment(var_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\ng +\n  coord_cartesian(xlim = c(3, 7)) +\n    geom_line(\n    data = broom::augment(par_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\ng +\n  coord_cartesian(xlim = c(3, 7)) +\n  geom_line(\n    data = broom::augment(sub_par_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\n\n\n(a) A regression estimating the effect of treatment \\(T\\) on \\(Y\\), adjusting for \\(X\\).\n\n\n\n\n\n\n\n(b) Allowing for an interaction sometimes makes extrapolation worse.\n\n\n\n\n\n\n\n\n\n(c) The view is restricted to the area of overlap.\n\n\n\n\n\n\n\n(d) New regression lines fitted using only observations in this overlapping region.\n\n\n\n\nFigure 5.5: Hypothetical data demonstrating the problems with extrapolation when there is lack of overlap between covariates. The “true” causal effect is the vertical distance between the two solid lines. The estimated causal effect is the vertical distance between the two dashed lines.\n\n\n\nThe dataset from the example in Figure 5.6 comes from a sample of nearly 4500 children born in the 1980s. A subset of 290 received several healthcare interventions; these children were targeted because they were born prematurely, had low birth weight, and lived in the eight cities were the intervention took place. We can start checking for imbalance for several covariates by examining their absolute standardized difference in means—i.e., a balance plot. Figure 5.6 (b) displays the absolute standardized difference in means values for a set of confounding covariates that might predict both program participation and subsequent test scores.\n\n\nCode\nurl <- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Childcare/data/cc2.csv\"\nd <- read_csv(url)[-1]\nd$momage <- sqrt(d$momageT)\n\ncovs <- c(\n  'birth weight' = 'bw',\n  'weeks preterm' = 'preterm', \n  'days in hospital' = 'dayskidh', \n  'male' = 'sex', \n  'first born' = 'first', \n  'age' = 'age', \n  'black' = 'black', \n  'hispanic' = 'hispanic', \n  'white' = 'white', \n  'unmarried at birth' = 'b.marr', \n  'less than high school' = 'lths', \n  'high school graduate' = 'hs', \n  'some college' = 'ltcoll', \n  'college graduate' = 'college', \n  'worked during pregnancy' = 'work.dur', \n  'had no prenatal care' = 'prenatal', \n  'age at birth' = 'momage'\n)\n\nd |> \n  mutate(treat = fct_rev(factor(treat))) |> \n  filter(between(bw, 1500, 5000)) |> \n  ggplot(aes(bw, ppvtr.36)) +\n  geom_point(aes(color = treat), size = 2/3) + \n  geom_smooth(aes(linetype = treat), method = \"lm\", linewidth = 1/2,\n              fullrange = TRUE, se = FALSE, color = \"#323232\") + \n  scale_color_grey() + \n  labs(x = \"birth weight\", y = \"test score at age 3\", \n       color = \"treatment\", linetype = \"treatment\") +\n  theme(legend.position = \"bottom\")\n\n## treatment indicator\nt <- which(as.logical(d$treat))\n\navg1 <- apply(d[t, covs], 2, mean)\navg0 <- apply(d[-t, covs], 2, mean)\nsd1 <- apply(d[t, covs], 2, sd) ## only uses sd of treated\nz <- (avg1 - avg0) / sd1\n\ntibble(var = covs, diff = z, label = names(covs)) |> \n  mutate(label = reorder(label, abs(diff))) |> \n  ggplot(aes(abs(diff), label)) + \n  geom_segment(aes(xend = abs(diff), yend = label), x = 0) + \n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"Absolute Standardized Differences in Means\")\n\n\n\n\n\n\n\n\n(a) Regression lines of test scores on birth weight (separately by treatment groups)\n\n\n\n\n\n\n\n(b) Imbalance in averages of confounding covariates across treatment groups.\n\n\n\n\nFigure 5.6: Lack of complete overlap. If birth weight is a confounding covariate, then we will have to rely on model extrapolations to make inferences about the effect of the program on children with birth weights over 2500 grams.\n\n\n\nNote. Lack of overlap is not the same as imbalance.\nThe next notebook discusses matching/weighting as strategies that help us deal with imbalance and rely less on modeling assumptions to deal with data outside the area of common support."
  },
  {
    "objectID": "causality-02-regression.html#exercises",
    "href": "causality-02-regression.html#exercises",
    "title": "5  Regression Adjustments",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. New York: Chapman; Hall/CRC. https://doi.org/10.1201/9781003226055.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177 (4): 292–98. https://doi.org/10.1093/aje/kws412."
  },
  {
    "objectID": "causality-03-matching.html#exact-matching",
    "href": "causality-03-matching.html#exact-matching",
    "title": "6  Matching",
    "section": "6.1 Exact Matching",
    "text": "6.1 Exact Matching\nThe logic of exact matching is relatively straightforward, we want apples to apples comparisons. This is the same logic behind regression adjustements.\nInstead of a simple independence assumption that we have for randomized experiments, we now have to rely on a conditional independence assumption (CIA).\n\\[\nY^0, Y^1 \\perp T \\mid S\n\\]\nThus, in expectation:\n\\[\n\\begin{align}\nE\\big[Y^1 \\mid T=1, S \\big] &= E\\big[Y^1 \\mid T=0, S \\big] \\\\\nE\\big[Y^0 \\mid T=1, S \\big] &= E\\big[Y^0 \\mid T=0, S \\big]\n\\end{align}\n\\tag{6.1}\\]\n\nConditional Ignorability\n\nThe assumption in Equation 6.1 means that we can observe counterfactuals, conditional on pre-treatment variables \\(S\\). Unfortunately, we cannot test the conditional independence assumption directly.\nThree overall assumptions:\n\nIgnorability or selection on observables.\nOverlap—any individual case has a non-zero probability of treatment.\nStable Unit Treatment Value Assumption (SUTVA)—i.e., no spillovers.\n\nSEE IF ABOVE SHOULD STILL BE HERE\nNote. In cases when we have treatment effect heterogeneity, the ATT, ATU, and ATE estimates will be different, as exemplified in Table 6.1.\n\n\nTable 6.1: Treatment Effect Heterogeneity\n\n\nSES\nN\ndegree\nearnings\n\n\n\n\n1\n150\n0\n2,000\n\n\n1\n50\n1\n4,000\n\n\n2\n100\n0\n6,000\n\n\n2\n100\n1\n8,000\n\n\n3\n50\n0\n10,000\n\n\n3\n150\n1\n14,000\n\n\n\n\nWhat is our estimand (Lundberg, Johnson, and Stewart 2021)?\nThe procedure is relatively straightforward:\n\nTake the differences between treated and untreated groups within each stratum of \\(S\\).\nWeight these differences by the right distribution the estimand we are interested in:\n\nATE: weight the differences by the total sample distribution of \\(S\\).\nATT: weight differences by the distribution of \\(S\\) for treated cases.\n\n\nThus, for Table 6.1:\n\\[\n\\begin{align}\n\\text{ATT} &= \\frac{2000(50) + 2000(100) + 4000(150)}{50 + 100 + 150} = 3,000 \\\\\\\\\n\\text{ATE} &= \\frac{2000 (200) + 2000(200) + 4000(200)}{200 + 200 + 200} = 2,666.7\n\\end{align}\n\\]\nThe ATT is usually our most common estimand because treatment cases tend to be less common than control cases. In some cases, like the one portrayed in Table 6.2, we might not be able to calculate the ATE.\n\n\nTable 6.2: Treatment Effect Heterogeneity (Modified Example)\n\n\n\n\n\n\n\n\nSES\nN\ndegree\nearnings\n\n\n\n\n1\n150\n0\n2,000\n\n\n1\n0\n1\n?\n\n\n2\n100\n0\n6,000\n\n\n2\n100\n1\n8,000\n\n\n3\n50\n0\n10,000\n\n\n3\n150\n1\n14,000\n\n\n\n\nBut the ATT is just as easy to calculate as before, for Table 6.2:\n\\[\n\\text{ATT} = \\frac{(?)(0) + 2000(100) + 4000(150)}{0 + 100 + 150} = 3,200\n\\]\nCommon Support\nStrata that have only treatment or control cases (not both) are called off support. Strata with both treatment and control cases are in the region of common support.\nMove to matching\nsubclassification (stratifying, binning, interval matching) vs exact matching\nFrom sparseness to propensity scores. All exact matching is propensity score matching, but not all propensity score matching is exact matching.\nEstimate them using logistic regression or any other classifier.\nWorkflow\nWe don’t have to do this with exact matching\n\n\n\n\n\ncompress one dimensional projection, expand and check for balance\nthe better your logistic regression is at predicting the outcome, the “worse” it is at providing common support; this makes sense because ultimately we want the ignorability assumption to hold\n\n\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review, June, 00031224211004187. https://doi.org/10.1177/00031224211004187."
  },
  {
    "objectID": "causality-04-time.html#fixed-effects",
    "href": "causality-04-time.html#fixed-effects",
    "title": "7  Identification and Time",
    "section": "7.1 Fixed Effects",
    "text": "7.1 Fixed Effects\nIdentification using variation within groups or individuals\nRepeated observations within groups or individuals can provide a means for adjusting for unobserved confounders. This is conceptually similar to twin studies in which we look for variation within pairs of twins. We can extend this to larger groups or repeated observations.\nThe simplest regression model that handles this simply adds varying-intercepts \\(\\alpha_i\\) or “fixed effects.”\n\\[\ny_{ij} = \\beta_0 + \\theta x_{ij} + \\alpha_i + \\varepsilon_{ij}\n\\]\nWith more than two observations per group, we can fit this model using differences.\n\\[\ny_{ij} - \\overline y_i = \\theta (x_{ij} - \\overline x_i) + \\varepsilon_{ij} - \\overline \\varepsilon_{i}\n\\]\nThe causal predictor needs to vary within groups.\n\nThis study design can create comparisons across different levels of the treatment variable only if it varies within group. In examples where the treatment is dichotomous, it is possible for a substantial portion of the sample to exhibit no variation at all in the causal variable within groups. For instance, suppose a varying-intercepts model is used to estimate the effect of maternal employment on child cognitive outcomes by including indicators for each family (set of siblings) in a regression of outcomes on an indicator for whether the mother worked in the first year of the child’s life. In some families, the mother may not have varied her employment status across children. Therefore, no inferences about the effect of maternal employment status can be made for these families. We can only estimate effects for the type of family where the mother varied her employment choice across the children (for example, working after her first child was born but staying home from work after the second).\nGelman, Hill, and Vehtari (2020, 441)\n\nWithin-person variation (or “fixed effects”)\nPanel data provides the most common use of varying intercepts to estimate causal effects. Here we use repeated observations within the same individuals to adjust for time-constant unobserved confounders. Here, issues of balance and overlap still exist, but they only apply for within-person comparisons.\nIf the purpose of matching/weighting is to get rid of all observable confounders, then the purpose of fixed-effects is to additionally get rid of all time-constant unobserved confounders.\nWeight Loss Example\nLet’s say I weigh an individual in January (time 1) and again in July (time 2). This individual was not dieting before January, but does diet between January and July. In July, we find that the individual weighs 10kg less.\nWe want to know how much the diet caused the individual’s weight to change. That is, I’d like to compare the actual weight loss ( \\(\\text{weight}_\\text{July}^1 - \\text{weight}_\\text{Jan}^1\\) ) to a counterfactual weight loss in a world where where there is no diet ( \\(\\text{weight}_\\text{July}^0 - \\text{weight}_\\text{Jan}^0\\) ).\nWe can represent weight at two different time points using the following equations:\n\\[\n\\begin{align}\n\\text{weight}_1 &= \\theta \\ \\text{diet}_1 + \\mu + \\epsilon_1, \\\\\n\\text{weight}_2 &= \\theta \\ \\text{diet}_2 + \\mu + \\epsilon_2\n\\end{align}\n\\]\nHere, \\(\\theta\\) is the treatment effect of the diet; \\(\\mu\\) represents everything about the individual that does not change (i.e., time-constant) and that affects their weight (i.e., a personal fixed effect); \\(\\epsilon_1\\) and \\(\\epsilon_2\\) represent other time-varying effects on weight.\nFinally, because we already assumed that the individual was not dieting at time 1, we know that \\(\\text{diet}_1 = 0\\). Subtracting both equations, we get the following:\n\\[\n\\text{weight}_2 - \\text{weight}_1 = \\theta + \\underbrace{(\\mu - \\mu)}_{0}  + (\\epsilon_2 - \\epsilon_1)\n\\]\nThe power of fixed-effects estimation allows us to get rid of \\((\\mu - \\mu)\\) by simple subtraction; by comparing individuals to themselves (within-person variance), we have gotten rid of everything about them that does not change (i.e., time-constant), even if we do not know what that is.\nIn other words, this simple pre-post comparison allows for a good estimate of \\(\\theta\\) if we are also willing to believe that \\(E[\\epsilon_2 - \\epsilon_1] = 0\\). Maybe the individual would have lost weight anyway even without dieting because January is during the holidays when it is easier to gain weight (i.e., \\(\\epsilon_1 > \\epsilon_2)\\).\nTo deal with this, we might want to compare people who diet between January and July with those who do not diet over the same period to capture the effect of the passage of time. This is called difference-in-differences estimation."
  },
  {
    "objectID": "causality-04-time.html#difference-in-differences",
    "href": "causality-04-time.html#difference-in-differences",
    "title": "7  Identification and Time",
    "section": "7.2 Difference-in-Differences",
    "text": "7.2 Difference-in-Differences\nIdentification using within and between group variation.\nAlmost all identification strategies rely on comparisons across groups that have and have not been exposed to a treatment. Difference-in-differences strategies additionally make use of another source of variation—i.e., time—to help adjust for potential unobserved differences across groups.\nThe goal is to be able to compare the within variation in the treated group to the within variation in the untreated group.\nAssumptions\nThe change in \\(y\\) for the control group needs to represent what would have happened to the treatment group in the absence of the treatment.\nPotential change, given that \\(P=1\\) means post-exposure and \\(P=0\\) means pre-exposure.\n\\[\n\\begin{align}\nd^0 &= y^0_{P=1} - y^0_{P=0}, \\\\\\\\\nd^1 &= y^1_{P=1} - y^1_{P=0}\n\\end{align}\n\\]\nAverage Treatment Effects:\n\\[\n\\begin{align}\n\\text{ATE} &= \\text{E}[d^1 - d^0], \\\\\\\\\n\\text{ATT} &= \\text{E}[d^1 - d^0 \\mid T = 1], \\\\\\\\\n\\text{ATU} &= \\text{E}[d^1 - d^0 \\mid T = 0]\n\\end{align}\n\\]\nIgnorability assumption:\n\\[\nd^0, d^1 \\perp T\n\\]\n\nThis design is often presented as if it provides a weaker set of assumptions than traditional observational studies, as it a quasi-experiment, not just a study where we have to hope we’ve measured all confounders. However, looking carefully at the assumptions, it is unclear if the assumption of randomly assigned changes in potential outcome is truly more plausible than the assumption of randomly assigned potential outcomes for those with the same value of the pre-treatment variable (and of course both approaches could allow for conditioning on additional covariates). In either case, we need not assume actual random manipulation of treatment assignment for either assumption to hold, only results that would be consistent with such manipulation.\nGelman, Hill, and Vehtari (2020, 444–45)\n\nThe parallel trends assumption is inherently unobservable. It says that the difference between the treated and untreated groups would have remained the same in the post-treatment period as it was in the pre-treatment period, as depicted in Figure 7.1. This assumption fails, among other reasons, when there are other treatments affecting the untreated groups that are absent in the treated groups.\n\n\nCode\ntibble(\n  t = c(\"Time 0\", \"Time 1\"),\n  y1 = 1:2,\n  y0 = 0:1,\n  y = c(1, 3)\n) |> \n  pivot_longer(!c(t, y), names_to = \"g\") |> \n  ggplot(aes(t, value, group = g)) + \n  geom_line(linetype = \"dashed\") + \n  geom_vline(xintercept = c(\"Time 0\", \"Time 1\")) +\n  geom_line(aes(t, y)) + \n  geom_segment(x = \"Time 1\", y = 2, yend = 3, xend = \"Time 1\", linewidth = 1.5, \n               color = \"steelblue1\") +\n  labs(y = NULL, x = NULL) + \n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), \n        panel.grid = element_blank()) + \n  annotate(\"text\", x = 2.2, y = 2.5, label = latex2exp::TeX(r\"($E[d^1 - d^0]$)\"), family = \"Crimson Text\")\n\n\n\n\n\nFigure 7.1: Parallel Trends\n\n\n\n\nNote. If parallel trends hold for \\(Y\\), then it does not hold for \\(\\log(Y)\\) or any other non-linear transformation.\nRegression Framework\nSuppose that time is represented with a simple before-and-after exposure variable \\(P\\). We can see the difference-in-differences strategies within the traditional regression framework in terms of a simple interaction.\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 P_i + \\underbrace{\\theta \\ x_i P_i}_\\text{diff-n-diff} + \\varepsilon_i\n\\tag{7.1}\\]\nThe difference-in-differences estimand is simply the coefficient on the interaction term.\n\nIntuitively this makes sense because the interaction term represents the difference in the slope coefficients on time across the treatment groups where the slope coefficients on time each represent a difference in means (across time points). Equivalently, we could say that the interaction term represents the difference in slope coefficients on treatment across the time periods.\nGelman, Hill, and Vehtari (2020, 443)\n\n\n\nTable 7.1: Toy Example\n\n\n\n\\(X = 0\\)\n\\(X = 1\\)\n\n\n\n\n\\(P = 0\\)\n80\n80\n\n\n\\(P=1\\)\n75\n70\n\n\n\n\n\nNote that everyone has the same time values “pre” and “post.” There is no between person variation in time. If there was, we would have to do something like two-way fixed effects.\n\nWe can combine the values in Table 7.1 with Equation 7.1 to get the following values:\n\\[\n\\beta_0 = 80, \\ \\beta_1 = 0, \\ \\beta_2 = -5, \\theta = -5\n\\]\nTwo-way fixed effects\nAlternatively, we can think of this in terms of two-way fixed effects.\n\\[\ny_i = \\alpha_{g[i]} + \\alpha_{t[i]} + \\underbrace{\\beta \\ T_i}_\\text{diff-n-diff} + \\varepsilon_i\n\\tag{7.2}\\]\nThe \\(\\theta\\) coefficient in Equation 7.1 and Equation 7.2 is equivalent, provided we only have two groups and two time periods.\nUnfortunately, the TWFE approach for calculating difference-in-differences effects does not work particularly well for “rollout designs” (or “staggered treatment timing”) in which the treatment is applied at different times to different groups.\nExample:\n\ndata(\"organ_donations\", package = \"causaldata\")\nlibrary(fixest)\nlibrary(lme4)\nlibrary(plm)\n\nd <- organ_donations |> \n  janitor::clean_names() |> \n  mutate(treated = state == \"California\" & quarter %in% c('Q32011','Q42011','Q12012')) |> \n  mutate(state = factor(state), quarter = factor(quarter))\n\nclfe <- feols(rate ~ treated | state + quarter, data = d)\nols <- lm(rate ~ treated + state + quarter, data = d)\n\ntwfe <- plm(\n  rate ~ treated, \n  index = c(\"quarter\", \"state\"), \n  model = \"within\", \n  effect = \"twoways\",\n  data = d\n)\n\nmodelsummary::modelsummary(\n  list(feols = clfe, lm = ols, plm = twfe), \n  coef_omit = c(-1), \n  gof_map = \"nobs\"\n)\n\n\n\n \n  \n      \n    feols \n    lm \n    plm \n  \n \n\n  \n    treatedTRUE \n    −0.022 \n    −0.022 \n    −0.022 \n  \n  \n     \n    (0.006) \n    (0.020) \n    (0.020) \n  \n  \n    Num.Obs. \n    162 \n    162 \n    162 \n  \n\n\n\n\n\nNote the difference in standard errors.\nTest of prior trends:\n\nUse the data from before the treatment period.\nDo a NHST on the \\(\\theta\\) coefficient in the following regression:\n\\[\ny_i = \\alpha_{g[i]} + \\beta \\ t_i + \\theta \\ t_i \\times g_i + \\varepsilon_i\n\\]\n\nDynamic Treatment Effects\nInstead of just focusing on “before” and “after” effects, we can add a separate effect for each time period. A common way of doing in this is to generated a centered time variable—i.e., you subtract the treatment period from the original time variable. Then we just interact the treatment variable with a set of indicator variables for each of the time periods.\nFor example, suppose we have 3 time periods and that the treatment occurs in \\(t = 0\\).\n\\[\ny_i = \\alpha_{g[i]} + \\alpha_{t[i]} + \\theta_{-1[i]} \\times T_i + \\theta_{1[i]} \\times T_i + \\varepsilon_i\n\\]\n\n\\(\\theta_0\\) is dropped to avoid perfect multicollinearity\n\nHere, we should not find discernible effects among the before-treatment coefficients. Our confidence intervals should also blow up in size because we have less data. Figure 7.2 presents the results for this kind of setup.\n\n\nCode\nlibrary(tidyverse); library(fixest)\nod <- causaldata::organ_donations\n\n# Treatment variable\nod <- od %>% mutate(California = State == 'California')\n\n# Interact quarter with being in the treated group using\n# the fixest i() function, which also lets us specify\n# a reference period (using the numeric version of Quarter)\nclfe <- feols(Rate ~ i(Quarter_Num, California, ref = 3) | \n            State + Quarter_Num, data = od)\n\n# And use coefplot() for a graph of effects\ncoefplot(clfe)\n\n\n\n\n\nFigure 7.2: Dynamic Effects in Organ Donation Rates Example\n\n\n\n\n\n## something fucked up is going on here\nunique(od$Quarter_Num - 3L)\n\n[1] -2 -1  0  1  2  3\n\nod <- od |> \n  mutate(num = factor(Quarter_Num - 3L)) |> \n  mutate(num = fct_relevel(num, \"0\"))\n  \nlm(Rate ~ factor(State) + num + num*California, data = od) |> \n  broom::tidy(conf.int = TRUE)  |> \n  slice(34:38) |> \n  ggplot(aes(term, estimate)) + \n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\")\n\n\n\n\nAdd example from Theory and Credibility\nNote. If you combine difference-in-differences with matching, the answer will be exactly the same as long as you match on time-constant observable."
  },
  {
    "objectID": "causality-04-time.html#exercises",
    "href": "causality-04-time.html#exercises",
    "title": "7  Identification and Time",
    "section": "7.3 Exercises",
    "text": "7.3 Exercises\ndiff-n-diff\n\nnsw <- haven::read_dta(\"data/nsw.dta\")\nnsw_long <- nsw |>\n  panelr::long_panel(\n    id = \"id\",\n    wave = \"t\",\n    periods = c(75, 78),\n    label_location = \"end\"\n  ) |> \n  mutate(post = if_else(t == 78, 1L, 0L))\n\nglimpse(nsw_long)\n\nRows: 1,444\nColumns: 12\nGroups: id [722]\n$ id        <fct> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10…\n$ t         <dbl> 75, 78, 75, 78, 75, 78, 75, 78, 75, 78, 75, 78, 75, 78, 75, …\n$ data_id   <chr> \"Lalonde Sample\", \"Lalonde Sample\", \"Lalonde Sample\", \"Lalon…\n$ treat     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ age       <dbl> 37, 37, 22, 22, 30, 30, 27, 27, 33, 33, 22, 22, 23, 23, 32, …\n$ education <dbl> 11, 11, 9, 9, 12, 12, 11, 11, 8, 8, 9, 9, 12, 12, 11, 11, 16…\n$ black     <dbl> 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, …\n$ hispanic  <dbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ married   <dbl> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ nodegree  <dbl> 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, …\n$ re        <dbl> 0.0000, 9930.0459, 0.0000, 3595.8940, 0.0000, 24909.4492, 0.…\n$ post      <int> 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, …\n\nggplot(nsw_long, aes(x = post, y = re, group = factor(treat))) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(limits = c(0, 1), breaks = c(0,1), labels = c(\"1975\", \"1978\")) +\n  theme(legend.position = \"top\") +\n  labs(x = \"year\",\n       y = \"income ($)\",\n       title = \"Difference in differences for NSW\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\ndidreg <- lmer(\n  re ~ treat * post + (1 | id), \n  data = nsw_long, \n  REML = FALSE\n)\n\ndidreg\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: re ~ treat * post + (1 | id)\n   Data: nsw_long\n      AIC       BIC    logLik  deviance  df.resid \n 29058.10  29089.75 -14523.05  29046.10      1438 \nRandom effects:\n Groups   Name        Std.Dev.\n id       (Intercept) 2212    \n Residual             5230    \nNumber of obs: 1444, groups:  id, 722\nFixed Effects:\n(Intercept)        treat         post   treat:post  \n    3026.68        39.42      2063.37       846.89  \n\nols <- lm(re ~ treat*post, data = nsw_long)\n\nm2 <- feols(re ~ treat*post | id, data = nsw_long)\n\nThe variable 'treat' has been removed because of collinearity (see $collin.var).\n\nmodelsummary::modelsummary(\n  list(lmer = didreg, lm = ols, feols = m2),\n  gof_map = c(\"nobs\", \"icc\")\n)\n\n\n\n \n  \n      \n    lmer \n    lm \n    feols \n  \n \n\n  \n    (Intercept) \n    3026.683 \n    3026.683 \n     \n  \n  \n     \n    (275.435) \n    (275.817) \n     \n  \n  \n    treat \n    39.415 \n    39.415 \n     \n  \n  \n     \n    (429.447) \n    (430.043) \n     \n  \n  \n    post \n    2063.366 \n    2063.366 \n    2063.366 \n  \n  \n     \n    (358.761) \n    (390.065) \n    (324.753) \n  \n  \n    treat × post \n    846.888 \n    846.888 \n    846.888 \n  \n  \n     \n    (559.365) \n    (608.173) \n    (581.796) \n  \n  \n    SD (Intercept id) \n    2211.709 \n     \n     \n  \n  \n    SD (Observations) \n    5229.795 \n     \n     \n  \n  \n    Num.Obs. \n    1444 \n    1444 \n    1444 \n  \n  \n    ICC \n    0.2 \n     \n     \n  \n\n\n\n\n\nSlide 82 add more adjustment variables and see that the estimates remain the same\nNote. Using within-person variation is a powerful strategy, but it does not allow for making generalizations about “the overall effect” of something—i.e., it’s the effect conditional on the fact that there is within person variation. We can’t generalize automatically to groups of people that don’t have within person variance."
  },
  {
    "objectID": "causality-04-time.html#regression-discontinuity",
    "href": "causality-04-time.html#regression-discontinuity",
    "title": "7  Identification and Time",
    "section": "7.4 Regression Discontinuity?",
    "text": "7.4 Regression Discontinuity?"
  },
  {
    "objectID": "causality-04-time.html#additional-resources",
    "href": "causality-04-time.html#additional-resources",
    "title": "7  Identification and Time",
    "section": "7.5 Additional Resources",
    "text": "7.5 Additional Resources\n\nDifference-in-Differences\n\nGoodman-Bacon (2021), Callaway and Sant’Anna (2021), Sant’Anna and Zhao (2020), Sun and Abraham (2021), De Chaisemartin and d’Haultfoeuille (2020)\n\nRegression Discontinuity\n\nGelman and Imbens (2019), Gelman and Zelizer (2015)"
  },
  {
    "objectID": "causality-04-time.html#extra",
    "href": "causality-04-time.html#extra",
    "title": "7  Identification and Time",
    "section": "7.6 Extra",
    "text": "7.6 Extra\nBetween and within variation.\nTwo typical situations:\n\nclusters (e.g., students nested in classrooms).\nrepeated observations (i.e., observations nested in subjects)\n\nfixed effects is like your mom saying “you should not compare yourself to other people”\nfixed effects driven estimates might not generalize to people who don’t change\nintra-class correlation \\(\\to\\) variance partitioning coefficient\nwhen there is no within-person variation (what year were you born?), vpc = 100%\naverages predicting averages is not the same as fluctuations predicting fluctuations\nExtending the mixed model (slide 131)\nWithin-between hybrid model\n\\[\ny_{it} = \\gamma_0 + \\gamma_1 z_i + \\beta_W (x_{it} - \\bar x_i) + \\beta_B \\bar x_i + \\theta_t + \\alpha_i + \\epsilon_{it}\n\\]\nby construction, \\(\\alpha_i\\) and \\((x_{it} - \\bar x_i)\\) are independent\nCorrelated random effects (contextual) model\n\\[\ny_{it} = \\gamma_0 + \\gamma_1 z_i + \\beta_W x_{it} + \\beta_C \\bar x_i + \\theta_t + \\alpha_i + \\epsilon_{it}\n\\]\nNote \\(\\beta_C = \\beta_B - \\beta_W\\)\nBoth fit the data equally well, but the interpretation of the coefficients is slightly different.\n\\(\\beta_W\\) is literally the same thing you get when using “fixed effects”\nSchunck and Perales Within and between cluster effects in generalized linear mixed models\n\n\n\n\nCallaway, Brantly, and Pedro HC Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics 225 (2): 200230.\n\n\nDe Chaisemartin, Clément, and Xavier d’Haultfoeuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 29642996.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order Polynomials Should Not Be Used in Regression Discontinuity Designs.” Journal of Business & Economic Statistics 37 (3): 447456.\n\n\nGelman, Andrew, and Adam Zelizer. 2015. “Evidence on the Deleterious Impact of Sustained Use of Polynomial Regression on Causal Inference.” Research & Politics 2 (1): 2053168015569830.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254277.\n\n\nSant’Anna, Pedro HC, and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101122.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 175–99. https://doi.org/10.1016/j.jeconom.2020.09.006."
  },
  {
    "objectID": "other-00-intro.html",
    "href": "other-00-intro.html",
    "title": "Other",
    "section": "",
    "text": "Some remarks"
  },
  {
    "objectID": "other-01-gps.html#background",
    "href": "other-01-gps.html#background",
    "title": "8  Gaussian Proccesses",
    "section": "8.1 Background",
    "text": "8.1 Background\nI started thinking that it would be useful to learn about Gaussian Processes in the context of a project of mine about the Ecology of Rulings.\nThis framework, adapted from March, Schulz, and Zhou (2000), builds upon three general ideas: (1) rules evolve over time in response to problems; (2) rules are interrelated within an organization; and (3) rules both record history and reflect learning within the organization.\nIn this framework, the rulings of a higher court are related to each other according to continuous time (measured at day level) and according to an “embedding” space constructed out of word usage and citation practices. I expect correlations of birthrates to be positive when they are close in time and “space” but to be negative when they are close in time but far away in “space.”\nThus, we have two separate distance metrics.\nThe problem is that we don’t have a discrete measurement of time (at least one that’s sufficiently meaningful) that we’d have with general concepts such as “cohort.” We also don’t have a discrete measurement of populations—i.e., rulings that are closer in embedding space exhibit more “groupiness,” while ones that are farther away are less “groupy.” It’s a matter of degree.\nThis is why I’m exploring Gaussian Processes.\nAlternative approach:\nGet the embedding, calculate distances, and then cluster.\nThen for each cluster \\(k\\) we have \\(N_k\\). Now N is the dependent variable and time is the independent variable. We expect different forms of density-dependence and cross-density dependence.\nmahalanobis??\nhyper-cube\n----\n4321 vs citations /// mesoudi setup /// look at judges // individual learning"
  },
  {
    "objectID": "other-01-gps.html#gaussian-processes",
    "href": "other-01-gps.html#gaussian-processes",
    "title": "8  Gaussian Proccesses",
    "section": "8.2 Gaussian Processes",
    "text": "8.2 Gaussian Processes\nWhat are they? Will I need them?\n\\[\n\\boldsymbol u \\sim \\text{N}(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\n\\]\nwhere \\(u_i = u(s_i)\\), \\(\\mu_i = \\mu(s_i)\\), and \\(\\Sigma_{i, j} = k(s_i, s_j)\\) for some positive definite covariance function \\(k\\) (or kernel).\nAnother notation:\nConsider a stochastic process denoted by \\(\\{ Y(\\boldsymbol r) : \\boldsymbol r \\in D \\}\\) where \\(\\boldsymbol r\\) is a location in \\(D\\) (some d-dimensional space). This process is called a Gaussian Process if it has all its finite-dimensional distributions determined by a mean function \\(\\mu (\\boldsymbol r)\\) and a covariance function \\(k(\\boldsymbol r, \\boldsymbol r^\\prime)\\) for any location \\(\\{\\boldsymbol r, \\boldsymbol r^\\prime \\} \\in D\\).\nThat makes little sense. What else do we have?\n\nA model for an unknown function.\nWe want to define a stochastic model for an unknown function \\(u(\\boldsymbol s)\\). We use a Gaussian distribution to model this joint distribution of \\(s_1, s_2, \\dots, s_k\\). Why? Because they are easy to use. But then we have to think hard about the covariance function, such that it stays “positive definite” (i.e., the eigenvalues stay non-negative).\n\n\nA Gaussian process is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions. Leaving mathematical sophistication aside, one can loosely think of a function as a very long vector, each entry in the vector specifying the function value f(x) at a particular input x. It turns out, that although this idea is a little naïve, it is surprisingly close what we need. Indeed, the question of how we deal computationally with these infinite dimensional objects has the most pleasant resolution imaginable: if you ask only for the properties of the function at a finite number of points, then inference in the Gaussian process will give you the same answer if you ignore the infinitely many other points, as if you would have taken them all into account!\nrasmussen 2\n\nSo this is all about functions too, eh?? We want to estimate posterior distributions over functions.\nGaussian Processes are well-known by geo-statisticians, who refer to it as kriging, altough they focus mostly on two-dimensional or three-dimensional input spaces.\nA GP is an infinite-dimensional generalization of multivariate normal distributions.\nThe kernel function generalizes to infinite dimensions/ observations/ predictions.\nContinuous categories and the Gaussian Process\nContinuous ordered categories (distances in ages, time, space); we want points that are closer to one another to share more information with one another.\nAge example.\n\nIndividuals of the same age share some of the same exposures. They listened to some of the same music, heard about the same politicians, and experienced the same weather events. And individuals of similar ages also experienced some of these same exposures, but to a lesser extent than individuals of the same age. The covariation falls off as any two individuals become increasingly dissimilar in age or income or stature or any other dimension that indexes background similarity.\nmcelreath 468\nThe general purpose is to define some dimension along which cases differ. This might be individual differences in age. Or it could be differences in location. Then we measure the distance between each pair of cases. What the model then does is estimate a function for the covariance between pairs of cases at different distances.\n468\n\nHe describes this as partial pooling for continuous categories…\nhttps://youtu.be/Y2ZLt4iOrXU?t=624 Add from Gelman about the distances\nDistance matrix:\n\ndata(\"islandsDistMatrix\", package = \"rethinking\")\nislandsDistMatrix\n\n           Malekula Tikopia Santa Cruz   Yap Lau Fiji Trobriand Chuuk Manus\nMalekula      0.000   0.475      0.631 4.363    1.234     2.036 3.178 2.794\nTikopia       0.475   0.000      0.315 4.173    1.236     2.007 2.877 2.670\nSanta Cruz    0.631   0.315      0.000 3.859    1.550     1.708 2.588 2.356\nYap           4.363   4.173      3.859 0.000    5.391     2.462 1.555 1.616\nLau Fiji      1.234   1.236      1.550 5.391    0.000     3.219 4.027 3.906\nTrobriand     2.036   2.007      1.708 2.462    3.219     0.000 1.801 0.850\nChuuk         3.178   2.877      2.588 1.555    4.027     1.801 0.000 1.213\nManus         2.794   2.670      2.356 1.616    3.906     0.850 1.213 0.000\nTonga         1.860   1.965      2.279 6.136    0.763     3.893 4.789 4.622\nHawaii        5.678   5.283      5.401 7.178    4.884     6.653 5.787 6.722\n           Tonga Hawaii\nMalekula   1.860  5.678\nTikopia    1.965  5.283\nSanta Cruz 2.279  5.401\nYap        6.136  7.178\nLau Fiji   0.763  4.884\nTrobriand  3.893  6.653\nChuuk      4.789  5.787\nManus      4.622  6.722\nTonga      0.000  5.037\nHawaii     5.037  0.000\n\n\nSome extra about distance metrics…"
  },
  {
    "objectID": "other-01-gps.html#examples",
    "href": "other-01-gps.html#examples",
    "title": "8  Gaussian Proccesses",
    "section": "8.3 Examples",
    "text": "8.3 Examples\nDo the phylogeny examples with hierarchical clustering, do gap statistic, and with GP for comparison.\nThen revisit second-year paper.\n\ndata(\"Primates301\", package = \"rethinking\")\ndata(\"Primates301_nex\", package = \"rethinking\")\n\nhttps://betanalpha.github.io/assets/case_studies/gaussian_processes.html\n\n\n\n\nMarch, James G., Martin Schulz, and Xueguang Zhou. 2000. The Dynamics of Rules: Change in Written Organizational Codes. Stanford University Press."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless\nEconometrics. Princeton university press.\n\n\nAronow, Peter M., and Cyrus Samii. 2017. “Estimating Average\nCausal Effects Under General Interference, with Application to a Social\nNetwork Experiment.” The Annals of Applied Statistics 11\n(4): 1912–47. https://doi.org/10.1214/16-AOAS1005.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita.\n2021. Theory and Credibility: Integrating Theoretical and Empirical\nSocial Science. Princeton University Press.\n\n\nBeecher, Henry K. 1955. “The Powerful Placebo.” Journal\nof the American Medical Association 159 (17): 16021606.\n\n\nBollen, Kenneth A. 1989. Structural Equations with Latent\nVariables. John Wiley & Sons.\n\n\nCallaway, Brantly, and Pedro HC Sant’Anna. 2021.\n“Difference-in-Differences with Multiple Time Periods.”\nJournal of Econometrics 225 (2): 200230.\n\n\nDe Chaisemartin, Clément, and Xavier d’Haultfoeuille. 2020.\n“Two-Way Fixed Effects Estimators with Heterogeneous Treatment\nEffects.” American Economic Review 110 (9): 29642996.\n\n\nDorie, Vincent, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone.\n2019. “Automated Versus Do-It-Yourself Methods for Causal\nInference: Lessons Learned from a Data Analysis Competition.”\nStatistical Science 34 (1): 43–68. https://doi.org/10.1214/18-STS667.\n\n\nDruckman, Jamie. 2022. Experimental Thinking. Cambridge\nUniversity Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order\nPolynomials Should Not Be Used in Regression Discontinuity\nDesigns.” Journal of Business & Economic Statistics\n37 (3): 447456.\n\n\nGelman, Andrew, and Adam Zelizer. 2015. “Evidence on the\nDeleterious Impact of Sustained Use of Polynomial Regression on Causal\nInference.” Research & Politics 2 (1):\n2053168015569830.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments:\nDesign, Analysis, and Interpretation. New York: W. W. Norton &\nCompany.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225 (2): 254277.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What\nIf. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60. https://doi.org/10.2307/2289064.\n\n\nHudgens, Michael G., and M. Elizabeth Halloran. 2008. “Toward\nCausal Inference with Interference.” Journal of the American\nStatistical Association 103 (482): 832842.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. New York: Chapman; Hall/CRC. https://doi.org/10.1201/9781003226055.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for\nStatistics, Social, and Biomedical Sciences: An Introduction. 1st\nedition. New York: Cambridge University Press.\n\n\nKing, Gary, and Margaret E. Roberts. 2015. “How Robust Standard\nErrors Expose Methodological Problems They Do Not Fix, and What to Do\nAbout It.” Political Analysis 23 (2): 159–79. https://doi.org/10.1093/pan/mpu015.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021.\n“What Is Your Estimand? Defining the Target Quantity Connects\nStatistical Evidence to Theory.” American Sociological\nReview, June, 00031224211004187. https://doi.org/10.1177/00031224211004187.\n\n\nMarch, James G., Martin Schulz, and Xueguang Zhou. 2000. The\nDynamics of Rules: Change in Written Organizational Codes. Stanford\nUniversity Press.\n\n\nMeissner, Karin, Ulrike Bingel, Luana Colloca, Tor D. Wager, Alison\nWatson, and Magne Arve Flaten. 2011. “The Placebo Effect: Advances\nfrom Different Methodological Approaches.” Journal of\nNeuroscience 31 (45): 1611716124.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals\nand Causal Inference: Methods and Principles for Social Research.\n2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference.\n2nd edition. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. 1st edition. Wiley.\n\n\nRosenbaum, Paul R. 2007. “Interference Between Units in Randomized\nExperiments.” Journal of the American Statistical\nAssociation 102 (477): 191200.\n\n\nRubin, Donald B. 2005. “Causal Inference Using Potential Outcomes:\nDesign, Modeling, Decisions.” Journal of the American\nStatistical Association 100 (469): 322331.\n\n\nSant’Anna, Pedro HC, and Jun Zhao. 2020. “Doubly Robust\nDifference-in-Differences Estimators.” Journal of\nEconometrics 219 (1): 101122.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic\nTreatment Effects in Event Studies with Heterogeneous Treatment\nEffects.” Journal of Econometrics, Themed Issue:\nTreatment Effect 1, 225 (2): 175–99. https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2\nFallacy: Presenting and Interpreting Confounder and Modifier\nCoefficients.” American Journal of Epidemiology 177 (4):\n292–98. https://doi.org/10.1093/aje/kws412."
  },
  {
    "objectID": "other-00-intro.html#misc",
    "href": "other-00-intro.html#misc",
    "title": "Spatial Statistics",
    "section": "Misc",
    "text": "Misc\n\nEverything is related to everything else, but near things are more related than distant things.\n\nspatial autocorrelation\nmaybe change to embedding section\nMartin: gravity model, decay parameters with distance, this looks like GP imho\n\\[\nI_{ij} = \\alpha \\cdot \\frac{M_i \\cdot M_j}{d_{ij}^{\\beta}}\n\\]\nIn such a model, \\(M\\) represents the “mass” at locations \\(i\\) and \\(j\\); \\(d\\) is some distance metric between \\(i\\) and \\(j\\); \\(\\alpha\\) and \\(\\beta\\) are parameters to be estimated; and \\(I_{ij}\\) represents the “flow” or “exchange” between \\(i\\) and \\(j\\).\nwhat the fuck are kernels?\nkernel density estimation?\nhttps://mgimond.github.io/Spatial/chp11_0.html#chp11_0\nhttps://mgimond.github.io/Spatial/spatial-autocorrelation.html\nvariograms\nhttps://doi-org.proxy.lib.duke.edu/10.1016/j.spasta.2017.02.006\nhttps://r-spatial.org/book/11-PointPattern.html"
  },
  {
    "objectID": "other-00-intro.html#spatial-point-pattern-analysis",
    "href": "other-00-intro.html#spatial-point-pattern-analysis",
    "title": "Spatial Statistics",
    "section": "Spatial Point Pattern Analysis",
    "text": "Spatial Point Pattern Analysis\nPoint pattern analysis is concerned with describing patterns of points over space and making inference about the process that could have generated an observed pattern.\n\nImportant concepts of point patterns analysis are the distinction between a point pattern and a point process: the latter is the stochastic process that, when sampled, generates a point pattern. A dataset is always a point pattern, and inference involves figuring out the properties of a process that could have generated a pattern like the one we observed. Properties of a spatial point process include\n\nfirst order properties: the intensity function measures the number of points per area unit; this function is spatially varying for a inhomogeneous point process\nsecond order properties: given a constant or varying intensity function, describe whether points are distributed independently from one another, tend to attract each other (clustering), or repulse each other (more regularly distributed than under complete spatial randomness)\n\n\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nD <- 2\nN <- 300\nmu <- rep(0, D)\n\n\nsim <- mvtnorm::rmvnorm(N, mu, diag(D))\n\ndf <- as.data.frame(sim) |> \n  setNames(letters[1:D]) |> \n  st_as_sf(coords = letters[1:D])\n\nplot(df, pch = 21, cex = 1/2)\n\n\n\nlibrary(spatstat)\n\nLoading required package: spatstat.data\n\n\nLoading required package: spatstat.geom\n\n\nspatstat.geom 3.2-1\n\n\nLoading required package: spatstat.random\n\n\nspatstat.random 3.1-5\n\n\nLoading required package: spatstat.explore\n\n\nLoading required package: nlme\n\n\nspatstat.explore 3.2-1\n\n\nLoading required package: spatstat.model\n\n\nLoading required package: rpart\n\n\nspatstat.model 3.2-4\n\n\nLoading required package: spatstat.linnet\n\n\nspatstat.linnet 3.1-1\n\n\n\nspatstat 3.0-6 \nFor an introduction to spatstat, type 'beginner' \n\npp <- as.ppp(df)\nqcount <- quadratcount(pp, nx = 10, ny = 10)\nplot(pp, cex = 1/5)\nplot(qcount, add = TRUE)\n\n\n\ndens <- density(pp, sigma = bw.diggle)\nplot(dens)\nplot(pp, cex = 1/5, add = TRUE, col = \"white\")\n\n\n\ns <- stars::st_as_stars(dens)\n\npt <- st_sfc(st_point(c(0, 0))) ## center\ns$dist <- st_as_sf(s, as_points = TRUE, na.rm = FALSE) |> st_distance(pt)\n\nmod <- ppm(pp ~ dist, data = list(dist = as.im(s[\"dist\"])))\nmod\n\nNonstationary Poisson process\nFitted to point pattern dataset 'pp'\n\nLog intensity:  ~dist\n\nFitted trend coefficients:\n(Intercept)        dist \n   4.600546   -1.389613 \n\n             Estimate      S.E.   CI95.lo   CI95.hi Ztest      Zval\n(Intercept)  4.600546 0.1113939  4.382218  4.818874   ***  41.29981\ndist        -1.389613 0.0759040 -1.538382 -1.240844   *** -18.30751\n\nplot(mod, se = FALSE)"
  }
]