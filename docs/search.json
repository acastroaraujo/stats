[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nIt contains my notes on various things related to applied statistics."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Some general remarks about statistics and sociology"
  },
  {
    "objectID": "00-regression.html",
    "href": "00-regression.html",
    "title": "Regression",
    "section": "",
    "text": "add stuff here."
  },
  {
    "objectID": "std-errors.html",
    "href": "std-errors.html",
    "title": "2  Standard Errors",
    "section": "",
    "text": "The sampling distribution of regression coefficients follows a normal distribution.\nIn the case of one predictor \\(X\\):\n\\[\n\\begin{align}\nY &\\sim \\text{Normal}(\\beta_0 + \\beta_1 X, \\ \\sigma^2) \\\\\\\\\n\\hat{\\beta_i} &\\sim \\text{Normal}(\\beta_i, \\text{SE} (\\hat \\beta_i)^2)\\\\\\\\\n\\text{SE}(\\hat \\beta_1) &= \\sqrt{\\frac{\\hat \\sigma^2}{\\sum_{i=1}^n (x - \\bar x)^2}} = \\sqrt{\\frac{\\hat \\sigma^2}{\\text{var}(X) \\times n}}\n\\end{align}\n\\]\n\nThe standard deviation of a sampling distribution is often referred to as a standard error.\n\nIn the case of multiple predictors (\\(\\mathbf X\\)) and coefficients (\\(\\boldsymbol \\beta\\)):\n\\[\n\\begin{align}\nY &\\sim \\text{Normal}(\\boldsymbol{X \\beta}, \\sigma^2 \\boldsymbol I) \\\\\n\\boldsymbol{\\hat \\beta} &\\sim \\text{Normal}(\\boldsymbol \\beta, \\boldsymbol \\Sigma) \\\\\n\\boldsymbol \\Sigma &=\\widehat{\\text{var}} \\hat{(\\boldsymbol \\beta)} = \\hat \\sigma^2 (\\boldsymbol{X^\\top X})^{-1}\n\\end{align}\n\\]\nThis is what allows us to do hypothesis testingTM on regression coefficients.\nWe calculate the standard error because we want to know the standard deviation of that sampling distribution. But our standard errors are probably always wrong.\nTwo assumptions:\n\n\\(\\varepsilon\\) is normally distributed.\n\\(\\varepsilon\\) is independent and identically distributed.\n\nThese assumptions are most obviously false when dealing with time series or data that’s geographically clustered (i.e., temporal or spatial autocorrelation).\nThere could also be heteroskedasticity—i.e., the variance of \\(\\varepsilon\\) is related to other variables in the model.\nFixing Standard Errors\n\nHeteroskedasticity-robust sandwich estimator (e.g., Huber-White). We weight observations with big residuals more when calculating the variance.\n\\[\n(\\boldsymbol{X^\\top X})^{-1}(\\boldsymbol{X^\\top \\Sigma X}) (\\boldsymbol{X}^\\top X)^{-1}\n\\]\nHeteroskedasticity and Autocorrelation Consistent (HEC) standard errors (e.g., Newey-West).\nClustered standard errors. Why not go with multilevel models instead?\nBootstrap methods.\n\n\nSandwich estimators leave the coefficient estimates (\\(\\boldsymbol{\\hat \\beta}\\)) intact!\n\nWhy do economists care so much about standard errors? They have a toxic culture that values “gotcha!” moments during conference presentations.\nWe can use the estimatr package to easily calculate these robust standard errors.\n\nlibrary(estimatr)\ndata(restaurant_inspections, package = \"causaldata\")\n\nm1 <- lm(inspection_score ~ Year + Weekend, \n         data = restaurant_inspections)\n\nm2 <- lm_robust(inspection_score ~ Year + Weekend, \n          data = restaurant_inspections, \n          se_type = \"HC1\"\n)\n\nmodelsummary::msummary(list(m1, m2), gof_omit = \".\")\n\n\n\n \n  \n      \n     (1) \n      (2) \n  \n \n\n  \n    (Intercept) \n    185.380 \n    185.380 \n  \n  \n     \n    (12.809) \n    (12.149) \n  \n  \n    Year \n    −0.046 \n    −0.046 \n  \n  \n     \n    (0.006) \n    (0.006) \n  \n  \n    WeekendTRUE \n    2.057 \n    2.057 \n  \n  \n     \n    (0.433) \n    (0.351) \n  \n\n\n\n\n\nThis is how the matrix algebra works:\n\nX <- model.matrix(m1)\nA <- solve(t(X) %*% X) \nr <- m1$residuals\nB <- (t(X) %*% diag(r^2) %*% X)\nvarB <- A %*% B %*% A\nsqrt(diag(varB)) ## robust standard errors\n\n(Intercept)        Year WeekendTRUE \n12.14823297  0.00604299  0.35122580 \n\n\nSee King and Roberts (2015) for more.\n\n\n\n\nKing, Gary, and Margaret E. Roberts. 2015. “How Robust Standard Errors Expose Methodological Problems They Do Not Fix, and What to Do About It.” Political Analysis 23 (2): 159–79. https://doi.org/10.1093/pan/mpu015."
  },
  {
    "objectID": "marginal-effects.html",
    "href": "marginal-effects.html",
    "title": "3  Marginal Effects",
    "section": "",
    "text": "https://theeffectbook.net/ch-StatisticalAdjustment.html#eq:regression-15\nMarginal effect at the mean. Icky.\nAverage Marginal effects."
  },
  {
    "objectID": "causality-00-intro.html#notation",
    "href": "causality-00-intro.html#notation",
    "title": "Causality",
    "section": "Notation",
    "text": "Notation\nThe difference between seeing and doing is enormous. It is the reason why we do not regard movements in a thermostat to be a cause of the temperature outside.\n\\[\n\\Pr(Y \\mid T) \\neq \\Pr(Y \\mid do(T))\n\\]\n\nPearl\n\nPotential Outcomes\n\n\\(T\\) is a binary treatment variable. The terms “treatment” and “cause” are used interchangeably.\n\\(Y\\) is the outcome we observe.\n\\(Y^0\\) is the the value the outcome would take if \\(T=0\\).\n\\(Y^1\\) is the value the outcome would take if \\(T=1\\).\n\\(Y^0\\) and \\(Y^1\\) are the potential outcomes.\nWe see \\(Y^0\\) or \\(Y^1\\) for the same unit, but never both. This is the fundamental problem of causal inference.\nWhen \\(T=1\\), \\(Y^0\\) is the counterfactual.\nWhen \\(T=0\\), \\(Y^1\\) is the counterfactual.\n\n\nRubin\n\nOne might object to the fundamental problem of causal inference by noting situations where we can actually measure both \\(Y^0\\) and \\(Y^1\\) on the same unit \\(i\\). For example, we could drink warm milk one evening and coffee another evening, and then measure our amount of sleep time. Holland (1986) refers to this as temporal stability and causal transience. This is the same situation in which we flick the same light switch on and off to figure out if it turns on the light to a room. In other settings, it is perfectly reasonable to assume that \\(Y^1_i = Y^1_j\\) and that \\(Y^0_i = Y^0_j\\) (unit homogeneity assumption). For example, this is very common in laboratories that take special care in standardizing the units of an experiment (e.g., mice).\n\nThese objections cannot usually be made in most social science applications.\nHowever, matched pair experiments of identical twins come close to this homogeneity assumption."
  },
  {
    "objectID": "causality-00-intro.html#experiments",
    "href": "causality-00-intro.html#experiments",
    "title": "Causality",
    "section": "Experiments",
    "text": "Experiments\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\n\\[\nY^0, Y^1 \\perp T\n\\tag{1}\\]\n\nIgnorability\n\nThus, in expectation:\n\\[\n\\begin{align}\nE[Y^0 \\mid T = 0] &= E[Y^0 \\mid T = 1], \\\\\\\\\nE[Y^1 \\mid T = 0] &= E[Y^1 \\mid T = 1]\n\\end{align}\n\\tag{2}\\]\n\n…ignorability implies that the value of someone’s potential outcomes does not provide any information about his or her treatment group assignment”.\nGelman, Hill, and Vehtari (2020, pp. 350–1)\n\nWhich means that we can easily estimate the average causal effect of \\(T\\) over all units in a population:\n\\[\n\\text{ATE} = \\underbrace{E[Y^1 - Y^0] = E[Y^1] - E[Y^0]}_\\text{by linearity of expectations}\n\\tag{3}\\]\nIn the absence of randomization, so that treatment and control groups differ on pre-treatment characteristics, we might observe the following biases:\n\nBaseline bias. The two groups might be different from each other whether they get treated or not.\nTreatment effect heterogeneity. The two groups might respond differently to the treatment.\nThis is important because researchers sometimes assume a constant effect for every unit in the population. This is implied in the unit homogeneity assumption. But if the variability of causal effects is large across the population, then the ATE might not represent the causal effect of a specific unit very well; the ATE might be irrelevant, no matter how carefully we estimate it.\n\nThree types of treatment effects:\n\n\\(\\text{ATE}\\), for all units (effect of switching)\n\\(\\text{ATT}\\), for treated units (effect of taking away treatment)\n\\(\\text{ATC}\\) or \\(\\text{ATU}\\), for untreated units (effect of adding treatment)\n\n\nSome times we see these terms pre-fixed with an \\(S\\) (for sample) or a \\(P\\) (for population).\n\nTable 4.1 allows us to gain further intuition on these calculations with a hypothetical example in which people are assigned college degrees randomly.\n\n\nTable 1: Potential Outcomes Example\n\n\nGroup (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\nCollege Degree (\\(T = 1\\))\n1000\n600*\n\n\nNo Degree (\\(T = 0\\))\n800*\n500\n\n\n\n\n\n\nSimple experiments of these kind are obviously impossible in the social sciences.\n* means unobservable\n\nIf 30% of the population has a degree…\n\n\n\nWhat is the naive estimate?\n500\nWhat is the ATT?\n400\n\n\n\n\nWhat is the ATC?\n300\nWhat is the ATE?\n\\(0.3\\times 400 + 0.7 \\times 300 = 330\\)\n\n\n\nMore than two treatment levels, continuous treatments, and multiple treatment factors\nMultiple treatment effects can be defined relative to a baseline level, following the general principles of regression modeling with indicator (or dummy) variables.\nTreatment levels can be continuous.\n\nTo conceptualize randomization with a continuous treatment, think of spinning a spinner that can land on any of the potential levels of the treatment assignment. As with regression inputs in general, it can make sense to fit more complicated models as suggested by theory or supported by data. A linear model—which estimates the average effect on \\(y\\) for each additional unit of \\(z\\)—is a natural starting point for effects that are believed to be monotonically increasing or decreasing functions of the treatment level.\nGelman, Hill, and Vehtari (2020, 342)\n\nFinally, we can also consider multiple simultaneous treatments.\n\n…multiple treatments can be administered in combination. For instance, depressed individuals could be randomly assigned to receive nothing, drugs, counseling sessions, or both drugs and counseling sessions. These combinations could be modeled as two treatments and their interaction or as four distinct treatments.\nGelman, Hill, and Vehtari (2020, 342)\n\nUsing design and analysis to address imbalance and lack of overlap between treatment and control groups\nRegression can be useful even in the context of randomized experiments.\n\nIn practice, we can never ensure that treatment and control groups are balanced on all relevant pre-treatment characteristics. However, there are statistical approaches that may bring us closer. At the design stage, we can use randomization to ensure that treatment and control groups are balanced in expectation, and we can use blocking to reduce the variation in any imbalance. At the analysis stage, we can adjust for pre-treatment variables to correct for differences between the two groups to reduce bias in our estimate of the sample average treatment effect. We can further adjust for differences between sample and population if our goal is to estimate the population average treatment effect.\nGelman, Hill, and Vehtari (2020, 344)\n\nWe can increase the precision of treatment effect estimates by adjusting for pre-treatment variables that are predictive of the outcome—i.e., we get lower standard errors on the treatment parameter.\nGroup or cluster-randomized experiments\nSometimes it’s difficult to randomize treatment assignments to individual units, and so we might randomize groups or clusters to receive the treatment instead.\n\nA decision to assign treatments at the group level can be driven by cost or logistical concerns. It might be more cost effective to provide free flu shots to a random subset of health clinics, for example, than to have professionals go to every clinic and then randomly assign individuals to receive shots. Assignment at the clinic level would also avoid creating ill will among potential study participants being deprived of a service that others in the same location are able to receive. Cluster-randomized experiments are also used to avoid spillover or contagion effects (which can also be considered as violations of the stable unit treatment value assumption or SUTVA).\nGelman, Hill, and Vehtari (2020, 349–50)\n\n\nSUTVA\nBesides ignorability (see Equation 1), randomization implies certain other properties which we might consider to be assumptions.\nThe most important one is the stable unit treatment value assumption (SUTVA). It means that we assume that there is no interference among units (i.e., no spillovers) and no hidden versions of the treatment.\nThus, one can imagine person \\(i\\)’s outcome to be “a function not only of her own treatment assignment, but also the treatment assignments of others in the sample” (Gelman, Hill, and Vehtari 2020, 353). These becomes intractable very quickly. For example, in a sample of just 10 people and a binary treatment, we would have 210 = 1024 potential outcomes for each person. Thus, researchers often hope that there is no interference among units, or else they think about modeling spillover in some way.\nSUTVA also implies that there are no hidden versions of treatments—i.e., we want \\(t_i\\) to equal \\(t_j\\).\n\nExamples of potential SUTVA violations abound. An experiment testing the effect of a new fertilizer by randomly assigning adjacent plots to treatment or control is a classic example. Fertilizer from one plot might leach into an adjacent plot assigned to receive no fertilizer and thus affect the yield in that control plot. Vaccines that reduce the probability of a contagious disease within a school, business, or community could easily lead to violation of SUTVA if the vaccine is actually effective. Consider an experiment that recruited families from the same public housing complex and randomized them to receive a voucher to move to a better neighborhood or not. This could suffer from interference if a given family moving might influence (positively or negatively) the well-being of another family that happened to be randomized to not receive the voucher.\nGelman, Hill, and Vehtari (2020, 353)\n\nIn educational settings, potential SUTVA violations are often a reason to assign treatments at the classroom or school level."
  },
  {
    "objectID": "causality-00-intro.html#identification-and-causal-diagrams",
    "href": "causality-00-intro.html#identification-and-causal-diagrams",
    "title": "Causality",
    "section": "Identification and Causal Diagrams",
    "text": "Identification and Causal Diagrams\nIdentification refers to the idea of identifying the effect of a treatment (or cause) on an outcome.\nCausal diagrams or DAGs are graphical representations of a data generating process. Everything we draw is hopefully an informed assumption; everything that’s not in the diagram is also an assumption. In other words, DAGs encode identifying assumptions.\n\nPaths.\nDirect effects.\nIndirect effects.\nTotal effects.\nFront door paths. They point away from the treatment. If we want to estimate “total effects,” then we want to keep all of them open.\nBack door paths. They point towards the treatment; we want to close them off.\nConfounding. A property of paths, not of variables.\nColliders. A variable is a collider in a path iff both arrows point at it.\n\\[\na \\to b \\to c_\\text{ollider} \\leftarrow d \\leftarrow e \\to f\n\\]\nHere, \\(b\\) and \\(c\\) are unrelated unless we remove variation in \\(c\\).\nIn other words, we close paths by removing variation from one variable along the path (i.e., adjusting); but if the variable is a collider, then removing variation will actually open a path that was already closed.\nAn often unacknowledged way of adjusting for colliders is during the sample selection phase. If we have a sample of college students, it means we are adjusting for college attendance.\nOpen Path. A path in which there is variation in all variables along the path (and no variation in colliders).\nClosed Path. A path in which there is at least one variable with no variation (or a collider with variation).\n\nThe idea of a directed acyclical graph (DAG) implies that there are no cycles. If a variable causes itself, it’s near impossible to isolate or identifying the cause of anything. The world is full with feedback loops of all sorts, but we deal with them through the incorporation of time or by isolating one effect through some kind of experimental scenario.\nNote that DAGs are agnostic about functional form. This includes interactions among variables! Some people deal with interactions by drawing arrows toward arrows or by representing interactions explicitly as separate nodes.\nThe nicest thing about DAGs is that they help us spell out the testable implications of our assumptions. For example, if our causal diagram implies that a relationship between variables is zero, we can check for that (this is called a placebo test).\n\nPlacebo test\nDon’t confuse with placebo effect!\n\nCausal diagrams also help us understand the idea the idea identification doesn’t necessarily require us to take care of all back door paths; sometimes we can use randomization. Sometimes this is achieved in experimental settings, but sometimes we can form some form of randomization in real-world settings (i.e., natural experiments). As depicted in Figure 1, experiments and instrumental variables have equivalent representations in DAGs.\n\nFinding front doors using randomization\n\n\n\n\nFigure 1: Randomization\n\n\n\nRandomness ensures that the path between unobserved \\(U\\) and the treatment is severed"
  },
  {
    "objectID": "causality-00-intro.html#extra",
    "href": "causality-00-intro.html#extra",
    "title": "Causality",
    "section": "Extra",
    "text": "Extra\n\nlibrary(tidyverse)\ntheme_set(\n  theme_light(base_family = \"Avenir Next Condensed\") +\n  theme(strip.background = element_rect(fill = \"#666666\"))\n)\n\n\nAverage Treatment Effects\n\n18.4 The table below describes a hypothetical experiment on 8 people. Each row of the table gives a participant and her pre-treatment predictor \\(x\\), treatment indicator \\(z\\), and potential outcomes \\(y^0\\) and \\(y^1\\).\n\n\n\nCode\nd <- data.frame(\n  X = c(3, 5, 2, 8, 5, 10, 2, 11),\n  Z = c(0, 0, 1, 0, 0, 1, 1, 1),\n  Y0 = c(5, 8, 5, 12, 4, 8, 4, 9),\n  Y1 = c(5, 10, 3, 13, 2, 9, 1, 13), \n  row.names = LETTERS[1:8]\n)\n\nd <- d |> \n  mutate(Y = ifelse(Z == 0, Y0, Y1))\n\nknitr::kable(d)\n\n\n\n\n\n\nX\nZ\nY0\nY1\nY\n\n\n\n\nA\n3\n0\n5\n5\n5\n\n\nB\n5\n0\n8\n10\n8\n\n\nC\n2\n1\n5\n3\n3\n\n\nD\n8\n0\n12\n13\n12\n\n\nE\n5\n0\n4\n2\n4\n\n\nF\n10\n1\n8\n9\n9\n\n\nG\n2\n1\n4\n1\n1\n\n\nH\n11\n1\n9\n13\n13\n\n\n\n\n\nNaive estimate:\n\ni <- as.logical(d$Z)\n\n## Naive Estimate: \nmean(d$Y1[i] - d$Y0[!i])\n\n[1] -0.75\n\n## ATT:\nmean(d$Y1[i] - d$Y0[i])\n\n[1] 0\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\n\nSimulate a new completely randomized experiment on these 8 people; that is, resample \\(z\\) at random with the constraint that equal numbers get the treatment and the control.\n\n\nj <- sample(i)\n\n## Naive Estimate: \nmean(d$Y1[j] - d$Y0[!j])\n\n[1] 0.75\n\n## ATT:\nmean(d$Y1[j] - d$Y0[j])\n\n[1] 0\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\nAgain:\n\nk <- sample(i)\n\n## Naive Estimate: \nmean(d$Y1[k] - d$Y0[!k])\n\n[1] 0.5\n\n## ATT:\nmean(d$Y1[k] - d$Y0[k])\n\n[1] 0.75\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\n\n\nThe Electric Company\nAdd description of dataset. This is a randomized experiment.\n\n\n\n\n\n\n\nCode\nurl <- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\n\nd <- read_csv(url)[, -1] |> \n  mutate(across(grade:pair_id, as.integer))\n\nglimpse(d)\n\n\nRows: 192\nColumns: 6\n$ post_test <dbl> 48.9, 70.5, 89.7, 44.2, 77.5, 84.7, 78.9, 86.8, 60.8, 75.7, …\n$ pre_test  <dbl> 13.8, 16.5, 18.5, 8.8, 15.3, 15.0, 19.4, 15.0, 11.8, 16.4, 1…\n$ grade     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ treatment <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ supp      <int> 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, …\n$ pair_id   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n\n\nThe supp variable indicates a subtlety in the experiment—i.e., every teacher had the choice of replacing or supplementing the regular reading program with the television show. Thus, this experiment really estimated the effect of making the program available.\n\n\nCode\nd |> \n  mutate(\n    grade = paste(\"Grade\", grade),\n    t = ifelse(treatment == 1, \"Treatment\", \"Control\")) |> \n  ggplot(aes(post_test, y = grade, color = t)) + \n  stat_summary(size = 1/8, fun.args = list(mult = 2), position = position_dodge(1/3)) +\n  labs(y = NULL, color = NULL) + \n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThe point of this exercise is to demonstrate that it’s almost always a good idea to include pre-treatment information when analyzing experimental data.\n\nUnder a clean randomization, adjusting for pre-treatment predictors in this way does not change what we are estimating. However, if the predictor has a strong association with the outcome it can help to bring each estimate closer (on average) to the truth, and if the randomization was less than pristine, the addition of predictors to the equation may help us adjust for systematically unbalanced characteristics across groups. Thus, this strategy has the potential to adjust for both random and systematic differences between the treatment and control groups (that is, to reduce both variance and bias), as long as these differences are characterized by differences in the pre-test.\n368\n\nJust look at the change in coefficient and standard error estimates:\n\nmod1 <- lm(post_test ~ treatment, data = d)\nmod2 <- lm(post_test ~ treatment + pre_test, data = d)\nmod3 <- lm(post_test ~ treatment + pre_test + grade, data = d)\n\nmodelsummary::msummary(\n  models = list(mod1, mod2, mod3), \n  gof_map = NA\n)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    94.321 \n    61.558 \n    63.068 \n  \n  \n     \n    (1.794) \n    (1.471) \n    (1.536) \n  \n  \n    treatment \n    5.657 \n    4.734 \n    4.548 \n  \n  \n     \n    (2.537) \n    (1.160) \n    (1.140) \n  \n  \n    pre_test \n     \n    0.460 \n    0.553 \n  \n  \n     \n     \n    (0.017) \n    (0.036) \n  \n  \n    grade \n     \n     \n    −3.341 \n  \n  \n     \n     \n     \n    (1.163) \n  \n\n\n\n\n\nAnd here’s a separate model for each grade:\n\n\nCode\nfit1 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ treatment, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy(conf.int = TRUE) |> \n    filter(term == \"treatment\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit2 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ treatment + pre_test, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy(conf.int = TRUE) |> \n    filter(term == \"treatment\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit1 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ treatment)\")\n\nfit2 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ treatment + pre_test)\")\n\n\n\n\n\n\n\n\n(a) Regression on treatment indicator\n\n\n\n\n\n\n\n(b) Regression on treatment indicator and pre-test\n\n\n\n\nFigure 2: Estimates, 50%, and 95% intervals for the effect of watching The Electric Company. The same model has been fitted separately for each grade.\n\n\n\nSo, adding pre-treatment information will always make for more precise and less biased estimates. But don’t adjust for post-treatment variables. In fact, always draw a DAG.\nAdjusting for a post-treatment variable \\(q\\) breaks down the assumption of ignorability.\n\\[\nY^0, Y^1 \\not \\perp T \\mid q\n\\]"
  },
  {
    "objectID": "causality-00-intro.html#additional-resources",
    "href": "causality-00-intro.html#additional-resources",
    "title": "Causality",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGeneral Resources\n\nHernan and Robins (2023), Imbens and Rubin (2015), Morgan and Winship (2014), Angrist and Pischke (2009), Ashworth, Berry, and Mesquita (2021)\n\nPotential Outcomes\n\nRubin (2005), Holland (1986)\n\nCausal Diagrams\n\nPearl (2009), Pearl, Glymour, and Jewell (2016)\n\nMachine Learning and Causal Inference\n\nDorie et al. (2019)\n\n\n\n\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless Econometrics. Princeton university press.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita. 2021. Theory and Credibility: Integrating Theoretical and Empirical Social Science. Princeton University Press.\n\n\nBollen, Kenneth A. 1989. Structural Equations with Latent Variables. John Wiley & Sons.\n\n\nDorie, Vincent, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone. 2019. “Automated Versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition.” Statistical Science 34 (1): 43–68. https://doi.org/10.1214/18-STS667.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What If. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60. https://doi.org/10.2307/2289064.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. 1st edition. New York: Cambridge University Press.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals and Causal Inference: Methods and Principles for Social Research. 2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference. 2nd edition. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. 1st edition. Wiley.\n\n\nRubin, Donald B. 2005. “Causal Inference Using Potential Outcomes: Design, Modeling, Decisions.” Journal of the American Statistical Association 100 (469): 322331."
  },
  {
    "objectID": "causality-02-regression.html#translating-dags",
    "href": "causality-02-regression.html#translating-dags",
    "title": "5  Regression Adjustments",
    "section": "5.1 Translating DAGs",
    "text": "5.1 Translating DAGs\n\n\n\nFigure 5.1: Flow Chart For Constructing Regression Equations (Huntington-Klein 2021, 199)\n\n\nSuppose our causal diagram is as follows:\n\n\n\n\n\nFurther suppose that the true data generating process is as follows:\n\\[\ny_i = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i + \\varepsilon_i\n\\]\nIf we omit \\(z\\), then our regression is as follows:\n\\[\ny_i = \\beta_0^* + \\beta_1^* t_i+ \\varepsilon^*_i\n\\]\nFinally, suppose that the association between \\(z\\) and \\(t\\) is defined in terms of a third regression:\n\\[\nz_i = \\gamma_0 + \\gamma_1 t_i + u_i\n\\]\nIf we substitute this representation of \\(t\\) into the original “true” regression and rearrange the terms, we then have the following:\n\\[\ny_i = \\underbrace{(\\beta_0 + \\beta_2 \\gamma_0)}_{\\beta_0^*} + \\underbrace{(\\beta_1 + \\beta_2 \\gamma_1)}_{\\beta_1^*} \\ t_i + \\underbrace{(\\beta_2 u + \\varepsilon)}_{\\varepsilon^*}\n\\]\nThus, if there’s no association between the treatment and the purported confounder (\\(\\gamma_1 = 0\\)), then there is no bias.\nIn R:\n\nN <- 1e3\nb0 <- 1 \nb1 <- 2  ## effect of t\nb2 <- 3\ng0 <- 2\ng1 <- 1  ## source of correlation between t and z\n\nd <- tibble::tibble(\n  z = runif(N, min = -50, max = 50),\n  t = rnorm(N, mean = (z - g0)/g1, sd = 1),\n  y = rnorm(N, mean = b0 + b1*t + b2*z, sd = 1)\n)\n\nmtrue <- lm(y ~ t + z, data = d)\nmconf <- lm(y ~ t, data = d)\n\nmodelsummary::msummary(\n  models = list(\"True DGP\" = mtrue, \"Omitted Variable\" = mconf), \n  gof_map = NA\n)\n\n\n\n \n  \n      \n    True DGP \n     Omitted Variable \n  \n \n\n  \n    (Intercept) \n    1.040 \n    6.910 \n  \n  \n     \n    (0.071) \n    (0.099) \n  \n  \n    t \n    2.017 \n    4.991 \n  \n  \n     \n    (0.032) \n    (0.003) \n  \n  \n    z \n    2.983 \n     \n  \n  \n     \n    (0.032) \n     \n  \n\n\n\n\n\nHere, we see that \\(\\beta_1 \\approx 2\\) and \\(\\beta_1^* \\approx 5\\).\n\n5.1.1 The Electric Company\nSetting up a regression for causal inference\nGoing back to the Electric Company example, we can think of supp, whether teachers chose to replace or supplementing the regular reading program with the television show, as a different treatment.\nGiven that this decision is not randomized, we cannot simply compare outcomes across the two new treatment groups. However, we can assume ignorability if we are willing to believe that pre_test is the only confounding variable—i.e., that the probability of assignment was determined by the average pre-test scores in that classroom.\nThis is a strong assumption, it requires a leap of faith.\n\n\nSetup\nlibrary(tidyverse)\n\ntheme_set(\n  theme_light(base_family = \"Avenir Next Condensed\") +\n  theme(strip.background = element_rect(fill = \"#666666\"))\n)\n\nurl <- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\n\nd <- read_csv(url)[, -1] |> \n  mutate(across(grade:pair_id, as.integer)) |> \n  filter(treatment == 1)\n\n\nJust look at the change in coefficient and standard error estimates:\n\nmod1 <- lm(post_test ~ supp, data = d)\nmod2 <- lm(post_test ~ supp + pre_test, data = d)\nmod3 <- lm(post_test ~ supp + pre_test + grade, \n           data = mutate(d, grade = factor(grade)))\n\nmodelsummary::msummary(\n  models = list(mod1, mod2, mod3), \n  gof_map = NA\n)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    98.189 \n    66.758 \n    61.964 \n  \n  \n     \n    (2.830) \n    (2.377) \n    (2.467) \n  \n  \n    supp \n    2.816 \n    3.468 \n    3.589 \n  \n  \n     \n    (3.550) \n    (1.796) \n    (1.658) \n  \n  \n    pre_test \n     \n    0.424 \n    0.820 \n  \n  \n     \n     \n    (0.026) \n    (0.093) \n  \n  \n    grade2 \n     \n     \n    −24.741 \n  \n  \n     \n     \n     \n    (5.992) \n  \n  \n    grade3 \n     \n     \n    −34.816 \n  \n  \n     \n     \n     \n    (7.613) \n  \n  \n    grade4 \n     \n     \n    −37.935 \n  \n  \n     \n     \n     \n    (8.832) \n  \n\n\n\n\n\nNote. The grade coefficients are increasingly negative because they are positively correlated with pre-test scores. Remember, we cannot interpret confounders causally(see Westreich and Greenland 2013).\nAnd here’s a separate model for each grade:\n\n\nCode\nfit1 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ supp, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy() |> \n    filter(term == \"supp\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit2 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ supp + pre_test, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy() |> \n    filter(term == \"supp\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit1 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ supp)\")\n\nfit2 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ supp + pre_test)\")\n\n\n\n\n\n\n\n\n(a) Regression on non-random treatment indicator\n\n\n\n\n\n\n\n(b) Regression on non-random treatment indicator and pre-test\n\n\n\n\nFigure 5.2: Estimates, 50%, and 95% intervals for the effect of watching The Electric Company as a supplement rather than a replacement. The same model has been fitted separately for each grade."
  },
  {
    "objectID": "causality-02-regression.html#balance-and-overlap",
    "href": "causality-02-regression.html#balance-and-overlap",
    "title": "5  Regression Adjustments",
    "section": "5.3 Balance and Overlap",
    "text": "5.3 Balance and Overlap\nThe sort of bias that we get from confounding can be interpreted more precisely as imbalance in the potential outcomes across treatment groups. This is the sort of imbalance is unlikely with randomization, but it’s almost guaranteed in observational studies.\nThere are different methods that attempt to address imbalance and lack of overlap: stratification, regression adjustments, matching, weighting, or a combination of these.\n\nImbalance and lack of complete overlap are issues for causal inference even if ignorability holds because they force us to rely more heavily on model specification and less on direct support from the data.\nGelman, Hill, and Vehtari (2020, 391)\n\n\n5.3.1 Imbalance\nImbalance occurs if the distributions of confounders differ for the treatment and control groups. Figure 5.3 shows two examples of imbalance with respect to a single covariate \\(x\\). The problem with imbalance is that it forces us to rely more heavily on model specification and less on direct support from the data. This is true even if ignorability holds.\n\n\nCode\nggplot() + \n  xlim(0, 1) +\n  geom_function(fun = \\(x) dbeta(x, 5, 2)) +\n  geom_function(fun = \\(x) dbeta(x, 2, 5), linetype = \"dashed\") +\n  geom_vline(xintercept = c(0.4, 0.6), linetype = \"dotted\") +\n  labs(x = \"X\", y = NULL) +\n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) \n\nggplot() + \n  xlim(0, 1) +\n  geom_function(fun = \\(x) dbeta(x, 3, 2.2), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dbeta(x, 2.2, 3)) +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\") +\n  labs(x = \"X\", y = NULL) +\n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) \n\n\n\n\n\n\n\n\n(a) The groups differ in their averages but cover the same range of \\(x\\)\n\n\n\n\n\n\n\n(b) A more subtle form of imbalance, the groups have the same average but different distributions\n\n\n\n\nFigure 5.3: Imbalance in distributions across treatment and control groups\n\n\n\nConsider what happens if we try to make inferences about the effect of a treatment \\(\\theta\\) on \\(y\\), while adjusting for \\(x\\).\n\\[\n\\begin{align}\n\\text{treated:}  &&y_i &= \\beta_0 + \\theta + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i \\\\ \\\\\n\\text{control:} &&y_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i\n\\end{align}\n\\]\nAveraging over each group separately, solving the second equation for \\(\\beta_0\\), and then plugging that into the first equation to solve for \\(\\theta\\) yields the following equation:\n\\[\n\\theta = (\\overline y_t - \\overline y_c) - \\underbrace{\\beta_1 (\\overline x_t - \\overline x_c) - \\beta_2 (\\overline {x^2_t} - \\overline {x^2_c})}_\\text{adjustment}\n\\tag{5.4}\\]\nThis is why it’s so important to get the model specification right. For example, notice that if we don’t include the quadratic term in the linear model, the estimate of \\(\\theta\\) will be off by \\(\\beta_2 (\\overline {x^2_t} - \\overline {x^2_c})\\).\nMore importantly, if the distribution of covariates is similar across treatment groups, then the model specification matters less—i.e., if there is balance, then \\(\\overline x_t - \\overline x_c\\) is closer to zero. This is why methods that match or weight to create balance may help to create some immunity from failure to correctly specify the model.\n\n\n5.3.2 Lack of Complete Overlap\nFigure 5.4 shows what lack of complete overlap (with respect to \\(x\\)) might look like:\n\n\nCode\nggplot() + \n  xlim(-4, 10) + \n  geom_function(fun = \\(x) dnorm(x, -1, 1), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dnorm(x, 7, 1)) + \n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) + \n  labs(x = \"X\", y = NULL, title = \"No overlap\")\n\nggplot() + \n  xlim(-4, 10) + \n  geom_function(fun = \\(x) dnorm(x, 1, 1), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dnorm(x, 4, 1)) + \n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) + \n  labs(x = \"X\", y = NULL, title = \"Partial overlap\")\n\nggplot() + \n  xlim(-4, 10) + \n  geom_function(fun = \\(x) dnorm(x, 2, 3), linetype = \"dashed\") +\n  geom_function(fun = \\(x) dnorm(x, 7, 1)) + \n  theme_bw(base_family = \"Avenir Next Condensed\", base_line_size = 0) + \n  theme(axis.text = element_blank()) + \n  labs(x = \"X\", y = NULL, title = \"Partial overlap\")\n\n\n\n\n\n\n\n\n(a) Two distributions with no overlap\n\n\n\n\n\n\n\n(b) Two distributions with partial overlap\n\n\n\n\n\n\n\n(c) The range of one distribution is a subset of the range of the other.\n\n\n\n\nFigure 5.4: Lack of complete overlap in distributions across treatment and control groups.\n\n\n\nLack of complete overlap creates problems because in this setting we have treatment or control observations for which we have no empirical counterfactuals. Thus, in regions of non-overlap, knowledge about treatment effects is inherently limited.\nFigure 5.5 shows how inferences regarding areas with no overlap inevitably rely on modeling assumptions. A traditional regression model fitted to data without complete overlap is forced to extrapolate beyond the support of the data.\nNote, however, that even the incorrectly specified linear regression lines provide a decent fit in the overlapping region, as shown in Figure 5.5 (c) and Figure 5.5 (d).\n\n\nCode\nfY1 <- function(x) exp(x/5) + 2*cos(x + 1) + 15\nfY0 <- function(x) 6*log(x + 1) - 0.08*x^2 + 5\n\nd_sim <- tibble(\n  x = runif(100, 0, 10),\n  control = rnorm(length(x), mean = fY0(x), sd = 1),\n  treatment = rnorm(length(x), mean = fY1(x), sd = 1),\n) |> \n  pivot_longer(\n    cols = control:treatment, \n    names_to = \"z\", \n    values_to = \"y\", \n    names_transform = factor\n  ) |> \n  filter(z == \"control\" & x <= 7 | z == \"treatment\" & x >= 3)\n\npar_s <- lm(y ~ x + z, data = d_sim)\nvar_s <- lm(y ~ x*z, data = d_sim)\nsub_par_s <- lm(y ~ x + z, data = d_sim, subset = x >= 3 & x <= 7)\n\ng <- d_sim |> \n  ggplot(aes(x, y)) + \n  geom_function(fun = fY0) + \n  geom_function(fun = fY1) + \n  geom_point(aes(shape = z), show.legend = FALSE, \n             fill = \"skyblue\", color = \"#4C4C4C\", size = 1) + \n  scale_shape_manual(values = c(19, 21)) +\n  labs(x = \"X\", y = \"Y\") +\n  theme(\n    axis.title.y = element_text(angle = 0, vjust = 1/2),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  )\n\nd_grid <- tidyr::crossing(\n  x = seq(0, 10, length.out = 100),\n  z = factor(c(\"control\", \"treatment\"))\n)\n\ng +\n  geom_line(\n    data = broom::augment(par_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\ng +\n  geom_line(\n    data = broom::augment(var_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\ng +\n  coord_cartesian(xlim = c(3, 7)) +\n    geom_line(\n    data = broom::augment(par_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\ng +\n  coord_cartesian(xlim = c(3, 7)) +\n  geom_line(\n    data = broom::augment(sub_par_s, newdata = d_grid),\n    mapping = aes(x, .fitted, group = z),\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\n\n\n(a) A regression estimating the effect of treatment \\(T\\) on \\(Y\\), adjusting for \\(X\\).\n\n\n\n\n\n\n\n(b) Allowing for an interaction sometimes makes extrapolation worse.\n\n\n\n\n\n\n\n\n\n(c) The view is restricted to the area of overlap.\n\n\n\n\n\n\n\n(d) New regression lines fitted using only observations in this overlapping region.\n\n\n\n\nFigure 5.5: Hypothetical data demonstrating the problems with extrapolation when there is lack of overlap between covariates. The “true” causal effect is the vertical distance between the two solid lines. The estimated causal effect is the vertical distance between the two dashed lines.\n\n\n\nTemporal ordering Gelman, Hill, and Vehtari (2020, 414)\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. New York: Chapman; Hall/CRC. https://doi.org/10.1201/9781003226055.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177 (4): 292–98. https://doi.org/10.1093/aje/kws412."
  },
  {
    "objectID": "causality-03-matching.html#exact-matching",
    "href": "causality-03-matching.html#exact-matching",
    "title": "6  Matching",
    "section": "6.1 Exact Matching",
    "text": "6.1 Exact Matching\nThe logic of exact matching is relatively straightforward, we want apples to apples comparisons. This is the same logic behind regression adjustements.\nInstead of a simple independence assumption that we have for randomized experiments, we now have to rely on a conditional independence assumption (CIA).\n\\[\nY^0, Y^1 \\perp T \\mid S\n\\]\nThus, in expectation:\n\\[\n\\begin{align}\nE\\big[Y^1 \\mid T=1, S \\big] &= E\\big[Y^1 \\mid T=0, S \\big] \\\\\nE\\big[Y^0 \\mid T=1, S \\big] &= E\\big[Y^0 \\mid T=0, S \\big]\n\\end{align}\n\\tag{6.1}\\]\n\nConditional Ignorability\n\nThe assumption in Equation 6.1 means that we can observe counterfactuals, conditional on pre-treatment variables \\(S\\). Unfortunately, we cannot test the conditional independence assumption directly.\nThree overall assumptions:\n\nIgnorability or selection on observables.\nOverlap—any individual case has a non-zero probability of treatment.\nStable Unit Treatment Value Assumption (SUTVA)—i.e., no spillovers.\n\nSEE IF ABOVE SHOULD STILL BE HERE\nNote. In cases when we have treatment effect heterogeneity, the ATT, ATU, and ATE estimates will be different, as exemplified in Table 6.1.\n\n\nTable 6.1: Treatment Effect Heterogeneity\n\n\nSES\nN\ndegree\nearnings\n\n\n\n\n1\n150\n0\n2,000\n\n\n1\n50\n1\n4,000\n\n\n2\n100\n0\n6,000\n\n\n2\n100\n1\n8,000\n\n\n3\n50\n0\n10,000\n\n\n3\n150\n1\n14,000\n\n\n\n\nWhat is our estimand (Lundberg, Johnson, and Stewart 2021)?\nThe procedure is relatively straightforward:\n\nTake the differences between treated and untreated groups within each stratum of \\(S\\).\nWeight these differences by the right distribution the estimand we are interested in:\n\nATE: weight the differences by the total sample distribution of \\(S\\).\nATT: weight differences by the distribution of \\(S\\) for treated cases.\n\n\nThus, for Table 6.1:\n\\[\n\\begin{align}\n\\text{ATT} &= \\frac{2000(50) + 2000(100) + 4000(150)}{50 + 100 + 150} = 3,000 \\\\\\\\\n\\text{ATE} &= \\frac{2000 (200) + 2000(200) + 4000(200)}{200 + 200 + 200} = 2,666.7\n\\end{align}\n\\]\nThe ATT is usually our most common estimand because treatment cases tend to be less common than control cases. In some cases, like the one portrayed in Table 6.2, we might not be able to calculate the ATE.\n\n\nTable 6.2: Treatment Effect Heterogeneity (Modified Example)\n\n\n\n\n\n\n\n\nSES\nN\ndegree\nearnings\n\n\n\n\n1\n150\n0\n2,000\n\n\n1\n0\n1\n?\n\n\n2\n100\n0\n6,000\n\n\n2\n100\n1\n8,000\n\n\n3\n50\n0\n10,000\n\n\n3\n150\n1\n14,000\n\n\n\n\nBut the ATT is just as easy to calculate as before, for Table 6.2:\n\\[\n\\text{ATT} = \\frac{(?)(0) + 2000(100) + 4000(150)}{0 + 100 + 150} = 3,200\n\\]\nCommon Support\nStrata that have only treatment or control cases (not both) are called off support. Strata with both treatment and control cases are in the region of common support.\n\n\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review, June, 00031224211004187. https://doi.org/10.1177/00031224211004187."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless\nEconometrics. Princeton university press.\n\n\nAronow, Peter M., and Cyrus Samii. 2017. “Estimating Average\nCausal Effects Under General Interference, with Application to a Social\nNetwork Experiment.” The Annals of Applied Statistics 11\n(4): 1912–47. https://doi.org/10.1214/16-AOAS1005.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita.\n2021. Theory and Credibility: Integrating Theoretical and Empirical\nSocial Science. Princeton University Press.\n\n\nBeecher, Henry K. 1955. “The Powerful Placebo.” Journal\nof the American Medical Association 159 (17): 16021606.\n\n\nBollen, Kenneth A. 1989. Structural Equations with Latent\nVariables. John Wiley & Sons.\n\n\nDorie, Vincent, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone.\n2019. “Automated Versus Do-It-Yourself Methods for Causal\nInference: Lessons Learned from a Data Analysis Competition.”\nStatistical Science 34 (1): 43–68. https://doi.org/10.1214/18-STS667.\n\n\nDruckman, Jamie. 2022. Experimental Thinking. Cambridge\nUniversity Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments:\nDesign, Analysis, and Interpretation. New York: W. W. Norton &\nCompany.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What\nIf. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60. https://doi.org/10.2307/2289064.\n\n\nHudgens, Michael G., and M. Elizabeth Halloran. 2008. “Toward\nCausal Inference with Interference.” Journal of the American\nStatistical Association 103 (482): 832842.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. New York: Chapman; Hall/CRC. https://doi.org/10.1201/9781003226055.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for\nStatistics, Social, and Biomedical Sciences: An Introduction. 1st\nedition. New York: Cambridge University Press.\n\n\nKing, Gary, and Margaret E. Roberts. 2015. “How Robust Standard\nErrors Expose Methodological Problems They Do Not Fix, and What to Do\nAbout It.” Political Analysis 23 (2): 159–79. https://doi.org/10.1093/pan/mpu015.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021.\n“What Is Your Estimand? Defining the Target Quantity Connects\nStatistical Evidence to Theory.” American Sociological\nReview, June, 00031224211004187. https://doi.org/10.1177/00031224211004187.\n\n\nMeissner, Karin, Ulrike Bingel, Luana Colloca, Tor D. Wager, Alison\nWatson, and Magne Arve Flaten. 2011. “The Placebo Effect: Advances\nfrom Different Methodological Approaches.” Journal of\nNeuroscience 31 (45): 1611716124.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals\nand Causal Inference: Methods and Principles for Social Research.\n2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference.\n2nd edition. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. 1st edition. Wiley.\n\n\nRosenbaum, Paul R. 2007. “Interference Between Units in Randomized\nExperiments.” Journal of the American Statistical\nAssociation 102 (477): 191200.\n\n\nRubin, Donald B. 2005. “Causal Inference Using Potential Outcomes:\nDesign, Modeling, Decisions.” Journal of the American\nStatistical Association 100 (469): 322331."
  },
  {
    "objectID": "causality-01-experiments.html#experiments",
    "href": "causality-01-experiments.html#experiments",
    "title": "4  Experiments",
    "section": "4.1 Experiments",
    "text": "4.1 Experiments\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\n\\[\nY^0, Y^1 \\perp T\n\\]\n\nIgnorability\n\nThus, in expectation:\n\\[\n\\begin{align}\nE[Y^0 \\mid T = 0] &= E[Y^0 \\mid T = 1], \\\\\\\\\nE[Y^1 \\mid T = 0] &= E[Y^1 \\mid T = 1]\n\\end{align}\n\\]\n\n…ignorability implies that the value of someone’s potential outcomes does not provide any information about his or her treatment group assignment”.\nGelman, Hill, and Vehtari (2020, pp. 350–1)\n\nWhich means that we can easily estimate the average causal effect of \\(T\\) over all units in a population:\n\\[\n\\text{ATE} = \\underbrace{E[Y^1 - Y^0] = E[Y^1] - E[Y^0]}_\\text{by linearity of expectations}\n\\]\nIn the absence of randomization, so that treatment and control groups differ on pre-treatment characteristics, we might observe the following biases:\n\nBaseline bias. The two groups might be different from each other whether they get treated or not.\nTreatment effect heterogeneity. The two groups might respond differently to the treatment.\nThis is important because researchers sometimes assume a constant effect for every unit in the population. This is implied in the unit homogeneity assumption. But if the variability of causal effects is large across the population, then the ATE might not represent the causal effect of a specific unit very well; the ATE might be irrelevant, no matter how carefully we estimate it.\n\nThree types of treatment effects:\n\n\\(\\text{ATE}\\), for all units (effect of switching)\n\\(\\text{ATT}\\), for treated units (effect of taking away treatment)\n\\(\\text{ATC}\\) or \\(\\text{ATU}\\), for untreated units (effect of adding treatment)\n\n\nSome times we see these terms pre-fixed with an \\(S\\) (for sample) or a \\(P\\) (for population).\n\nTable 4.1 allows us to gain further intuition on these calculations with a hypothetical example in which people are assigned college degrees randomly.\n\n\nTable 4.1: Potential Outcomes Example\n\n\nGroup (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\nCollege Degree (\\(T = 1\\))\n1000\n600*\n\n\nNo Degree (\\(T = 0\\))\n800*\n500\n\n\n\n\n\n\nSimple experiments of these kind are obviously impossible in the social sciences.\n* means unobservable\n\nIf 30% of the population has a degree…\n\n\n\nWhat is the naive estimate?\n500\nWhat is the ATT?\n400\n\n\n\n\nWhat is the ATC?\n300\nWhat is the ATE?\n\\(0.3\\times 400 + 0.7 \\times 300 = 330\\)\n\n\n\nMore than two treatment levels, continuous treatments, and multiple treatment factors\nMultiple treatment effects can be defined relative to a baseline level, following the general principles of regression modeling with indicator (or dummy) variables.\nTreatment levels can be continuous.\n\nTo conceptualize randomization with a continuous treatment, think of spinning a spinner that can land on any of the potential levels of the treatment assignment. As with regression inputs in general, it can make sense to fit more complicated models as suggested by theory or supported by data. A linear model—which estimates the average effect on \\(y\\) for each additional unit of \\(z\\)—is a natural starting point for effects that are believed to be monotonically increasing or decreasing functions of the treatment level.\nGelman, Hill, and Vehtari (2020, 342)\n\nFinally, we can also consider multiple simultaneous treatments.\n\n…multiple treatments can be administered in combination. For instance, depressed individuals could be randomly assigned to receive nothing, drugs, counseling sessions, or both drugs and counseling sessions. These combinations could be modeled as two treatments and their interaction or as four distinct treatments.\nGelman, Hill, and Vehtari (2020, 342)\n\nUsing design and analysis to address imbalance and lack of overlap between treatment and control groups\nRegression can be useful even in the context of randomized experiments.\n\nIn practice, we can never ensure that treatment and control groups are balanced on all relevant pre-treatment characteristics. However, there are statistical approaches that may bring us closer. At the design stage, we can use randomization to ensure that treatment and control groups are balanced in expectation, and we can use blocking to reduce the variation in any imbalance. At the analysis stage, we can adjust for pre-treatment variables to correct for differences between the two groups to reduce bias in our estimate of the sample average treatment effect. We can further adjust for differences between sample and population if our goal is to estimate the population average treatment effect.\nGelman, Hill, and Vehtari (2020, 344)\n\nWe can increase the precision of treatment effect estimates by adjusting for pre-treatment variables that are predictive of the outcome—i.e., we get lower standard errors on the treatment parameter.\nGroup or cluster-randomized experiments\nSometimes it’s difficult to randomize treatment assignments to individual units, and so we might randomize groups or clusters to receive the treatment instead.\n\nA decision to assign treatments at the group level can be driven by cost or logistical concerns. It might be more cost effective to provide free flu shots to a random subset of health clinics, for example, than to have professionals go to every clinic and then randomly assign individuals to receive shots. Assignment at the clinic level would also avoid creating ill will among potential study participants being deprived of a service that others in the same location are able to receive. Cluster-randomized experiments are also used to avoid spillover or contagion effects (which can also be considered as violations of the stable unit treatment value assumption or SUTVA).\nGelman, Hill, and Vehtari (2020, 349–50)"
  },
  {
    "objectID": "causality-01-experiments.html#sutva",
    "href": "causality-01-experiments.html#sutva",
    "title": "4  Experiments",
    "section": "4.2 SUTVA",
    "text": "4.2 SUTVA\nBesides ignorability (see ?eq-ignore), randomization implies certain other properties which we might consider to be assumptions.\nThe most important one is the stable unit treatment value assumption (SUTVA). It means that we assume that there is no interference among units (i.e., no spillovers) and no hidden versions of the treatment.\nThus, one can imagine person \\(i\\)’s outcome to be “a function not only of her own treatment assignment, but also the treatment assignments of others in the sample” (Gelman, Hill, and Vehtari 2020, 353). These becomes intractable very quickly. For example, in a sample of just 10 people and a binary treatment, we would have 210 = 1024 potential outcomes for each person. Thus, researchers often hope that there is no interference among units, or else they think about modeling spillover in some way.\nSUTVA also implies that there are no hidden versions of treatments—i.e., we want \\(t_i\\) to equal \\(t_j\\).\n\nExamples of potential SUTVA violations abound. An experiment testing the effect of a new fertilizer by randomly assigning adjacent plots to treatment or control is a classic example. Fertilizer from one plot might leach into an adjacent plot assigned to receive no fertilizer and thus affect the yield in that control plot. Vaccines that reduce the probability of a contagious disease within a school, business, or community could easily lead to violation of SUTVA if the vaccine is actually effective. Consider an experiment that recruited families from the same public housing complex and randomized them to receive a voucher to move to a better neighborhood or not. This could suffer from interference if a given family moving might influence (positively or negatively) the well-being of another family that happened to be randomized to not receive the voucher.\nGelman, Hill, and Vehtari (2020, 353)\n\nIn educational settings, potential SUTVA violations are often a reason to assign treatments at the classroom or school level."
  },
  {
    "objectID": "causality-01-experiments.html#extra",
    "href": "causality-01-experiments.html#extra",
    "title": "4  Experiments",
    "section": "4.3 Extra",
    "text": "4.3 Extra\n\nlibrary(tidyverse)\ntheme_set(\n  theme_light(base_family = \"Avenir Next Condensed\") +\n  theme(strip.background = element_rect(fill = \"#666666\"))\n)\n\n\n4.3.1 Average Treatment Effects\n\n18.4 The table below describes a hypothetical experiment on 8 people. Each row of the table gives a participant and her pre-treatment predictor \\(x\\), treatment indicator \\(z\\), and potential outcomes \\(y^0\\) and \\(y^1\\).\n\n\n\nCode\nd <- data.frame(\n  X = c(3, 5, 2, 8, 5, 10, 2, 11),\n  Z = c(0, 0, 1, 0, 0, 1, 1, 1),\n  Y0 = c(5, 8, 5, 12, 4, 8, 4, 9),\n  Y1 = c(5, 10, 3, 13, 2, 9, 1, 13), \n  row.names = LETTERS[1:8]\n)\n\nd <- d |> \n  mutate(Y = ifelse(Z == 0, Y0, Y1))\n\nknitr::kable(d)\n\n\n\n\n\n\nX\nZ\nY0\nY1\nY\n\n\n\n\nA\n3\n0\n5\n5\n5\n\n\nB\n5\n0\n8\n10\n8\n\n\nC\n2\n1\n5\n3\n3\n\n\nD\n8\n0\n12\n13\n12\n\n\nE\n5\n0\n4\n2\n4\n\n\nF\n10\n1\n8\n9\n9\n\n\nG\n2\n1\n4\n1\n1\n\n\nH\n11\n1\n9\n13\n13\n\n\n\n\n\nNaive estimate:\n\ni <- as.logical(d$Z)\n\n## Naive Estimate: \nmean(d$Y1[i] - d$Y0[!i])\n\n[1] -0.75\n\n## ATT:\nmean(d$Y1[i] - d$Y0[i])\n\n[1] 0\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\n\nSimulate a new completely randomized experiment on these 8 people; that is, resample \\(z\\) at random with the constraint that equal numbers get the treatment and the control.\n\n\nj <- sample(i)\n\n## Naive Estimate: \nmean(d$Y1[j] - d$Y0[!j])\n\n[1] 3.75\n\n## ATT:\nmean(d$Y1[j] - d$Y0[j])\n\n[1] 1\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\nAgain:\n\nk <- sample(i)\n\n## Naive Estimate: \nmean(d$Y1[k] - d$Y0[!k])\n\n[1] 2\n\n## ATT:\nmean(d$Y1[k] - d$Y0[k])\n\n[1] 0.75\n\n## ATE:\nmean(d$Y1 - d$Y0)\n\n[1] 0.125\n\n\n\n\n4.3.2 The Electric Company\nAdd description of dataset. This is a randomized experiment.\n\n\n\n\n\n\n\nCode\nurl <- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\n\nd <- read_csv(url)[, -1] |> \n  mutate(across(grade:pair_id, as.integer))\n\nglimpse(d)\n\n\nRows: 192\nColumns: 6\n$ post_test <dbl> 48.9, 70.5, 89.7, 44.2, 77.5, 84.7, 78.9, 86.8, 60.8, 75.7, …\n$ pre_test  <dbl> 13.8, 16.5, 18.5, 8.8, 15.3, 15.0, 19.4, 15.0, 11.8, 16.4, 1…\n$ grade     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ treatment <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ supp      <int> 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, …\n$ pair_id   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n\n\nThe supp variable indicates a subtlety in the experiment—i.e., every teacher had the choice of replacing or supplementing the regular reading program with the television show. Thus, this experiment really estimated the effect of making the program available.\n\nThe notebook on regression adjustments and causal inference revisits the supp variable as a different treatment on its own.\n\n\n\nCode\nd |> \n  mutate(\n    grade = paste(\"Grade\", grade),\n    t = ifelse(treatment == 1, \"Treatment\", \"Control\")) |> \n  ggplot(aes(post_test, y = grade, color = t)) + \n  stat_summary(size = 1/8, fun.args = list(mult = 2), position = position_dodge(1/3)) +\n  labs(y = NULL, color = NULL) + \n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThe point of this exercise is to demonstrate that it’s almost always a good idea to include pre-treatment information when analyzing experimental data.\n\nUnder a clean randomization, adjusting for pre-treatment predictors in this way does not change what we are estimating. However, if the predictor has a strong association with the outcome it can help to bring each estimate closer (on average) to the truth, and if the randomization was less than pristine, the addition of predictors to the equation may help us adjust for systematically unbalanced characteristics across groups. Thus, this strategy has the potential to adjust for both random and systematic differences between the treatment and control groups (that is, to reduce both variance and bias), as long as these differences are characterized by differences in the pre-test.\nGelman, Hill, and Vehtari (2020, 368)\n\nJust look at the change in coefficient and standard error estimates:\n\nmod1 <- lm(post_test ~ treatment, data = d)\nmod2 <- lm(post_test ~ treatment + pre_test, data = d)\nmod3 <- lm(post_test ~ treatment + pre_test + grade, \n           data = mutate(d, grade = factor(grade)))\n\nmodelsummary::msummary(\n  models = list(mod1, mod2, mod3), \n  gof_map = NA\n)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    94.321 \n    61.558 \n    58.456 \n  \n  \n     \n    (1.794) \n    (1.471) \n    (1.485) \n  \n  \n    treatment \n    5.657 \n    4.734 \n    4.052 \n  \n  \n     \n    (2.537) \n    (1.160) \n    (1.063) \n  \n  \n    pre_test \n     \n    0.460 \n    0.800 \n  \n  \n     \n     \n    (0.017) \n    (0.055) \n  \n  \n    grade2 \n     \n     \n    −21.722 \n  \n  \n     \n     \n     \n    (3.504) \n  \n  \n    grade3 \n     \n     \n    −29.846 \n  \n  \n     \n     \n     \n    (4.666) \n  \n  \n    grade4 \n     \n     \n    −32.870 \n  \n  \n     \n     \n     \n    (5.242) \n  \n\n\n\n\n\nNote. The grade coefficients are increasingly negative because they are positively correlated with pre_test. Remember, we can’t expect to interpret these coefficients causally (Westreich and Greenland 2013).\nAnd here’s a separate model for each grade:\n\n\nCode\nfit1 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ treatment, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy(conf.int = TRUE) |> \n    filter(term == \"treatment\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit2 <- map_df(1:4, function(i) {\n  lm(formula = post_test ~ treatment + pre_test, \n     data = d, \n     subset = grade == i) |> \n    broom::tidy(conf.int = TRUE) |> \n    filter(term == \"treatment\") |> \n    mutate(grade = paste(\"Grade\", i))\n})\n\nfit1 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ treatment)\")\n\nfit2 |> \n  ggplot(aes(estimate, grade)) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.05),\n    xmax = estimate + std.error*qnorm(0.95)), \n  ) +\n  geom_linerange(aes(\n    xmin = estimate + std.error*qnorm(0.25),\n    xmax = estimate + std.error*qnorm(0.75)\n    ), linewidth = 1.5\n  ) +\n  geom_point(shape = 21, fill = \"white\") + \n  labs(y = NULL, x = \"treatment\",\n       title = \"lm(post_treat ~ treatment + pre_test)\")\n\n\n\n\n\n\n\n\n(a) Regression on treatment indicator\n\n\n\n\n\n\n\n(b) Regression on treatment indicator and pre-test\n\n\n\n\nFigure 4.1: Estimates, 50%, and 95% intervals for the effect of watching The Electric Company. The same model has been fitted separately for each grade.\n\n\n\nSo, adding pre-treatment information will always make for more precise and less biased estimates. But don’t adjust for post-treatment variables. In fact, always draw a DAG.\nAdjusting for a post-treatment variable \\(q\\) breaks down the assumption of ignorability.\n\\[\nY^0, Y^1 \\not \\perp T \\mid q\n\\]"
  },
  {
    "objectID": "causality-01-experiments.html#additional-resources",
    "href": "causality-01-experiments.html#additional-resources",
    "title": "4  Experiments",
    "section": "4.4 Additional Resources",
    "text": "4.4 Additional Resources\n\nExperiments\n\nDruckman (2022), Gerber and Green (2012)\n\nInterference Among Units\n\nRosenbaum (2007), Hudgens and Halloran (2008), Aronow and Samii (2017).\n\nPlacebo effects\n\nMeissner et al. (2011), Beecher (1955)\n\n\n\n\n\n\nAronow, Peter M., and Cyrus Samii. 2017. “Estimating Average Causal Effects Under General Interference, with Application to a Social Network Experiment.” The Annals of Applied Statistics 11 (4): 1912–47. https://doi.org/10.1214/16-AOAS1005.\n\n\nBeecher, Henry K. 1955. “The Powerful Placebo.” Journal of the American Medical Association 159 (17): 16021606.\n\n\nDruckman, Jamie. 2022. Experimental Thinking. Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: W. W. Norton & Company.\n\n\nHudgens, Michael G., and M. Elizabeth Halloran. 2008. “Toward Causal Inference with Interference.” Journal of the American Statistical Association 103 (482): 832842.\n\n\nMeissner, Karin, Ulrike Bingel, Luana Colloca, Tor D. Wager, Alison Watson, and Magne Arve Flaten. 2011. “The Placebo Effect: Advances from Different Methodological Approaches.” Journal of Neuroscience 31 (45): 1611716124.\n\n\nRosenbaum, Paul R. 2007. “Interference Between Units in Randomized Experiments.” Journal of the American Statistical Association 102 (477): 191200.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177 (4): 292–98. https://doi.org/10.1093/aje/kws412."
  },
  {
    "objectID": "causality-01-experiments.html#introduction",
    "href": "causality-01-experiments.html#introduction",
    "title": "4  Experiments",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\n\\[\nY^0, Y^1 \\perp T\n\\]\n\nIgnorability\n\nThus, in expectation:\n\\[\n\\begin{align}\nE[Y^0 \\mid T = 0] &= E[Y^0 \\mid T = 1], \\\\\\\\\nE[Y^1 \\mid T = 0] &= E[Y^1 \\mid T = 1]\n\\end{align}\n\\]\n\n…ignorability implies that the value of someone’s potential outcomes does not provide any information about his or her treatment group assignment”.\nGelman, Hill, and Vehtari (2020, pp. 350–1)\n\nWhich means that we can easily estimate the average causal effect of \\(T\\) over all units in a population:\n\\[\n\\text{ATE} = \\underbrace{E[Y^1 - Y^0] = E[Y^1] - E[Y^0]}_\\text{by linearity of expectations}\n\\]\nIn the absence of randomization, so that treatment and control groups differ on pre-treatment characteristics, we might observe the following biases:\n\nBaseline bias. The two groups might be different from each other whether they get treated or not.\nTreatment effect heterogeneity. The two groups might respond differently to the treatment.\nThis is important because researchers sometimes assume a constant effect for every unit in the population. This is implied in the unit homogeneity assumption. But if the variability of causal effects is large across the population, then the ATE might not represent the causal effect of a specific unit very well; the ATE might be irrelevant, no matter how carefully we estimate it.\n\nThree types of treatment effects:\n\n\\(\\text{ATE}\\), for all units (effect of switching)\n\\(\\text{ATT}\\), for treated units (effect of taking away treatment)\n\\(\\text{ATC}\\) or \\(\\text{ATU}\\), for untreated units (effect of adding treatment)\n\n\nSome times we see these terms pre-fixed with an \\(S\\) (for sample) or a \\(P\\) (for population).\n\nTable 4.1 allows us to gain further intuition on these calculations with a hypothetical example in which people are assigned college degrees randomly.\n\n\nTable 4.1: Potential Outcomes Example\n\n\nGroup (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\nCollege Degree (\\(T = 1\\))\n1000\n600*\n\n\nNo Degree (\\(T = 0\\))\n800*\n500\n\n\n\n\n\n\nSimple experiments of these kind are obviously impossible in the social sciences.\n* means unobservable\n\nIf 30% of the population has a degree…\n\n\n\nWhat is the naive estimate?\n500\nWhat is the ATT?\n400\n\n\n\n\nWhat is the ATC?\n300\nWhat is the ATE?\n\\(0.3\\times 400 + 0.7 \\times 300 = 330\\)\n\n\n\nMore than two treatment levels, continuous treatments, and multiple treatment factors\nMultiple treatment effects can be defined relative to a baseline level, following the general principles of regression modeling with indicator (or dummy) variables.\nTreatment levels can be continuous.\n\nTo conceptualize randomization with a continuous treatment, think of spinning a spinner that can land on any of the potential levels of the treatment assignment. As with regression inputs in general, it can make sense to fit more complicated models as suggested by theory or supported by data. A linear model—which estimates the average effect on \\(y\\) for each additional unit of \\(z\\)—is a natural starting point for effects that are believed to be monotonically increasing or decreasing functions of the treatment level.\nGelman, Hill, and Vehtari (2020, 342)\n\nFinally, we can also consider multiple simultaneous treatments.\n\n…multiple treatments can be administered in combination. For instance, depressed individuals could be randomly assigned to receive nothing, drugs, counseling sessions, or both drugs and counseling sessions. These combinations could be modeled as two treatments and their interaction or as four distinct treatments.\nGelman, Hill, and Vehtari (2020, 342)\n\nUsing design and analysis to address imbalance and lack of overlap between treatment and control groups\nRegression can be useful even in the context of randomized experiments.\n\nIn practice, we can never ensure that treatment and control groups are balanced on all relevant pre-treatment characteristics. However, there are statistical approaches that may bring us closer. At the design stage, we can use randomization to ensure that treatment and control groups are balanced in expectation, and we can use blocking to reduce the variation in any imbalance. At the analysis stage, we can adjust for pre-treatment variables to correct for differences between the two groups to reduce bias in our estimate of the sample average treatment effect. We can further adjust for differences between sample and population if our goal is to estimate the population average treatment effect.\nGelman, Hill, and Vehtari (2020, 344)\n\nWe can increase the precision of treatment effect estimates by adjusting for pre-treatment variables that are predictive of the outcome—i.e., we get lower standard errors on the treatment parameter.\nGroup or cluster-randomized experiments\nSometimes it’s difficult to randomize treatment assignments to individual units, and so we might randomize groups or clusters to receive the treatment instead.\n\nA decision to assign treatments at the group level can be driven by cost or logistical concerns. It might be more cost effective to provide free flu shots to a random subset of health clinics, for example, than to have professionals go to every clinic and then randomly assign individuals to receive shots. Assignment at the clinic level would also avoid creating ill will among potential study participants being deprived of a service that others in the same location are able to receive. Cluster-randomized experiments are also used to avoid spillover or contagion effects (which can also be considered as violations of the stable unit treatment value assumption or SUTVA).\nGelman, Hill, and Vehtari (2020, 349–50)"
  },
  {
    "objectID": "causality-01-experiments.html#notation",
    "href": "causality-01-experiments.html#notation",
    "title": "4  Experiments",
    "section": "4.1 Notation",
    "text": "4.1 Notation\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\n\\[\nY^0, Y^1 \\perp T\n\\tag{4.1}\\]\n\nIgnorability\n\nThus, in expectation:\n\\[\n\\begin{align}\nE[Y^0 \\mid T = 0] &= E[Y^0 \\mid T = 1], \\\\\\\\\nE[Y^1 \\mid T = 0] &= E[Y^1 \\mid T = 1]\n\\end{align}\n\\tag{4.2}\\]\n\n…ignorability implies that the value of someone’s potential outcomes does not provide any information about his or her treatment group assignment”.\nGelman, Hill, and Vehtari (2020, pp. 350–1)\n\nWhich means that we can easily estimate the average causal effect of \\(T\\) over all units in a population:\n\\[\n\\text{ATE} = \\underbrace{E[Y^1 - Y^0] = E[Y^1] - E[Y^0]}_\\text{by linearity of expectations}\n\\tag{4.3}\\]\nIn the absence of randomization, so that treatment and control groups differ on pre-treatment characteristics, we might observe the following biases:\n\nBaseline bias. The two groups might be different from each other whether they get treated or not.\nTreatment effect heterogeneity. The two groups might respond differently to the treatment.\nThis is important because researchers sometimes assume a constant effect for every unit in the population. This is implied in the unit homogeneity assumption. But if the variability of causal effects is large across the population, then the ATE might not represent the causal effect of a specific unit very well; the ATE might be irrelevant, no matter how carefully we estimate it.\n\nThree types of treatment effects:\n\n\\(\\text{ATE}\\), for all units (effect of switching)\n\\(\\text{ATT}\\), for treated units (effect of taking away treatment)\n\\(\\text{ATC}\\) or \\(\\text{ATU}\\), for untreated units (effect of adding treatment)\n\n\nSome times we see these terms pre-fixed with an \\(S\\) (for sample) or a \\(P\\) (for population).\n\nTable 4.1 allows us to gain further intuition on these calculations with a hypothetical example in which people are assigned college degrees randomly.\n\n\nTable 4.1: Potential Outcomes Example\n\n\nGroup (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\nCollege Degree (\\(T = 1\\))\n1000\n600*\n\n\nNo Degree (\\(T = 0\\))\n800*\n500\n\n\n\n\n\n\nSimple experiments of these kind are obviously impossible in the social sciences.\n* means unobservable\n\nIf 30% of the population has a degree…\n\n\n\nWhat is the naive estimate?\n500\nWhat is the ATT?\n400\n\n\n\n\nWhat is the ATC?\n300\nWhat is the ATE?\n\\(0.3\\times 400 + 0.7 \\times 300 = 330\\)\n\n\n\nMore than two treatment levels, continuous treatments, and multiple treatment factors\nMultiple treatment effects can be defined relative to a baseline level, following the general principles of regression modeling with indicator (or dummy) variables.\nTreatment levels can be continuous.\n\nTo conceptualize randomization with a continuous treatment, think of spinning a spinner that can land on any of the potential levels of the treatment assignment. As with regression inputs in general, it can make sense to fit more complicated models as suggested by theory or supported by data. A linear model—which estimates the average effect on \\(y\\) for each additional unit of \\(z\\)—is a natural starting point for effects that are believed to be monotonically increasing or decreasing functions of the treatment level.\nGelman, Hill, and Vehtari (2020, 342)\n\nFinally, we can also consider multiple simultaneous treatments.\n\n…multiple treatments can be administered in combination. For instance, depressed individuals could be randomly assigned to receive nothing, drugs, counseling sessions, or both drugs and counseling sessions. These combinations could be modeled as two treatments and their interaction or as four distinct treatments.\nGelman, Hill, and Vehtari (2020, 342)\n\nUsing design and analysis to address imbalance and lack of overlap between treatment and control groups\nRegression can be useful even in the context of randomized experiments.\n\nIn practice, we can never ensure that treatment and control groups are balanced on all relevant pre-treatment characteristics. However, there are statistical approaches that may bring us closer. At the design stage, we can use randomization to ensure that treatment and control groups are balanced in expectation, and we can use blocking to reduce the variation in any imbalance. At the analysis stage, we can adjust for pre-treatment variables to correct for differences between the two groups to reduce bias in our estimate of the sample average treatment effect. We can further adjust for differences between sample and population if our goal is to estimate the population average treatment effect.\nGelman, Hill, and Vehtari (2020, 344)\n\nWe can increase the precision of treatment effect estimates by adjusting for pre-treatment variables that are predictive of the outcome—i.e., we get lower standard errors on the treatment parameter.\nGroup or cluster-randomized experiments\nSometimes it’s difficult to randomize treatment assignments to individual units, and so we might randomize groups or clusters to receive the treatment instead.\n\nA decision to assign treatments at the group level can be driven by cost or logistical concerns. It might be more cost effective to provide free flu shots to a random subset of health clinics, for example, than to have professionals go to every clinic and then randomly assign individuals to receive shots. Assignment at the clinic level would also avoid creating ill will among potential study participants being deprived of a service that others in the same location are able to receive. Cluster-randomized experiments are also used to avoid spillover or contagion effects (which can also be considered as violations of the stable unit treatment value assumption or SUTVA).\nGelman, Hill, and Vehtari (2020, 349–50)"
  },
  {
    "objectID": "causality-02-regression.html#notation",
    "href": "causality-02-regression.html#notation",
    "title": "5  Regression Adjustments",
    "section": "5.2 Notation",
    "text": "5.2 Notation\nThe logic of regression adjustments is relatively straightforward, we want apples to apples comparisons. This is the same logic behind matching and weighting.\nInstead of a simple independence assumption that we have for randomized experiments, we now have to rely on a conditional ignorability. Just like in the case of experiments, we want distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator should be independent, conditional on the covariates \\(\\boldsymbol{X}\\) used in the analysis.\n\nIgnorability, Conditional Independence Assumption (CIA), or Selection on Observables\n\n\\[\nY^0, Y^1 \\perp T \\mid \\boldsymbol X\n\\tag{5.1}\\]\nThus, in expectation:\n\\[\n\\begin{align}\nE\\big[Y^1 \\mid T=1, \\boldsymbol{X} \\big] &= E\\big[Y^1 \\mid T=0, \\boldsymbol{X} \\big] \\\\\nE\\big[Y^0 \\mid T=1, \\boldsymbol{X} \\big] &= E\\big[Y^0 \\mid T=0, \\boldsymbol{X} \\big]\n\\end{align}\n\\tag{5.2}\\]\nThese conditional quantities can be used to construct the marginal effects needed to construct the average treatment effect.\n\\[\n\\text{ATE} = E[Y^1 - Y^0] = \\underbrace{E\\Big[E[Y^1 \\mid \\boldsymbol{X}] \\Big] - E \\Big[E[Y^0 \\mid \\boldsymbol{X}] \\Big] = E[Y^1] - E[Y^0]}_\\text{Law of Iterated Expectations}\n\\tag{5.3}\\]\nThis strategy will get more complicated as the vector \\(\\boldsymbol X\\) grows in size.\n\nOnce the number and type of confounders gets more complicated, perhaps the simplest parametric model that we can fit to estimate these expectations is linear regression. For instance, if we assume that the treatment effect, \\(\\tau\\), is constant (or at least additive) we might posit that \\(E( Y^Z \\mid \\boldsymbol X) = \\beta_0+ \\boldsymbol{X \\beta}+ \\tau Z\\) . If ignorability is satisfied and this model holds, we simply need to regress the outcome on the treatment indicator and confounders. The estimated coefficient on \\(Z\\) from this fit, \\(\\hat \\tau\\), can be conceptualized as a weighted version of all of the conditional effect estimates. However, fitting a model to estimate these quantities is not without potential weaknesses. The two most obvious concerns are imbalance and lack of complete overlap…\nGelman, Hill, and Vehtari (2020, 390)\n\nNote. This sometimes shows up in econometrics as the exogeneity assumption."
  }
]