# Causality

*Here are the major types of research designs associated with causal inference:*

-   **Randomized experiments**

-   **Adjusting for confounding**

-   **Matching/weighting**

-   **Difference-in-Differences**

-   **Regression discontinuity**

-   **Instrumental variables (IV)**

The first three strategies rely on the assumption of **ignorability** (or selection on observables). The last three strategies rely on a slightly different set of assumptions that may be more plausible in certain settings.

A different approach altogether involves assessing the **sensitivity** of our treatment effect estimates.

------------------------------------------------------------------------

Not all people agree about the definition of causality.

One group of people emphasizes ***intervention*** or *treatment.*

> NO CAUSATION WITHOUT MANIPULATION
>
> @holland1986 [pp. 959]

These people adopt the so-called **potential outcomes framework**. Here, the word cause is reserved for things that can be treatments in an experiment, at least in principle. This includes "natural" and "hypothetical" experiments.

<aside>*i.e., a causal effect is conceptualized as a comparison between different potential outcomes of what might have occurred under different scenarios.*</aside>

Thus, it's important to distinguish between *causes* and *attributes.*

For example:

-   "She did well on the exam because she is a woman \[*attribute*\]."

    An attribute cannot be a cause in an experiment; thus, the only way for an attribute to change its value is for the same unit to undergo transformative change.

    *However, attributes might be central in understanding **heterogeneous treatment effects**.*

-   "She did well on the exam because she was coached by her teacher \[*treatment*\]."

    This is OK.

Consider the famous example of musicians auditioning for jobs at orchestras behind screens in order to prevent discrimination. We can think of the binary treatment $T$ as an indicator of whether the audition was performed behind the screen or not; $Y$ indicates whether the applicant gets a job offer or not; and we have other background variables $X$ that cannot be manipulated, like sex. Here, we would most likely care about the interaction between $T$ and $X$ (i.e., treatment effect heterogeneity).

@gelman2020 further discuss the importance of defining *manipulable* "treatment" variables.

> ...consider the oft-studied "effect" of height on earnings. This effect is ill-defined without reference to the particular treatment that could change one's height. Otherwise, what does it mean to define a potential outcome for a person that would occur if he or she had been shorter or taller? A hypothetical swapping of genes, for example, would have different effects than a change in the fetal environment, which in turn is different from changing a child's diet or introducing medication such as growth hormone.
>
> @gelman2020 [pp. 414]

*Others disagree.*

> This view leads to other counterintuitive ideas about causation: the moon does not cause the tides, tornadoes and hurricanes do not cause the destruction of property, and so on.
>
> @bollen1989 [pp. 41]

Thus, these researchers believe that manipulation is neither a necessary nor sufficient condition for causality. @bollen1989 [pp. 41] argues that we only need to think about isolation, association, and direction of influence. @pearl2009 has a similar but more elaborate framework.

## Notation

The difference between *seeing* and *doing* is enormous. It is the reason why we do not regard movements in a thermostat to be a cause of the temperature outside.

$$
\Pr(Y \mid T) \neq \Pr(Y \mid do(T))
$$

<aside>Pearl</aside>

**Potential Outcomes**

-   $T$ is a binary treatment variable. The terms "treatment" and "cause" are used interchangeably.

-   $Y$ is the outcome we observe.

-   $Y^0$ is the the value the outcome *would* take if $T=0$.

-   $Y^1$ is the value the outcome *would* take if $T=1$.

-   $Y^0$ and $Y^1$ are the *potential outcomes.*

-   We see $Y^0$ *or* $Y^1$ for the same unit, but never both. This is the **fundamental problem of causal inference**.

-   When $T=1$, $Y^0$ is the *counterfactual.*

-   When $T=0$, $Y^1$ is the *counterfactual.*

<aside>Rubin</aside>

One might object to the fundamental problem of causal inference by noting situations where we can actually measure both $Y^0$ and $Y^1$ on the same unit $i$. For example, we could drink warm milk one evening and coffee another evening, and then measure our amount of sleep time. @holland1986 refers to this as **temporal stability and causal transience**. This is the same situation in which we flick the same light switch on and off to figure out if it turns on the light to a room. In other settings, it is perfectly reasonable to assume that $Y^1_i = Y^1_j$ and that $Y^0_i = Y^0_j$ (**unit homogeneity assumption**). For example, this is very common in laboratories that take special care in standardizing the units of an experiment (e.g., mice).

<aside>

These objections cannot usually be made in most social science applications.

However, matched pair experiments of identical twins come close to this homogeneity assumption.

</aside>

## Experiments

Experiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are *independent.*

$$
Y^0, Y^1 \perp T
$$ {#eq-ignore}

<aside>*Ignorability*</aside>

Thus, in expectation:

$$
\begin{align}
E[Y^0 \mid T = 0] &= E[Y^0 \mid T = 1], \\\\
E[Y^1 \mid T = 0] &= E[Y^1 \mid T = 1]
\end{align}
$$ {#eq-ind}

<aside>

*...ignorability implies that the value of someone's potential outcomes does not provide any information about his or her treatment group assignment".*

@gelman2020 [*pp. 350-1*]

</aside>

Which means that we can easily estimate the *average causal effect* of $T$ over all units in a population:

$$
\text{ATE} = \underbrace{E[Y^1 - Y^0] = E[Y^1] - E[Y^0]}_\text{by linearity of expectations}
$$ {#eq-ate}

In the absence of randomization, so that treatment and control groups differ on pre-treatment characteristics, we might observe the following biases:

-   **Baseline bias**. The two groups might be different from each other whether they get treated or not.

-   **Treatment effect heterogeneity.** The two groups might respond differently to the treatment.

    This is important because researchers sometimes assume a **constant effect** for every unit in the population. This is implied in the unit homogeneity assumption. But if the variability of causal effects is large across the population, then the ATE might not represent the causal effect of a specific unit very well; the ATE might be irrelevant, no matter how carefully we estimate it.

Three types of treatment effects:

-   $\text{ATE}$, for all units (effect of switching)

-   $\text{ATT}$, for treated units (effect of taking away treatment)

-   $\text{ATC}$ or $\text{ATU}$, for untreated units (effect of adding treatment)

<aside>Some times we see these terms pre-fixed with an $S$ (for sample) or a $P$ (for population).</aside>

@tbl-pot-example allows us to gain further intuition on these calculations with a hypothetical example in which people are assigned college degrees randomly.

| Group ($T$)              | $E[Y^1]$ | $E[Y^0]$ |
|--------------------------|----------|----------|
| College Degree ($T = 1$) | **1000** | 600^\*^  |
| No Degree ($T = 0$)      | 800^\*^  | **500**  |

: Potential Outcomes Example {#tbl-pot-example}

<aside>

<br>

Simple experiments of these kind are obviously impossible in the social sciences.

\* means unobservable

</aside>

If **30%** of the population has a degree...

::: grid
::: g-col-6
-   What is the naive estimate?

    [500]{style="color: #C5C5C5;"}

-   What is the ATT?

    [400]{style="color: #C5C5C5;"}
:::

::: g-col-6
-   What is the ATC?

    [300]{style="color: #C5C5C5;"}

-   What is the ATE?

    [$0.3\times 400 + 0.7 \times 300 = 330$]{style="color: #C5C5C5;"}
:::
:::

**More than two treatment levels, continuous treatments, and multiple treatment factors**

Multiple treatment effects can be defined relative to a baseline level, following the general principles of regression modeling with indicator (or dummy) variables.

Treatment levels can be continuous.

> To conceptualize randomization with a continuous treatment, think of spinning a spinner that can land on any of the potential levels of the treatment assignment. As with regression inputs in general, it can make sense to fit more complicated models as suggested by theory or supported by data. A linear model---which estimates the average effect on $y$ for each additional unit of $z$---is a natural starting point for effects that are believed to be monotonically increasing or decreasing functions of the treatment level.
>
> @gelman2020 [pp. 342]

Finally, we can also consider multiple simultaneous treatments.

> ...multiple treatments can be administered in combination. For instance, depressed individuals could be randomly assigned to receive nothing, drugs, counseling sessions, or both drugs and counseling sessions. These combinations could be modeled as two treatments and their interaction or as four distinct treatments.
>
> @gelman2020 [pp. 342]

**Using design and analysis to address imbalance and lack of overlap between treatment and control groups**

Regression can be useful even in the context of randomized experiments.

> In practice, we can never ensure that treatment and control groups are balanced on all relevant pre-treatment characteristics. However, there are statistical approaches that may bring us closer. At the design stage, we can use *randomization* to ensure that treatment and control groups are balanced in expectation, and we can use *blocking* to reduce the variation in any imbalance. At the analysis stage, we can *adjust* for pre-treatment variables to correct for differences between the two groups to reduce bias in our estimate of the sample average treatment effect. We can further adjust for differences between sample and population if our goal is to estimate the population average treatment effect.
>
> @gelman2020 [pp. 344]

We can increase the precision of treatment effect estimates by adjusting for pre-treatment variables that are predictive of the outcome---i.e., we get lower standard errors on the treatment parameter.

**Group or cluster-randomized experiments**

Sometimes it's difficult to randomize treatment assignments to individual units, and so we might randomize groups or clusters to receive the treatment instead.

> A decision to assign treatments at the group level can be driven by cost or logistical concerns. It might be more cost effective to provide free flu shots to a random subset of health clinics, for example, than to have professionals go to every clinic and then randomly assign individuals to receive shots. Assignment at the clinic level would also avoid creating ill will among potential study participants being deprived of a service that others in the same location are able to receive. Cluster-randomized experiments are also used to avoid spillover or contagion effects (which can also be considered as violations of the stable unit treatment value assumption or SUTVA).
>
> @gelman2020 [pp. 349-50]

### SUTVA

Besides **ignorability** (see @eq-ignore), randomization implies certain other properties which we might consider to be **assumptions**.

The most important one is the **stable unit treatment value assumption** (SUTVA). It means that we assume that there is no interference among units (i.e., no spillovers) and no hidden versions of the treatment.

Thus, one can imagine person $i$'s outcome to be "a function not only of her own treatment assignment, but also the treatment assignments of others in the sample" [@gelman2020, pp. 353]. These becomes intractable very quickly. For example, in a sample of just 10 people and a binary treatment, we would have 2^10^ = 1024 potential outcomes for each person. Thus, researchers often hope that there is no interference among units, or else they think about modeling spillover in some way.

SUTVA also implies that there are no hidden versions of treatments---i.e., we want $t_i$ to equal $t_j$.

> Examples of potential SUTVA violations abound. An experiment testing the effect of a new fertilizer by randomly assigning adjacent plots to treatment or control is a classic example. Fertilizer from one plot might leach into an adjacent plot assigned to receive no fertilizer and thus affect the yield in that control plot. Vaccines that reduce the probability of a contagious disease within a school, business, or community could easily lead to violation of SUTVA if the vaccine is actually effective. Consider an experiment that recruited families from the same public housing complex and randomized them to receive a voucher to move to a better neighborhood or not. This could suffer from interference if a given family moving might influence (positively or negatively) the well-being of another family that happened to be randomized to not receive the voucher.
>
> @gelman2020 [pp. 353]

In educational settings, potential SUTVA violations are often a reason to assign treatments at the classroom or school level.

## Identification and Causal Diagrams

**Identification** refers to the idea of *identifying* the effect of a treatment (or cause) on an outcome.

Causal diagrams or DAGs are graphical representations of a **data generating process**. Everything we draw is hopefully an informed assumption; everything that's not in the diagram is *also* an assumption. In other words, DAGs encode identifying assumptions.

-   **Paths**.

-   **Direct effects**.

-   **Indirect effects**.

-   **Total effects**.

-   **Front door paths**. They point *away* from the treatment. If we want to estimate "total effects," then we want to keep all of them open.

-   **Back door paths**. They point *towards* the treatment; we want to close them off.

-   **Confounding**. A property of paths, not of variables.

-   **Colliders**. A variable is a collider in a path *iff* both arrows point at it.

    $$
    a \to b \to c_\text{ollider} \leftarrow d \leftarrow e \to f
    $$

    Here, $b$ and $c$ are unrelated *unless* we remove variation in $c$.

    In other words, we close paths by removing variation from one variable along the path (i.e., adjusting); but if the variable is a collider, then removing variation will actually *open* a path that was already closed.

    An often unacknowledged way of adjusting for colliders is during the sample selection phase. If we have a sample of college students, it means we are adjusting for college attendance.

-   **Open Path.** A path in which there is variation in all variables along the path (and no variation in colliders).

-   **Closed Path.** A path in which there is at least one variable with no variation (or a collider with variation).

The idea of a directed acyclical graph (DAG) implies that there are *no cycles.* If a variable causes itself, it's near impossible to isolate or identifying the cause of anything. The world is full with feedback loops of all sorts, but we deal with them through the incorporation of time or by isolating one effect through some kind of experimental scenario.

*Note that DAGs are agnostic about functional form. This includes interactions among variables! Some people deal with interactions by drawing arrows toward arrows or by representing interactions explicitly as separate nodes.*

The nicest thing about DAGs is that they help us spell out the **testable implications** of our assumptions. For example, if our causal diagram implies that a relationship between variables is zero, we can check for that (this is called a *placebo test).*

<aside>

*Placebo test*

*Don't confuse with placebo effect!*

</aside>

Causal diagrams also help us understand the idea the idea identification doesn't necessarily require us to take care of all back door paths; *sometimes we can use **randomization**.* Sometimes this is achieved in experimental settings, but sometimes we can form some form of randomization in real-world settings (i.e., *natural experiments).* As depicted in @fig-front-door, **experiments** and **instrumental variables** have equivalent representations in DAGs.

<aside>Finding front doors using randomization</aside>

![Randomization](images/front-door.png){#fig-front-door fig-align="center" width="80%"}

<aside>*Randomness ensures that the path between unobserved* $U$ *and the treatment is severed*</aside>

## Extra

```{r}
#| message: false
#| warning: false
library(tidyverse)
theme_set(
  theme_light(base_family = "Avenir Next Condensed") +
  theme(strip.background = element_rect(fill = "#666666"))
)
```

#### Average Treatment Effects

> 18.4 The table below describes a hypothetical experiment on 8 people. Each row of the table gives a participant and her pre-treatment predictor $x$, treatment indicator $z$, and potential outcomes $y^0$ and $y^1$.

```{r}
#| code-fold: true

d <- data.frame(
  X = c(3, 5, 2, 8, 5, 10, 2, 11),
  Z = c(0, 0, 1, 0, 0, 1, 1, 1),
  Y0 = c(5, 8, 5, 12, 4, 8, 4, 9),
  Y1 = c(5, 10, 3, 13, 2, 9, 1, 13), 
  row.names = LETTERS[1:8]
)

d <- d |> 
  mutate(Y = ifelse(Z == 0, Y0, Y1))

knitr::kable(d)
```

Naive estimate:

```{r}
i <- as.logical(d$Z)

## Naive Estimate: 
mean(d$Y1[i] - d$Y0[!i])
## ATT:
mean(d$Y1[i] - d$Y0[i])
## ATE:
mean(d$Y1 - d$Y0)
```

> Simulate a new completely randomized experiment on these 8 people; that is, resample $z$ at random with the constraint that equal numbers get the treatment and the control.

```{r}
j <- sample(i)

## Naive Estimate: 
mean(d$Y1[j] - d$Y0[!j])
## ATT:
mean(d$Y1[j] - d$Y0[j])
## ATE:
mean(d$Y1 - d$Y0)
```

Again:

```{r}
k <- sample(i)

## Naive Estimate: 
mean(d$Y1[k] - d$Y0[!k])
## ATT:
mean(d$Y1[k] - d$Y0[k])
## ATE:
mean(d$Y1 - d$Y0)
```

#### The Electric Company

Add description of dataset. This is a randomized experiment.

![](images/electric-company.png){fig-align="center" width="90%"}

```{r}
#| code-fold: true
#| message: false
#| warning: false

url <- "https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv"

d <- read_csv(url)[, -1] |> 
  mutate(across(grade:pair_id, as.integer))

glimpse(d)
```

The `supp` variable indicates a subtlety in the experiment---i.e., every teacher had the choice of *replacing* or *supplementing* the regular reading program with the television show. Thus, this experiment really estimated *the effect of making the program available.*

```{r}
#| code-fold: true
#| message: false
#| warning: false
#| fig-height: 3
#| fig-width: 5
#| fig-align: center

d |> 
  mutate(
    grade = paste("Grade", grade),
    t = ifelse(treatment == 1, "Treatment", "Control")) |> 
  ggplot(aes(post_test, y = grade, color = t)) + 
  stat_summary(size = 1/8, fun.args = list(mult = 2), position = position_dodge(1/3)) +
  labs(y = NULL, color = NULL) + 
  theme(legend.position = "top")
```

The point of this exercise is to demonstrate that it's *almost* always a good idea to include pre-treatment information when analyzing experimental data.

> Under a clean randomization, adjusting for pre-treatment predictors in this way does not change what we are estimating. However, if the predictor has a strong association with the outcome it can help to bring each estimate closer (on average) to the truth, and if the randomization was less than pristine, the addition of predictors to the equation may help us adjust for *systematically* unbalanced characteristics across groups. Thus, this strategy has the potential to adjust for both random and systematic differences between the treatment and control groups (that is, to reduce both variance and bias), as long as these differences are characterized by differences in the pre-test.
>
> 368

Just look at the change in coefficient and standard error estimates:

```{r}
mod1 <- lm(post_test ~ treatment, data = d)
mod2 <- lm(post_test ~ treatment + pre_test, data = d)
mod3 <- lm(post_test ~ treatment + pre_test + grade, data = d)

modelsummary::msummary(
  models = list(mod1, mod2, mod3), 
  gof_map = NA
)

```

And here's a separate model for each grade:

```{r}
#| code-fold: true
#| label: fig-electric
#| fig-cap: "Estimates, 50%, and 95% intervals for the effect of watching The Electric Company. The same model has been fitted separately for each grade."
#| fig-subcap: 
#|   - "Regression on treatment indicator"
#|   - "Regression on treatment indicator and pre-test"
#| layout-ncol: 2

fit1 <- map_df(1:4, function(i) {
  lm(formula = post_test ~ treatment, 
     data = d, 
     subset = grade == i) |> 
    broom::tidy(conf.int = TRUE) |> 
    filter(term == "treatment") |> 
    mutate(grade = paste("Grade", i))
})

fit2 <- map_df(1:4, function(i) {
  lm(formula = post_test ~ treatment + pre_test, 
     data = d, 
     subset = grade == i) |> 
    broom::tidy(conf.int = TRUE) |> 
    filter(term == "treatment") |> 
    mutate(grade = paste("Grade", i))
})

fit1 |> 
  ggplot(aes(estimate, grade)) + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_linerange(aes(
    xmin = estimate + std.error*qnorm(0.05),
    xmax = estimate + std.error*qnorm(0.95)), 
  ) +
  geom_linerange(aes(
    xmin = estimate + std.error*qnorm(0.25),
    xmax = estimate + std.error*qnorm(0.75)
    ), linewidth = 1.5
  ) +
  geom_point(shape = 21, fill = "white") + 
  labs(y = NULL, x = "treatment",
       title = "lm(post_treat ~ treatment)")

fit2 |> 
  ggplot(aes(estimate, grade)) + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_linerange(aes(
    xmin = estimate + std.error*qnorm(0.05),
    xmax = estimate + std.error*qnorm(0.95)), 
  ) +
  geom_linerange(aes(
    xmin = estimate + std.error*qnorm(0.25),
    xmax = estimate + std.error*qnorm(0.75)
    ), linewidth = 1.5
  ) +
  geom_point(shape = 21, fill = "white") + 
  labs(y = NULL, x = "treatment",
       title = "lm(post_treat ~ treatment + pre_test)")
```

So, adding pre-treatment information will always make for more precise and less biased estimates. But don't adjust for post-treatment variables. In fact, always draw a DAG.

Adjusting for a post-treatment variable $q$ breaks down the assumption of ignorability.

$$
Y^0, Y^1 \not \perp T \mid q
$$

## Additional Resources

General Resources

:   @hernan2023, @imbens2015, @morgan2014, @angrist2009, @ashworth2021

Potential Outcomes

:   @rubin2005, @holland1986

Causal Diagrams

:   @pearl2009, @pearl2016

Machine Learning and Causal Inference

:   @dorie2019
